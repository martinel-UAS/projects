{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Mining information from Text Data \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Mining information from Text Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the whole anthologies abstract dataset. Extract a list of the authors and editors per publication and create baskets and perform a search of similar items, for example:\n",
    "\n",
    "- basket 1: Mostafazadeh Davani Aida,Kiela Douwe,Lambert Mathias,Vidgen, Bertie Prabhakaran Vinodkumar, Waseem, Zeerak\n",
    "- basket 2: Singh Sumer, Li Sheng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find the frequent pair of items (2-tuples) using the na誰ve, A-priori and PCY algorithms. For each of these compare the time of execution and results for supports s=10, 50, 100. Comment your results. \n",
    "\n",
    "2. For the PCY algorithm, create up to 5 compact hash tables. What is  the difference in results and time of execution for 1,2,3,4 and 5 tables? Comment your results.\n",
    "\n",
    "3. Find the final list of k-frequent items (k-tuples) for k=3 and 4. Experiment a bit and describe the best value for the support in each case. *Warning*: You can use any of the three algorithms, but be careful, because the algorithm can take too long if you don't chose it properly (well, basically don't use the na誰ve approach ;)).\n",
    "\n",
    "4. Using one of the results of the previous items, for one k (k=2 or 3) find the possible clusters using the 1-NN criteria. Comment your results.\n",
    "\n",
    "> 1-NN means that if you have a tuple {A,B,C} and {C,E,F} then because they share one element {C}, then they belong to the same cluster  {A,B,C,E,F}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File anthology+abstracts.bib already available in folder /data\n"
     ]
    }
   ],
   "source": [
    "url  = 'https://aclanthology.org/anthology+abstracts.bib.gz'   # url where the file is stored\n",
    "filename = 'anthology+abstracts.bib'                           # bib filename\n",
    "folder   = 'data'                                              # folder name\n",
    "minimum = 200                                                  # minimum number of words in the abtract to be considered \n",
    "\n",
    "# Create the path to store the files\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "file = folder + '/' + filename\n",
    "\n",
    "# Download the file if it doesn't exist locally\n",
    "if(not os.path.exists(file)):\n",
    "  print(\"Downloading \" + url + \" to /\" + folder + \"...\" )\n",
    "  with gzip.open(BytesIO(urlopen(url).read()), 'rb') as fb:\n",
    "    with open(file, 'wb') as f:\n",
    "        f.write(fb.read())\n",
    "else:\n",
    "  print(\"File \" + filename + \" already available in folder /\" + folder)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read authors / editors from the bib file\n",
    "\n",
    "elements = []\n",
    "with open(file, 'r',errors='ignore') as f:\n",
    "    string = ''\n",
    "    found = False\n",
    "    # skip all lines until author/editor\n",
    "    for line in f:\n",
    "      if found:\n",
    "        if '=' in line:                                        \n",
    "          elements.append(string)\n",
    "          string = ''\n",
    "          found = False\n",
    "        else:\n",
    "          string = string + line\n",
    "      if 'author = \"' in line:                                 \n",
    "        found = True\n",
    "        string = string + line       \n",
    "      if 'editor = \"' in line:                                 \n",
    "        found = True\n",
    "        string = string + line        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Save list of authors/editors in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of baskets: 70449\n",
      "Number of items: 217901\n"
     ]
    }
   ],
   "source": [
    "# Preporcess and clean the data and save it to the file ./data/authors.txt\n",
    "authors_fname = './data/authors.txt'\n",
    "baskets = []\n",
    "for e in elements:\n",
    "    new_string = e.replace(\"and\", \"\")\n",
    "    new_string = new_string.replace(\"\\n\", \"\")\n",
    "    new_string = new_string.replace(\"    editor = \", \"\")\n",
    "    new_string = new_string.replace(\"    author = \", \"\")\n",
    "    new_string = new_string.replace(',', \"\")\n",
    "    new_string = new_string.replace('\"', \"\")\n",
    "    new_string = new_string.replace('        ', \",\")\n",
    "    baskets.append(new_string)\n",
    "\n",
    "with open(authors_fname, 'w') as file:    \n",
    "    for s in baskets:        \n",
    "        print(s, file=file)\n",
    "\n",
    "nitems = 0\n",
    "with open(authors_fname, \"rt\", encoding='latin1') as f:\n",
    "    for line in f:\n",
    "        C_k  = line.rstrip().split(',')\n",
    "        nitems = nitems + len(C_k)\n",
    "\n",
    "nbaskets = len(baskets)\n",
    "print('Number of baskets:', nbaskets)\n",
    "print('Number of items:', nitems)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### 1. Find the frequent pair of items (2-tuples) using the na誰ve, A-priori and PCY algorithms. For each of these compare the time of execution and results for supports s=10, 50, 100. Comment your results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Na誰ve approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the date and generate the frozenset\n",
    "def readdata(k, fname=authors_fname):    \n",
    "    with open(fname, \"rt\", encoding='latin1') as f:\n",
    "        for line in f:\n",
    "            C_k  = line.rstrip().split(',')\n",
    "            for itemset in itertools.combinations(C_k, k):\n",
    "                    yield frozenset(itemset)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'Kiela Douwe', 'Mostafazadeh Davani Aida'})\n",
      "frozenset({'Mostafazadeh Davani Aida', 'Lambert Mathias'})\n",
      "frozenset({'Vidgen Bertie', 'Mostafazadeh Davani Aida'})\n",
      "frozenset({'Prabhakaran Vinodkumar', 'Mostafazadeh Davani Aida'})\n",
      "frozenset({'Waseem Zeerak', 'Mostafazadeh Davani Aida'})\n",
      "frozenset({'Kiela Douwe', 'Lambert Mathias'})\n",
      "frozenset({'Vidgen Bertie', 'Kiela Douwe'})\n",
      "frozenset({'Prabhakaran Vinodkumar', 'Kiela Douwe'})\n",
      "frozenset({'Waseem Zeerak', 'Kiela Douwe'})\n",
      "frozenset({'Vidgen Bertie', 'Lambert Mathias'})\n"
     ]
    }
   ],
   "source": [
    "nitems = 10\n",
    "for C_k in readdata(k=2):\n",
    "    print(C_k)    \n",
    "    nitems -= 1\n",
    "    if nitems == 0: \n",
    "        break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_C(k):\n",
    "    start = time()\n",
    "    C = {}\n",
    "    for key in readdata(k):\n",
    "        if key not in C:\n",
    "            C[key] = 1\n",
    "        else:\n",
    "            C[key] += 1\n",
    "    print(\"Took {}s for k={}\".format((time() - start), k))\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 0.3750131130218506s for k=1\n",
      "C1 contains 61387 items\n",
      "Took 0.6956984996795654s for k=2\n",
      "C2 contains 247357 items\n"
     ]
    }
   ],
   "source": [
    "C1 = get_C(1)\n",
    "print(\"C1 contains {} items\".format(len(C1)))\n",
    "C2 = get_C(2)\n",
    "print(\"C2 contains {} items\".format(len(C2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'Kiela Douwe', 'Mostafazadeh Davani Aida'}) 2\n",
      "frozenset({'Mostafazadeh Davani Aida', 'Lambert Mathias'}) 1\n",
      "frozenset({'Vidgen Bertie', 'Mostafazadeh Davani Aida'}) 2\n",
      "frozenset({'Prabhakaran Vinodkumar', 'Mostafazadeh Davani Aida'}) 3\n",
      "frozenset({'Waseem Zeerak', 'Mostafazadeh Davani Aida'}) 2\n"
     ]
    }
   ],
   "source": [
    "for (ck, n), _ in zip(C2.items(), range(5)):\n",
    "    print(ck,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def association_rules(nitems, C1, C2, L2):\n",
    "    for i in range(len(L2)):\n",
    "        A, B = L2[i]\n",
    "        support_AB = C2[frozenset([A, B])]\n",
    "        support_A = C1[frozenset([A])]\n",
    "        conf_A_leads_to_B = support_AB / support_A\n",
    "\n",
    "        support_B = C1[frozenset([B])]\n",
    "        prob_B = support_B / nitems\n",
    "\n",
    "        interest_A_leads_to_B = conf_A_leads_to_B - prob_B\n",
    "\n",
    "        if interest_A_leads_to_B > 0.7:\n",
    "            print(\"{} --> {} with interest {:3f}\".format(A, B, interest_A_leads_to_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "\n",
      "Support: 10\n",
      "Aguilar Gustavo --> Solorio Thamar with interest 0.907970\n",
      "Rashid Ahmad --> Rezagholizadeh Mehdi with interest 0.999773\n",
      "Negri Matteo --> Turchi Marco with interest 0.751200\n",
      "Chaudhary Vishrav --> Guzm{\\'a}n Francisco with interest 0.707709\n",
      "Erdmann Grant --> Gwinnup Jeremy with interest 0.999702\n",
      "Escolano Carlos --> Fonollosa Jos{\\'e} A. R. with interest 0.832680\n",
      "Wang Xing --> Tu Zhaopeng with interest 0.903896\n",
      "Wei Daimeng --> Shang Hengchao with interest 0.908935\n",
      "Lei Lizhi --> Wei Daimeng with interest 0.999844\n",
      "Lei Lizhi --> Shang Hengchao with interest 0.999844\n",
      "Guo Jiaxin --> Yang Hao with interest 0.999716\n",
      "Wang Minghan --> Yang Hao with interest 0.937216\n",
      "Lei Lizhi --> Yang Hao with interest 0.999716\n",
      "Li Bei --> Zhu Jingbo with interest 0.832354\n",
      "Xiao Tong --> Zhu Jingbo with interest 0.885384\n",
      "Hangya Viktor --> Fraser Alexer with interest 0.840998\n",
      "Stojanovski Dario --> Fraser Alexer with interest 0.921970\n",
      "Crego Josep --> Senellart Jean with interest 0.949333\n",
      "Chao Lidia S. --> Wong Derek F. with interest 0.999375\n",
      "Nakazawa Toshiaki --> Kurohashi Sadao with interest 0.800426\n",
      "Mino Hideya --> Goto Isao with interest 0.849574\n",
      "Hirasawa Tosho --> Komachi Mamoru with interest 0.998652\n",
      "Priyadharshini Ruba --> Chakravarthi Bharathi Raja with interest 0.999560\n",
      "Jauhiainen Tommi --> Jauhiainen Heidi with interest 0.833120\n",
      "Jauhiainen Tommi --> Lind{\\'e}n Krister with interest 0.832837\n",
      "Li Manling --> Ji Heng with interest 0.997544\n",
      "Mistica Meladel --> Baldwin Timothy with interest 0.829714\n",
      "Gaido Marco --> Negri Matteo with interest 0.998623\n",
      "Gaido Marco --> Turchi Marco with interest 0.998623\n",
      "Guo Xiaoxiao --> Yu Mo with interest 0.999361\n",
      "Kobayashi Sosuke --> Inui Kentaro with interest 0.831289\n",
      "Yokoi Sho --> Inui Kentaro with interest 0.907047\n",
      "Kuribayashi Tatsuki --> Inui Kentaro with interest 0.997956\n",
      "Barrett Maria --> S{\\o}gaard Anders with interest 0.821003\n",
      "Goldberger Jacob --> Dagan Ido with interest 0.738526\n",
      "Cui Yiming --> Liu Ting with interest 0.769817\n",
      "Cui Yiming --> Wang Shijin with interest 0.817841\n",
      "Che Wanxiang --> Liu Ting with interest 0.862259\n",
      "Wang Shijin --> Liu Ting with interest 0.830423\n",
      "Hu Guoping --> Liu Ting with interest 0.797090\n",
      "Wang Shijin --> Hu Guoping with interest 0.749645\n",
      "Illina Irina --> Fohr Dominique with interest 0.999815\n",
      "Chen Yubo --> Liu Kang with interest 0.779366\n",
      "Zhao Jun --> Liu Kang with interest 0.748879\n",
      "Liu Shengping --> Liu Kang with interest 0.998879\n",
      "Pimentel Tiago --> Cotterell Ryan with interest 0.998410\n",
      "Liu Zhengyuan --> Chen Nancy with interest 0.908736\n",
      "Winata Genta Indra --> Fung Pascale with interest 0.998325\n",
      "Mehri Shikib --> Eskenazi Maxine with interest 0.768507\n",
      "Madotto Andrea --> Fung Pascale with interest 0.929360\n",
      "Da San Martino Giovanni --> Nakov Preslav with interest 0.755390\n",
      "Chamberlain Jon --> Poesio Massimo with interest 0.844536\n",
      "Zhang Xuejie --> Wang Jin with interest 0.908509\n",
      "Xiang Rong --> Lu Qin with interest 0.768351\n",
      "Xiang Rong --> Huang Chu-Ren with interest 0.766236\n",
      "Pasunuru Ramakanth --> Bansal Mohit with interest 0.998226\n",
      "Galitsky Boris --> Ilvovsky Dmitry with interest 0.894425\n",
      "Georgiev Georgi --> Nakov Preslav with interest 0.731147\n",
      "Koychev Ivan --> Nakov Preslav with interest 0.747814\n",
      "Escolano Carlos --> Costa-juss{\\`a} Marta R. with interest 0.915645\n",
      "Kamigaito Hidetaka --> Okumura Manabu with interest 0.869591\n",
      "Osenova Petya --> Simov Kiril with interest 0.783721\n",
      "Villata Serena --> Cabrio Elena with interest 0.823104\n",
      "Kato Yoshihide --> Matsubara Shigeki with interest 0.999432\n",
      "Anderson Mark --> G{\\'o}mez-Rodr{\\'\\i}guez Carlos with interest 0.908211\n",
      "Gerlach Johanna --> Bouillon Pierrette with interest 0.932283\n",
      "Verhagen Marc --> Pustejovsky James with interest 0.873297\n",
      "Ning Qiang --> Roth Dan with interest 0.822936\n",
      "Demeester Thomas --> Develder Chris with interest 0.933135\n",
      "Malakasiotis Prodromos --> Androutsopoulos Ion with interest 0.726520\n",
      "Reimers Nils --> Gurevych Iryna with interest 0.913686\n",
      "Daxenberger Johannes --> Gurevych Iryna with interest 0.863686\n",
      "Conia Simone --> Navigli Roberto with interest 0.998410\n",
      "Yu Tao --> Radev Dragomir with interest 0.748907\n",
      "Wang Xinyi --> Neubig Graham with interest 0.906096\n",
      "van der Lee Chris --> Krahmer Emiel with interest 0.768422\n",
      "Zhao Tuo --> Jiang Haoming with interest 0.999830\n",
      "Sabharwal Ashish --> Khot Tushar with interest 0.791354\n",
      "Suleman Kaheer --> Trischler Adam with interest 0.768947\n",
      "Yan Hang --> Qiu Xipeng with interest 0.999063\n",
      "Wu Fangzhao --> Wu Chuhan with interest 0.874546\n",
      "Wu Chuhan --> Huang Yongfeng with interest 0.905782\n",
      "Wu Fangzhao --> Huang Yongfeng with interest 0.843282\n",
      "Yin Pengcheng --> Neubig Graham with interest 0.830338\n",
      "Dinan Emily --> Weston Jason with interest 0.999475\n",
      "Parikh Devi --> Batra Dhruv with interest 0.812259\n",
      "He Keqing --> Yan Yuanmeng with interest 0.833163\n",
      "Yan Yuanmeng --> Xu Weiran with interest 0.999645\n",
      "He Keqing --> Xu Weiran with interest 0.916312\n",
      "Wu Lijun --> Qin Tao with interest 0.908779\n",
      "Qin Tao --> Liu Tie-Yan with interest 0.817841\n",
      "Chen Kehai --> Utiyama Masao with interest 0.787756\n",
      "Chen Kehai --> Sumita Eiichiro with interest 0.786181\n",
      "Nie Yixin --> Bansal Mohit with interest 0.907317\n",
      "Yu Juntao --> Poesio Massimo with interest 0.748382\n",
      "Chakrabarty Tuhin --> Muresan Smara with interest 0.735919\n",
      "Yimam Seid Muhie --> Biemann Chris with interest 0.862018\n",
      "Lin Bill Yuchen --> Ren Xiang with interest 0.799035\n",
      "Chen Lu --> Yu Kai with interest 0.809112\n",
      "Yao Yuan --> Liu Zhiyuan with interest 0.998524\n",
      "Yao Yuan --> Sun Maosong with interest 0.998155\n",
      "Han Xu --> Liu Zhiyuan with interest 0.877312\n",
      "Huang Shujian --> Chen Jiajun with interest 0.828701\n",
      "Dai Xinyu --> Chen Jiajun with interest 0.730201\n",
      "Xiong Wenhan --> Wang William Yang with interest 0.936251\n",
      "Chiticariu Laura --> Li Yunyao with interest 0.932737\n",
      "Liu Tingwen --> Yu Bowen with interest 0.937287\n",
      "Brown Susan Windisch --> Palmer Martha with interest 0.906252\n",
      "Yanaka Hitomi --> Mineshima Koji with interest 0.916298\n",
      "Bekki Daisuke --> Mineshima Koji with interest 0.825718\n",
      "Bali Kalika --> Choudhury Monojit with interest 0.852047\n",
      "Kazantseva Anna --> Szpakowicz Stan with interest 0.772003\n",
      "Kijak Ewa --> Claveau Vincent with interest 0.999304\n",
      "Sakti Sakriani --> Nakamura Satoshi with interest 0.966578\n",
      "Perrier Guy --> Guillaume Bruno with interest 0.726960\n",
      "Postma Marten --> Vossen Piek with interest 0.998694\n",
      "Zhu Xiaoyan --> Huang Minlie with interest 0.771734\n",
      "Gangal Varun --> Hovy Eduard with interest 0.853495\n",
      "Vanetik Natalia --> Litvak Marina with interest 0.999603\n",
      "Chen Chung-Chi --> Chen Hsin-Hsi with interest 0.997885\n",
      "Huang Hen-Hsen --> Chen Hsin-Hsi with interest 0.997885\n",
      "Gong Ming --> Shou Linjun with interest 0.940949\n",
      "Chen Yufeng --> Xu Jinan with interest 0.814389\n",
      "Zhang Jiajun --> Zong Chengqing with interest 0.921487\n",
      "Qi Tao --> Huang Yongfeng with interest 0.946900\n",
      "Choubey Prafulla Kumar --> Huang Ruihong with interest 0.916070\n",
      "Gupta Deepak --> Ekbal Asif with interest 0.783897\n",
      "Zhang Jinchao --> Zhou Jie with interest 0.953353\n",
      "Scialom Thomas --> Staiano Jacopo with interest 0.916482\n",
      "Wachsmuth Henning --> Stein Benno with interest 0.738336\n",
      "Yang Liang --> Lin Hongfei with interest 0.999730\n",
      "He Pengcheng --> Liu Xiaodong with interest 0.916241\n",
      "Tan Xu --> Liu Tie-Yan with interest 0.999659\n",
      "Zhou Deyu --> He Yulan with interest 0.806897\n",
      "Lauscher Anne --> Glava{\\v{s}} Goran with interest 0.784792\n",
      "Liu Qian --> Lou Jian-Guang with interest 0.833106\n",
      "Lin Yankai --> Liu Zhiyuan with interest 0.772717\n",
      "Qi Fanchao --> Liu Zhiyuan with interest 0.998524\n",
      "Qi Fanchao --> Sun Maosong with interest 0.998155\n",
      "He Xiaofeng --> Wang Chengyu with interest 0.999801\n",
      "Kanojia Diptesh --> Bhattacharyya Pushpak with interest 0.965297\n",
      "Sun Changlong --> Liu Xiaozhong with interest 0.812273\n",
      "Wu Winston --> Yarowsky David with interest 0.776500\n",
      "Ma Mingbo --> Huang Liang with interest 0.998935\n",
      "Chen Yidong --> Shi Xiaodong with interest 0.894467\n",
      "Krishna Amrith --> Goyal Pawan with interest 0.999404\n",
      "Zhang Wei-Nan --> Liu Ting with interest 0.997090\n",
      "Potthast Martin --> Stein Benno with interest 0.817387\n",
      "Mittal Arpit --> Christodoulopoulos Christos with interest 0.749631\n",
      "Su Pei-Hao --> Mrk{\\v{s}}i{\\'c} Nikola with interest 0.768819\n",
      "Su Pei-Hao --> Wen Tsung-Hsien with interest 0.845785\n",
      "Lopez de Lacalle Oier --> Agirre Eneko with interest 0.748197\n",
      "Luan Huanbo --> Liu Yang with interest 0.820605\n",
      "Luan Huanbo --> Sun Maosong with interest 0.998155\n",
      "He Shizhu --> Liu Kang with interest 0.946247\n",
      "Yang Pengcheng --> Sun Xu with interest 0.748850\n",
      "Feng Xiaocheng --> Qin Bing with interest 0.856376\n",
      "Ding Xiao --> Liu Ting with interest 0.997090\n",
      "Qin Bing --> Liu Ting with interest 0.830423\n",
      "Hou Lei --> Li Juanzi with interest 0.999475\n",
      "Teng Zhiyang --> Zhang Yue with interest 0.830863\n",
      "Lin Hongyu --> Han Xianpei with interest 0.999418\n",
      "Lin Hongyu --> Sun Le with interest 0.943650\n",
      "Huang Lifu --> Ji Heng with interest 0.775322\n",
      "Pauls Adam --> Klein Dan with interest 0.787330\n",
      "Fern{\\'a}ndez-Gonz{\\'a}lez Daniel --> G{\\'o}mez-Rodr{\\'\\i}guez Carlos with interest 0.845274\n",
      "Han Wenjuan --> Jiang Yong with interest 0.713846\n",
      "Taniguchi Motoki --> Ohkuma Tomoko with interest 0.999603\n",
      "Taniguchi Tomoki --> Ohkuma Tomoko with interest 0.999603\n",
      "Chang Shiyu --> Yu Mo with interest 0.999361\n",
      "Brunato Dominique --> Dell{'}Orletta Felice with interest 0.999390\n",
      "Venturi Giulia --> Dell{'}Orletta Felice with interest 0.922467\n",
      "Ng Raymond --> Carenini Giuseppe with interest 0.881444\n",
      "Eckart de Castilho Richard --> Gurevych Iryna with interest 0.797019\n",
      "Mayhew Stephen --> Roth Dan with interest 0.730182\n",
      "Liu Haokun --> Bowman Samuel R. with interest 0.916141\n",
      "Kiesel Johannes --> Stein Benno with interest 0.999205\n",
      "Gao Yingbo --> Ney Hermann with interest 0.996494\n",
      "Croce Danilo --> Basili Roberto with interest 0.959219\n",
      "Lin Junyang --> Sun Xu with interest 0.855993\n",
      "Lu Yaojie --> Han Xianpei with interest 0.845572\n",
      "Wilcox Ethan --> Levy Roger with interest 0.999319\n",
      "Feng Xiaocheng --> Liu Ting with interest 0.711376\n",
      "Oh Jong-Hoon --> Torisawa Kentaro with interest 0.713235\n",
      "Lyu Michael --> King Irwin with interest 0.832978\n",
      "Song Wei --> Fu Ruiji with interest 0.714087\n",
      "Song Wei --> Liu Ting with interest 0.782804\n",
      "Liu Lizhen --> Song Wei with interest 0.999801\n",
      "Fu Ruiji --> Liu Ting with interest 0.997090\n",
      "Haque Rejwanul --> Way Andy with interest 0.842761\n",
      "Saunders Danielle --> Byrne Bill with interest 0.999375\n",
      "Burkett David --> Klein Dan with interest 0.997857\n",
      "Yamamoto Eiko --> Isahara Hitoshi with interest 0.855198\n",
      "Kawahara Daisuke --> Kurohashi Sadao with interest 0.794900\n",
      "Guti{\\'e}rrez Yoan --> Montoyo Andr{\\'e}s with interest 0.719489\n",
      "Silva Jo{\\~a}o --> Branco Ant{\\'o}nio with interest 0.922282\n",
      "Venturi Giulia --> Montemagni Simonetta with interest 0.768578\n",
      "Katsumata Satoru --> Komachi Mamoru with interest 0.998652\n",
      "Cui Lei --> Zhou Ming with interest 0.806855\n",
      "Bossy Robert --> N{\\'e}dellec Claire with interest 0.833106\n",
      "Lu Di --> Ji Heng with interest 0.843698\n",
      "Thompson Paul --> Ananiadou Sophia with interest 0.760244\n",
      "Mapelli Val{\\'e}rie --> Choukri Khalid with interest 0.763783\n",
      "Van den Bogaert Joachim --> Vanallemeersch Tom with interest 0.833064\n",
      "Zalmout Nasser --> Habash Nizar with interest 0.820747\n",
      "Camelin Nathalie --> Est{\\`e}ve Yannick with interest 0.764067\n",
      "Saedi Chakaveh --> Branco Ant{\\'o}nio with interest 0.845359\n",
      "Maks Isa --> Vossen Piek with interest 0.784408\n",
      "Rudinger Rachel --> Van Durme Benjamin with interest 0.758084\n",
      "Marciniak Malgorzata --> Mykowiecka Agnieszka with interest 0.999744\n",
      "Remus Steffen --> Biemann Chris with interest 0.907473\n",
      "Proisl Thomas --> Kabashi Besim with interest 0.733163\n",
      "Persing Isaac --> Ng Vincent with interest 0.998552\n",
      "Cho Eunah --> Waibel Alex with interest 0.738938\n",
      "Delvaux V{\\'e}ronique --> Huet Kathy with interest 0.908949\n",
      "Delvaux V{\\'e}ronique --> Piccaluga Myriam with interest 0.908949\n",
      "Delvaux V{\\'e}ronique --> Harmegnies Bernard with interest 0.908949\n",
      "Huet Kathy --> Piccaluga Myriam with interest 0.999858\n",
      "Huet Kathy --> Harmegnies Bernard with interest 0.999858\n",
      "Piccaluga Myriam --> Harmegnies Bernard with interest 0.999858\n",
      "Wubben Ser --> Krahmer Emiel with interest 0.776969\n",
      "Liu Shih-Hung --> Chen Berlin with interest 0.832297\n",
      "Li Xiujun --> Gao Jianfeng with interest 0.998155\n",
      "Gupta Deepak --> Bhattacharyya Pushpak with interest 0.709885\n",
      "Shen Dinghan --> Carin Lawrence with interest 0.823189\n",
      "Miura Yasuhide --> Ohkuma Tomoko with interest 0.949603\n",
      "Kim Yunsu --> Ney Hermann with interest 0.996494\n",
      "Chamberlain Jon --> Kruschwitz Udo with interest 0.768904\n",
      "Tadepalli Prasad --> Fern Xiaoli with interest 0.833163\n",
      "Mishra Abhijit --> Bhattacharyya Pushpak with interest 0.709885\n",
      "Ure{\\v{s}}ov{\\'a} Zde{\\v{n}}ka --> Haji{\\v{c}} Jan with interest 0.771805\n",
      "Wang Xiaolin --> Sumita Eiichiro with interest 0.796707\n",
      "Ha Thanh-Le --> Niehues Jan with interest 0.749006\n",
      "El-Hajj Wassim --> Hajj Hazem with interest 0.768947\n",
      "Reisert Paul --> Inui Kentaro with interest 0.997956\n",
      "Hellrich Johannes --> Hahn Udo with interest 0.998850\n",
      "Lee Alan --> Prasad Rashmi with interest 0.832893\n",
      "Cocos Anne --> Callison-Burch Chris with interest 0.997771\n",
      "Basile Pierpaolo --> Semeraro Giovanni with interest 0.733163\n",
      "Montes Manuel --> Solorio Thamar with interest 0.832212\n",
      "Chen Jhih-Jie --> Chang Jason with interest 0.714059\n",
      "Dahlmeier Daniel --> Ng Hwee Tou with interest 0.771137\n",
      "Hsieh Yu-Lun --> Hsu Wen-Lian with interest 0.999248\n",
      "Miller Tristan --> Gurevych Iryna with interest 0.797019\n",
      "Kiritchenko Svetlana --> Mohammad Saif with interest 0.784962\n",
      "Romanov Alexey --> Rumshisky Anna with interest 0.927890\n",
      "Shibata Tomohide --> Kurohashi Sadao with interest 0.930480\n",
      "Qian Longhua --> Zhou Guodong with interest 0.998368\n",
      "Chang Yung-Chun --> Hsu Wen-Lian with interest 0.784962\n",
      "Sasaki Minoru --> Shinnou Hiroyuki with interest 0.951827\n",
      "Ziai Ramon --> Meurers Detmar with interest 0.908438\n",
      "Papavassiliou Vassilis --> Prokopidis Prokopis with interest 0.999702\n",
      "Alkhouli Tamer --> Ney Hermann with interest 0.871494\n",
      "Rojas-Barahona Lina M. --> Ga{\\v{s}}i{\\'c} Milica with interest 0.749546\n",
      "Galescu Lucian --> Allen James with interest 0.908367\n",
      "Ga{\\v{s}}i{\\'c} Milica --> Young Steve with interest 0.718111\n",
      "Sinha Manjira --> Dasgupta Tirthankar with interest 0.922750\n",
      "Kotani Katsunori --> Yoshimi Takehiko with interest 0.714087\n",
      "Chen Liang-Pu --> Wu Shih-Hung with interest 0.832794\n",
      "Barbieri Francesco --> Saggion Horacio with interest 0.732141\n",
      "Ronzano Francesco --> Saggion Horacio with interest 0.998808\n",
      "Hermann Karl Moritz --> Blunsom Phil with interest 0.763684\n",
      "Aufrant Lauriane --> Wisniewski Guillaume with interest 0.915858\n",
      "Aufrant Lauriane --> Yvon Fran{\\c{c}}ois with interest 0.998183\n",
      "Deng Li --> He Xiaodong with interest 0.845373\n",
      "Sammons Mark --> Roth Dan with interest 0.944217\n",
      "Paetzold Gustavo --> Specia Lucia with interest 0.836934\n",
      "Salakoski Tapio --> Ginter Filip with interest 0.845572\n",
      "Nicosia Massimo --> Moschitti Alessro with interest 0.844209\n",
      "Felt Paul --> Ringger Eric with interest 0.922665\n",
      "Felt Paul --> Seppi Kevin with interest 0.999588\n",
      "Patra Braja Gopal --> Das Dipankar with interest 0.999262\n",
      "Cho Eunah --> Niehues Jan with interest 0.739747\n",
      "Kamran Amir --> Bojar Ond{\\v{r}}ej with interest 0.831403\n",
      "Xiao Jianguo --> Wan Xiaojun with interest 0.998609\n",
      "Perez-Beltrachini Laura --> Gardent Claire with interest 0.725711\n",
      "Al-Badrashiny Mohamed --> Diab Mona with interest 0.810314\n",
      "Hawwari Abdelati --> Diab Mona with interest 0.931147\n",
      "Banjade Rajendra --> Rus Vasile with interest 0.999631\n",
      "Lu Zhengdong --> Li Hang with interest 0.811790\n",
      "Finch Andrew --> Sumita Eiichiro with interest 0.746707\n",
      "Liu Zhanyi --> Wu Hua with interest 0.998680\n",
      "Thomson Blaise --> Young Steve with interest 0.856504\n",
      "Wang Yu-Chun --> Tsai Richard Tzong-Han with interest 0.866212\n",
      "Sinha Manjira --> Basu Anupam with interest 0.768819\n",
      "Ghoneim Mahmoud --> Diab Mona with interest 0.997814\n",
      "Saers Markus --> Wu Dekai with interest 0.998453\n",
      "Mediani Mohammed --> Niehues Jan with interest 0.936506\n",
      "Wuebker Joern --> Ney Hermann with interest 0.812283\n",
      "Elfardy Heba --> Diab Mona with interest 0.712100\n",
      "Liu Zhanyi --> Wang Haifeng with interest 0.844763\n",
      "Navas Eva --> Saratxaga Ibon with interest 0.714144\n",
      "Gonz{\\'a}lez-Rubio Jes{\\'u}s --> Casacuberta Francisco with interest 0.817032\n",
      "Xu Liheng --> Liu Kang with interest 0.915545\n",
      "Mizuno Junta --> Inui Kentaro with interest 0.831289\n",
      "Mediani Mohammed --> Herrmann Teresa with interest 0.749659\n",
      "Mediani Mohammed --> Waibel Alex with interest 0.873197\n",
      "Toda Tomoki --> Neubig Graham with interest 0.901767\n",
      "Feng Minwei --> Ney Hermann with interest 0.905585\n",
      "Haertel Robbie --> Ringger Eric with interest 0.845742\n",
      "Haertel Robbie --> Seppi Kevin with interest 0.845742\n",
      "Taylor Sarah --> Shaikh Samira with interest 0.916170\n",
      "Taylor Sarah --> Strzalkowski Tomek with interest 0.915801\n",
      "Guo Weiwei --> Diab Mona with interest 0.747814\n",
      "Ganitkevitch Juri --> Callison-Burch Chris with interest 0.734614\n",
      "Yao Xuchen --> Van Durme Benjamin with interest 0.712369\n",
      "Liu Xiaohua --> Zhou Ming with interest 0.797331\n",
      "Cuay{\\'a}huitl Heriberto --> Dethlefs Nina with interest 0.799716\n",
      "Sofianopoulos Sokratis --> Vassiliou Marina with interest 0.833149\n",
      "Hanneman Greg --> Lavie Alon with interest 0.855638\n",
      "Zhang Longkai --> Wang Houfeng with interest 0.999177\n",
      "Janarthanam Srinivasan --> Lemon Oliver with interest 0.865517\n",
      "Huang Shu-Ling --> Chen Keh-Jiann with interest 0.732098\n",
      "Eck Matthias --> Waibel Alex with interest 0.880550\n",
      "Bai Ming-Hong --> Chen Keh-Jiann with interest 0.748765\n",
      "Costa Francisco --> Branco Ant{\\'o}nio with interest 0.999205\n",
      "Tablan Valentin --> Bontcheva Kalina with interest 0.713420\n",
      "De Saeger Stijn --> Torisawa Kentaro with interest 0.998950\n",
      "Kazama Jun{'}ichi --> Torisawa Kentaro with interest 0.905200\n",
      "Nicholson Jeremy --> Baldwin Timothy with interest 0.829714\n",
      "Amig{\\'o} Enrique --> Gonzalo Julio with interest 0.908693\n",
      "Verdejo Felisa --> Gonzalo Julio with interest 0.799603\n",
      "Lardilleux Adrien --> Lepage Yves with interest 0.799106\n",
      "Watanabe Yasuhiko --> Okada Yoshihiro with interest 0.749830\n",
      "Patry Alexre --> Langlais Philippe with interest 0.998836\n",
      "Ma Yanjun --> Way Andy with interest 0.788274\n",
      "Robba Isabelle --> Vilnat Anne with interest 0.777082\n",
      "Davidov Dmitry --> Rappoport Ari with interest 0.999120\n",
      "Nilsson Jens --> Nivre Joakim with interest 0.998070\n",
      "Hockey Beth Ann --> Rayner Manny with interest 0.832382\n",
      "Eck Matthias --> Vogel Stephan with interest 0.821798\n",
      "Zollmann Andreas --> Venugopal Ashish with interest 0.845955\n",
      "Bertagna Francesca --> Calzolari Nicoletta with interest 0.798935\n",
      "Ma Qing --> Isahara Hitoshi with interest 0.901281\n",
      "Zhou Liang --> Hovy Eduard with interest 0.729685\n",
      "Helmreich Stephen --> Farwell David with interest 0.732936\n",
      "Shimohata Mitsuo --> Sumita Eiichiro with interest 0.996707\n",
      "Macleod Catherine --> Grishman Ralph with interest 0.831261\n",
      "Gates Donna --> Lavie Alon with interest 0.798495\n",
      "Zampolli Antonio --> Calzolari Nicoletta with interest 0.704818\n",
      "Polifroni Joseph --> Seneff Stephanie with interest 0.777111\n",
      "Norton Lewis M. --> Dahl Deborah A. with interest 0.922750\n",
      "Della Pietra Vincent J. --> Della Pietra Stephen A. with interest 0.845998\n",
      "Della Pietra Stephen A. --> Brown Peter F. with interest 0.908906\n",
      "Della Pietra Vincent J. --> Brown Peter F. with interest 0.845969\n",
      "Mercer Robert L. --> Brown Peter F. with interest 0.856958\n",
      "Mercer Robert L. --> Della Pietra Stephen A. with interest 0.714130\n",
      "Della Pietra Vincent J. --> Mercer Robert L. with interest 0.922878\n",
      "Phillips Michael --> Zue Victor with interest 0.922807\n",
      "Phillips Michael --> Seneff Stephanie with interest 0.845487\n",
      "\n",
      "Support: 50\n",
      "Negri Matteo --> Turchi Marco with interest 0.751200\n",
      "Che Wanxiang --> Liu Ting with interest 0.862259\n",
      "Zhao Jun --> Liu Kang with interest 0.748879\n",
      "Osenova Petya --> Simov Kiril with interest 0.783721\n",
      "Kawahara Daisuke --> Kurohashi Sadao with interest 0.794900\n",
      "\n",
      "Support: 100\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "supports = [10, 50, 100]\n",
    "naive = []\n",
    "L2s = []\n",
    "for s in supports:\n",
    "    t = time()\n",
    "    L2 = {}\n",
    "    for key, n in C2.items():\n",
    "        if n >= s:\n",
    "            L2[key] = n    \n",
    "\n",
    "    L2 = [elem for elem in L2 if len(elem) > 1]  # clean our the list a bit.\n",
    "    L2s.append(L2)\n",
    "    t2 = round(time() - t,3)\n",
    "    naive.append(str(len(L2)) + ' items with > ' + str(s) + ' occurrences in ' + str(t2) + 's')\n",
    "    print(\"\\nSupport: {}\".format(s))\n",
    "    association_rules(sum(1 for line in open(authors_fname)), C1, C2, L2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1705 items with > 10 occurrences in 0.037s', '12 items with > 50 occurrences in 0.039s', '0 items with > 100 occurrences in 0.037s']\n"
     ]
    }
   ],
   "source": [
    "print(naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A-priori algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold =  10\n",
      "Took 0.5669822692871094s for k=1\n",
      "C1 contains 61387 items\n",
      "4154 of those items with > 10 occurrences\n",
      "C2 contains 37362 items\n",
      "A-priori: 1705 items with >10 occurrences\n",
      "\n",
      "Threshold =  50\n",
      "Took 0.26999974250793457s for k=1\n",
      "C1 contains 61387 items\n",
      "450 of those items with > 50 occurrences\n",
      "C2 contains 3506 items\n",
      "A-priori: 12 items with >50 occurrences\n",
      "\n",
      "Threshold =  100\n",
      "Took 0.3229794502258301s for k=1\n",
      "C1 contains 61387 items\n",
      "95 of those items with > 100 occurrences\n",
      "C2 contains 436 items\n",
      "A-priori: 0 items with >100 occurrences\n",
      "\n"
     ]
    }
   ],
   "source": [
    "supports = [10, 50, 100]\n",
    "apriori = []\n",
    "L2s = []\n",
    "for s in supports:\n",
    "\n",
    "    print('Threshold = ', s)\n",
    "    t = time()\n",
    "    C1 = get_C(1)\n",
    "    print(\"C1 contains {} items\".format(len(C1)))\n",
    "    \n",
    "    # filter stage\n",
    "    L1 = {}\n",
    "    for key, count in C1.items():\n",
    "        if count >= s:\n",
    "            L1[key] = count\n",
    "    \n",
    "    print('{} of those items with > {} occurrences'.format(len(L1),s))\n",
    "\n",
    "    C2_items = set([a.union(b) for a in L1.keys() for b in L1.keys()]) # List comprehensions in python\n",
    "    \n",
    "    # find frequent 2-tuples\n",
    "    C2 = {}\n",
    "    for key in readdata(k=2):\n",
    "        # filter out non-frequent tuples\n",
    "        if key not in C2_items:\n",
    "            continue\n",
    "\n",
    "        # record frequent tuples\n",
    "        if key not in C2:\n",
    "            C2[key] = 1\n",
    "        else:\n",
    "            C2[key] += 1\n",
    "    \n",
    "    print(\"C2 contains {} items\".format(len(C2)))\n",
    "\n",
    "    # filter stage\n",
    "    L2 = {}    \n",
    "    for key, count in C2.items():\n",
    "        if count >= s:\n",
    "            L2[key] = count        \n",
    "  \n",
    "    #print(L2)\n",
    "    t2 = round(time() - t,3)\n",
    "    print('A-priori: {} items with >{} occurrences\\n'.format(len(L2), s))    \n",
    "    apriori.append(str(len(L2)) + ' items with > ' + str(s) + ' occurrences in ' + str(t2) + 's')\n",
    "    L2s.append(L2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1705 items with > 10 occurrences in 52.205s', '12 items with > 50 occurrences in 3.75s', '0 items with > 100 occurrences in 0.622s']\n"
     ]
    }
   ],
   "source": [
    "print(apriori)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCY algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hash table\n",
    "max_hash1 = 10 * 1000000\n",
    "H1 = np.zeros((max_hash1, ), dtype=np.int64)\n",
    "\n",
    "for key in readdata(k=2):\n",
    "    hash_cell_1 = hash(key) % max_hash1\n",
    "    H1[hash_cell_1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold =  10\n",
      "Took 0.26100802421569824s for k=1\n",
      "C1 contains 61387 items\n",
      "4154 of those items with > 10 occurrences\n",
      "C2 contains 1741 items\n",
      "PCY: 1705 items with >1 occurrences\n",
      "\n",
      "Threshold =  50\n",
      "Took 0.2680017948150635s for k=1\n",
      "C1 contains 61387 items\n",
      "450 of those items with > 50 occurrences\n",
      "C2 contains 12 items\n",
      "PCY: 12 items with >1 occurrences\n",
      "\n",
      "Threshold =  100\n",
      "Took 0.26900196075439453s for k=1\n",
      "C1 contains 61387 items\n",
      "95 of those items with > 100 occurrences\n",
      "C2 contains 0 items\n",
      "PCY: 0 items with >1 occurrences\n",
      "\n"
     ]
    }
   ],
   "source": [
    "supports = [10, 50, 100]\n",
    "pcy = []\n",
    "L2s = []\n",
    "for s in supports:\n",
    "\n",
    "    print('Threshold = ', s)\n",
    "    t = time()\n",
    "    C1 = get_C(1)\n",
    "    print(\"C1 contains {} items\".format(len(C1)))\n",
    "    \n",
    "    L1 = {}\n",
    "    for key, count in C1.items():\n",
    "        if count >= s:\n",
    "            L1[key] = count\n",
    "    \n",
    "    print('{} of those items with > {} occurrences'.format(len(L1),s))\n",
    "\n",
    "    C2_items = set([a.union(b) for a in L1.keys() for b in L1.keys()]) # List comprehensions in python  \n",
    "\n",
    "    # find frequent 2-tuples\n",
    "    C2 = {}\n",
    "    N = 10\n",
    "    for key in readdata(k=2):\n",
    "    \n",
    "        # hash-based filtering stage from PCY\n",
    "        hash_cell_1 = hash(key) % max_hash1\n",
    "        if H1[hash_cell_1] < s:\n",
    "            continue\n",
    "\n",
    "        # filter out non-frequent tuples\n",
    "        if key not in C2_items:\n",
    "            continue\n",
    "\n",
    "        # record frequent tuples\n",
    "        if key not in C2:\n",
    "            C2[key] = 1\n",
    "        else:\n",
    "            C2[key] += 1\n",
    "        \n",
    "    print(\"C2 contains {} items\".format(len(C2)))\n",
    "\n",
    "    # filter stage\n",
    "    L2 = {}    \n",
    "    for key, count in C2.items():\n",
    "        if count >= s:\n",
    "            L2[key] = count\n",
    "            \n",
    "    t2 = round(time() - t,3)\n",
    "    print('PCY: {} items with >{} occurrences\\n'.format(len(L2), n))\n",
    "    pcy.append(str(len(L2)) + ' items with > ' + str(s) + ' occurrences in ' + str(t2) + 's')    \n",
    "    L2s.append(L2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Na誰ve: ['1705 items with > 10 occurrences in 0.037s', '12 items with > 50 occurrences in 0.039s', '0 items with > 100 occurrences in 0.037s']\n",
      "A-priori: ['1705 items with > 10 occurrences in 52.205s', '12 items with > 50 occurrences in 3.75s', '0 items with > 100 occurrences in 0.622s']\n",
      "PCY: ['1705 items with > 10 occurrences in 43.831s', '12 items with > 50 occurrences in 4.043s', '0 items with > 100 occurrences in 0.987s']\n"
     ]
    }
   ],
   "source": [
    "print('Na誰ve:', naive)\n",
    "print('A-priori:', apriori)\n",
    "print('PCY:', pcy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the results obtained are the same with the 3 methods but the number of items to count in C2 is less with A-priori algorithm and even more with the PCY algorithm which makes those algorithms better in terms of computational memory requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### 2. For the PCY algorithm, create up to 5 compact hash tables. What is  the difference in results and time of execution for 1,2,3,4 and 5 tables? Comment your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold = 10\n",
      "\n",
      "1705 items with > 10 occurrences found in 39.009 using 1 hashing functions\n",
      "1705 items with > 10 occurrences found in 39.489 using 2 hashing functions\n",
      "1705 items with > 10 occurrences found in 39.97 using 3 hashing functions\n",
      "1705 items with > 10 occurrences found in 40.475 using 4 hashing functions\n",
      "1705 items with > 10 occurrences found in 40.99 using 5 hashing functions\n",
      "Threshold = 50\n",
      "\n",
      "12 items with > 50 occurrences found in 5.645 using 1 hashing functions\n",
      "12 items with > 50 occurrences found in 6.042 using 2 hashing functions\n",
      "12 items with > 50 occurrences found in 6.428 using 3 hashing functions\n",
      "12 items with > 50 occurrences found in 6.812 using 4 hashing functions\n",
      "12 items with > 50 occurrences found in 7.199 using 5 hashing functions\n",
      "Threshold = 100\n",
      "\n",
      "0 items with > 100 occurrences found in 2.642 using 1 hashing functions\n",
      "0 items with > 100 occurrences found in 3.03 using 2 hashing functions\n",
      "0 items with > 100 occurrences found in 3.415 using 3 hashing functions\n",
      "0 items with > 100 occurrences found in 3.818 using 4 hashing functions\n",
      "0 items with > 100 occurrences found in 4.206 using 5 hashing functions\n"
     ]
    }
   ],
   "source": [
    "supports = [10, 50, 100]\n",
    "\n",
    "# Definie hash tables\n",
    "max_hash1 = 5*1000000-673\n",
    "max_hash2 = 5*1000000+673\n",
    "max_hash3 = 5*1000000-1673\n",
    "max_hash4 = 5*1000000+1673\n",
    "max_hash5 = 5*1000000+11673\n",
    "\n",
    "for s in supports:\n",
    "\n",
    "    t = time()\n",
    "    print('Threshold = {}\\n'.format(s))\n",
    "\n",
    "    H1 = np.zeros((max_hash1,), dtype=int)\n",
    "    H2 = np.zeros((max_hash2,), dtype=int)\n",
    "    H3 = np.zeros((max_hash3,), dtype=int)\n",
    "    H4 = np.zeros((max_hash4,), dtype=int)\n",
    "    H5 = np.zeros((max_hash5,), dtype=int)\n",
    "\n",
    "    for key in readdata(k=2):\n",
    "        hash_cell_1 = hash(key) % max_hash1\n",
    "        H1[hash_cell_1] += 1\n",
    "        hash_cell_2 = hash(key) % max_hash2\n",
    "        H2[hash_cell_2] += 1\n",
    "        hash_cell_3 = hash(key) % max_hash3\n",
    "        H3[hash_cell_3] += 1\n",
    "        hash_cell_4 = hash(key) % max_hash4\n",
    "        H4[hash_cell_4] += 1\n",
    "        hash_cell_5 = hash(key) % max_hash5\n",
    "        H5[hash_cell_5] += 1\n",
    "\n",
    "    # compact hash table\n",
    "    H_good_1 = set(np.where(H1 >= s)[0])\n",
    "    H_good_2 = set(np.where(H2 >= s)[0])\n",
    "    H_good_3 = set(np.where(H3 >= s)[0])\n",
    "    H_good_4 = set(np.where(H4 >= s)[0])\n",
    "    H_good_5 = set(np.where(H5 >= s)[0])\n",
    "\n",
    "    del H1\n",
    "    del H2\n",
    "    del H3\n",
    "    del H4\n",
    "    del H5\n",
    "\n",
    "    # find frequent 1-tuples (individual items)\n",
    "    C1 = {}\n",
    "    for key in readdata(k=1):\n",
    "        if key not in C1:\n",
    "            C1[key] = 1\n",
    "        else:\n",
    "            C1[key] += 1    \n",
    "\n",
    "    # filter stage\n",
    "    L1 = {}\n",
    "    for key, count in C1.items():\n",
    "        if count >= s:\n",
    "            L1[key] = count\n",
    "\n",
    "    C2_items = set([a.union(b) for a in L1.keys() for b in L1.keys()]) # List comprehensions in python\n",
    "\n",
    "    # find frequent 2-tuples\n",
    "    C2 = {}\n",
    "\n",
    "\n",
    "    for i in range (1,6):\n",
    "        for key in readdata(k=2):\n",
    "            # hash-based filtering stage from PCY\n",
    "            if i >= 1:\n",
    "                hash_cell_1 = hash(key) % max_hash1\n",
    "                if hash_cell_1 not in H_good_1:\n",
    "                    continue\n",
    "            if i >= 2:\n",
    "                hash_cell_2 = hash(key) % max_hash2\n",
    "                if hash_cell_2 not in H_good_2:\n",
    "                    continue\n",
    "            if i >= 3:\n",
    "                hash_cell_3 = hash(key) % max_hash3\n",
    "                if hash_cell_3 not in H_good_3:\n",
    "                    continue\n",
    "            if i >= 4:\n",
    "                hash_cell_4 = hash(key) % max_hash4\n",
    "                if hash_cell_4 not in H_good_4:\n",
    "                    continue\n",
    "            if i >= 5:\n",
    "                hash_cell_5 = hash(key) % max_hash5\n",
    "                if hash_cell_5 not in H_good_5:\n",
    "                    continue\n",
    "                    \n",
    "            # filter out non-frequent tuples\n",
    "            if key not in C2_items:\n",
    "                continue\n",
    "\n",
    "            # record frequent tuples\n",
    "            if key not in C2:\n",
    "                C2[key] = 1\n",
    "            else:\n",
    "                C2[key] += 1\n",
    "        \n",
    "        # filter stage\n",
    "        L2 = {}\n",
    "        for key, count in C2.items():\n",
    "            if count >= s:\n",
    "                L2[key] = count\n",
    "        t2 = round(time() - t,3)\n",
    "\n",
    "        print('{} items with > {} occurrences found in {} using {} hashing functions'.format(len(L2), s, t2, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that we do not improve the execution time by addig more hashing functions but we can appreciare that, by splitting memory among too many hash-tables, the hash-tables get smaller, resulting in more collisions and, too many collisions, may result in an unefficient filtering out of infrequent pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### 3. Find the final list of k-frequent items (k-tuples) for k=3 and 4. Experiment a bit and describe the best value for the support in each case. *Warning*: You can use any of the three algorithms, but be careful, because the algorithm can take too long if you don't chose it properly (well, basically don't use the na誰ve approach ;))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342 3-tuples items with > 10 occurrences found in 6.963\n",
      "103 4-tuples items with > 10 occurrences found in 12.85\n",
      "\n",
      "8 3-tuples items with > 20 occurrences found in 5.288\n",
      "0 4-tuples items with > 20 occurrences found in 10.566\n",
      "\n",
      "1 3-tuples items with > 30 occurrences found in 3.252\n",
      "0 4-tuples items with > 30 occurrences found in 11.234\n",
      "\n"
     ]
    }
   ],
   "source": [
    "supports = [10, 20, 30]\n",
    "k_tuples = [3, 4]\n",
    "\n",
    "for s in supports:\n",
    "    for k in k_tuples:\n",
    "\n",
    "        t = time()\n",
    "\n",
    "        # find frequent 1-tuples (individual items)\n",
    "        C1 = {}\n",
    "        for key in readdata(k=1):\n",
    "            if key not in C1:\n",
    "                C1[key] = 1\n",
    "            else:\n",
    "                C1[key] += 1    \n",
    "            \n",
    "        # filter stage\n",
    "        L1 = {}\n",
    "        for key, count in C1.items():\n",
    "            if count >= s:\n",
    "                L1[key] = count\n",
    "            \n",
    "        # find frequent 2-tuples    \n",
    "        C2 = {}\n",
    "        for key in readdata(k=2):\n",
    "        # record frequent tuples\n",
    "            if key not in C2:\n",
    "                C2[key] = 1\n",
    "            else:\n",
    "                C2[key] += 1\n",
    "\n",
    "        # filter stage\n",
    "        L2 = {}\n",
    "        for key, count in C2.items():\n",
    "            if count >= s:\n",
    "                L2[key] = count\n",
    "\n",
    "        if k == 3:\n",
    "\n",
    "            C3_items = set([a.union(b) for a in L2.keys() for b in L2.keys() ]) # List comprehensions in python\n",
    "\n",
    "            # Hash table\n",
    "            max_hash1 = 10 * 1000000\n",
    "            H1 = np.zeros((max_hash1, ), dtype=int)\n",
    "\n",
    "            for key in readdata(k=3):\n",
    "                hash_cell_1 = hash(key) % max_hash1\n",
    "                H1[hash_cell_1] += 1\n",
    "    \n",
    "            # find frequent 3-tuples\n",
    "            C3 = {}\n",
    "            for key in readdata(k=3):\n",
    "                # hash-based filtering stage from PCY\n",
    "                hash_cell_1 = hash(key) % max_hash1\n",
    "                if H1[hash_cell_1] < s:\n",
    "                    continue\n",
    "    \n",
    "                # filter out non-frequent tuples\n",
    "                if key not in C3_items:\n",
    "                    continue\n",
    "\n",
    "                # record frequent tuples\n",
    "                if key not in C3:\n",
    "                    C3[key] = 1\n",
    "                else:\n",
    "                    C3[key] += 1\n",
    "\n",
    "            # filter stage\n",
    "            L3 = {}\n",
    "            for key, count in C3.items():\n",
    "                if count >= s:\n",
    "                    L3[key] = count\n",
    "            t2 = round(time() - t,3)\n",
    "\n",
    "            print('{} {}-tuples items with > {} occurrences found in {}'.format(len(L3), k, s, t2))    \n",
    "\n",
    "        else:\n",
    "            # find frequent 3-tuples\n",
    "            C3 = {}\n",
    "            for key in readdata(k=3):            \n",
    "                # record frequent tuples\n",
    "                if key not in C3:\n",
    "                    C3[key] = 1\n",
    "                else:\n",
    "                    C3[key] += 1\n",
    "\n",
    "            # filter stage\n",
    "            L3 = {}\n",
    "            for key, count in C3.items():\n",
    "                if count >= s:\n",
    "                    L3[key] = count\n",
    "    \n",
    "            C4_items = set([a.union(b) for a in L3.keys() for b in L3.keys() ]) # List comprehensions in python  \n",
    "\n",
    "            # hash table\n",
    "            max_hash1 = 10 * 1000000\n",
    "            H1 = np.zeros((max_hash1, ), dtype=int)\n",
    "\n",
    "            for key in readdata(k=4):\n",
    "                hash_cell_1 = hash(key) % max_hash1\n",
    "                H1[hash_cell_1] += 1\n",
    "            \n",
    "            # find frequent 3-tuples\n",
    "            C4 = {}\n",
    "\n",
    "            for key in readdata(k=4):\n",
    "                # hash-based filtering stage from PCY\n",
    "                hash_cell_1 = hash(key) % max_hash1\n",
    "                if H1[hash_cell_1] < s:\n",
    "                    continue\n",
    "            \n",
    "                # filter out non-frequent tuples\n",
    "                if key not in C4_items:\n",
    "                    continue\n",
    "\n",
    "                # record frequent tuples\n",
    "                if key not in C4:\n",
    "                    C4[key] = 1\n",
    "                else:\n",
    "                    C4[key] += 1\n",
    "\n",
    "            # filter stage\n",
    "            L4 = {}\n",
    "            for key, count in C4.items():\n",
    "                if count >= s:\n",
    "                    L4[key] = count\n",
    "            t2 = round(time() - t,3)    \n",
    "            print('{} {}-tuples items with > {} occurrences found in {}\\n'.format(len(L4), k, s, t2))      \n",
    "                      \n",
    "            # saving result for next exercise\n",
    "            if s == 10:\n",
    "                result = L4   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### 4. Using one of the results of the previous items, for one k (k=2 or 3) find the possible clusters using the 1-NN criteria. Comment your results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
