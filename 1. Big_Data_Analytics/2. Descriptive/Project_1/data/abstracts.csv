,ID,title,abstract
0,boros-etal-2020-alleviating,"   Alleviating Digitization Errors in Named Entity Recognition for Historical Documents"",
",this paper tackles the task of named entity recognition (ner) applied to digitized historical texts obtained from processing digital images of newspapers using optical character recognition (ocr) techniques. we argue that the main challenge for this task is that the ocr process leads to misspellings and linguistic errors in the output text. moreover historical variations can be present in aged documents which can impact the performance of the ner process. we conduct a comparative evaluation on two historical datasets in german and french against previous state-of-the-art models and we propose a model based on a hierarchical stack of transformers to approach the ner task for historical data. our findings show that the proposed model clearly improves the results on both historical datasets and does not degrade the results for modern datasets.
1,mallart-etal-2020-relation,"   Relation, es-tu l{\`a} ? D{\'e}tection de relations par {LSTM} pour am{\'e}liorer l{'}extraction de relations (Relation, are you there ? {LSTM}-based relation detection to improve knowledge extraction )"",
",de nombreuses m{\'e}thodes d{'}extraction et de classification de relations ont {\'e}t{\'e} propos{\'e}es et test{\'e}es sur des donn{\'e}es de r{\'e}f{\'e}rence. cependant dans des donn{\'e}es r{\'e}elles le nombre de relations potentielles est {\'e}norme et les heuristiques souvent utilis{\'e}es pour distinguer de vraies relations de co-occurrences fortuites ne d{\'e}tectent pas les signaux faibles pourtant importants. dans cet article nous {\'e}tudions l{'}apport d{'}un mod{\`e}le de d{\'e}tection de relations identifiant si un couple d{'}entit{\'e}s dans une phrase exprime ou non une relation en tant qu{'}{\'e}tape pr{\'e}liminaire {\`a} la classification des relations. notre mod{\`e}le s{'}appuie sur le plus court chemin de d{\'e}pendances entre deux entit{\'e}s mod{\'e}lis{\'e} par un lstm et combin{\'e} avec les types des entit{\'e}s. sur la t{\^a}che de d{\'e}tection de relations nous obtenons de meilleurs r{\'e}sultats qu{'}un mod{\`e}le {\'e}tat de l{'}art pour la classification de relations avec une robustesse accrue aux relations in{\'e}dites. nous montrons aussi qu{'}une d{\'e}tection binaire en amont d{'}un mod{\`e}le de classification am{\'e}liore significativement ce dernier.
2,kafle-etal-2017-data,"   Data Augmentation for Visual Question Answering"",
",data augmentation is widely used to train deep neural networks for image classification tasks. simply flipping images can help learning tremendously by increasing the number of training images by a factor of two. however little work has been done studying data augmentation in natural language processing. here we describe two methods for data augmentation for visual question answering (vqa). the first uses existing semantic annotations to generate new questions. the second method is a generative approach using recurrent neural networks. experiments show that the proposed data augmentation improves performance of both baseline and state-of-the-art vqa algorithms.
3,dima-etal-2021-transformer,"   Transformer-based Multi-Task Learning for Adverse Effect Mention Analysis in Tweets"",
",this paper presents our contribution to the social media mining for health applications shared task 2021. we addressed all the three subtasks of task 1: subtask a (classification of tweets containing adverse effects) subtask b (extraction of text spans containing adverse effects) and subtask c (adverse effects resolution). we explored various pre-trained transformer-based language models and we focused on a multi-task training architecture. for the first subtask we also applied adversarial augmentation techniques and we formed model ensembles in order to improve the robustness of the prediction. our system ranked first at subtask b with 0.51 f1 score 0.514 precision and 0.514 recall. for subtask a we obtained 0.44 f1 score 0.49 precision and 0.39 recall and for subtask c we obtained 0.16 f1 score with 0.16 precision and 0.17 recall.
4,tarrade-etal-2017-typologies,"   Typologies pour l{'}annotation de textes non standard en fran{\c{c}}ais (Typologies for the annotation of non-standard {F}rench texts)"",
",la t{\^a}che de normalisation automatique des messages issus de la communication {\'e}lectronique m{\'e}di{\'e}e requiert une {\'e}tape pr{\'e}alable consistant {\`a} identifier les ph{\'e}nom{\`e}nes linguistiques. dans cet article nous proposons deux typologies pour l{'}annotation de textes non standard en fran{\c{c}}ais relevant respectivement des niveaux morpho-lexical et morpho-syntaxique. ces typologies ont {\'e}t{\'e} d{\'e}velopp{\'e}es en conciliant les typologies existantes et en les faisant {\'e}voluer en parall{\`e}le d{'}une annotation manuelle de tweets et de sms.
5,ciobanu-dinu-2016-computational,"   A Computational Perspective on the {R}omanian Dialects"",
",in this paper we conduct an initial study on the dialects of romanian. we analyze the differences between romanian and its dialects using the swadesh list. we analyze the predictive power of the orthographic and phonetic features of the words building a classification problem for dialect identification.
6,han-etal-2020-end,"   End-to-End Simultaneous Translation System for {IWSLT}2020 Using Modality Agnostic Meta-Learning"",
",in this paper we describe end-to-end simultaneous speech-to-text and text-to-text translation systems submitted to iwslt2020 online translation challenge. the systems are built by adding wait-k and meta-learning approaches to the transformer architecture. the systems are evaluated on different latency regimes. the simultaneous text-to-text translation achieved a bleu score of 26.38 compared to the competition baseline score of 14.17 on the low latency regime (average latency {\mbox{$\leq$}} 3). the simultaneous speech-to-text system improves the bleu score by 7.7 points over the competition baseline for the low latency regime (average latency {\mbox{$\leq$}} 1000).
7,nguyen-nguyen-2021-improving,"   Improving Cross-Lingual Transfer for Event Argument Extraction with Language-Universal Sentence Structures"",
",we study the problem of cross-lingual event argument extraction (ceae). the task aims to predict argument roles of entity mentions for events in text whose language is different from the language that a predictive model has been trained on. previous work on ceae has shown the cross-lingual benefits of universal dependency trees in capturing shared syntactic structures of sentences across languages. in particular this work exploits the existence of the syntactic connections between the words in the dependency trees as the anchor knowledge to transfer the representation learning across languages for ceae models (i.e. via graph convolutional neural networks {--} gcns). in this paper we introduce two novel sources of language-independent information for ceae models based on the semantic similarity and the universal dependency relations of the word pairs in different languages. we propose to use the two sources of information to produce shared sentence structures to bridge the gap between languages and improve the cross-lingual performance of the ceae models. extensive experiments are conducted with arabic chinese and english to demonstrate the effectiveness of the proposed method for ceae.
8,yadav-etal-2017-learning,"   Learning and Knowledge Transfer with Memory Networks for Machine Comprehension"",
",enabling machines to read and comprehend unstructured text remains an unfulfilled goal for nlp research. recent research efforts on the {``}machine comprehension{''} task have managed to achieve close to ideal performance on simulated data. however achieving similar levels of performance on small real world datasets has proved difficult; major challenges stem from the large vocabulary size complex grammar and the frequent ambiguities in linguistic structure. on the other hand the requirement of human generated annotations for training in order to ensure a sufficiently diverse set of questions is prohibitively expensive. motivated by these practical issues we propose a novel curriculum inspired training procedure for memory networks to improve the performance for machine comprehension with relatively small volumes of training data. additionally we explore various training regimes for memory networks to allow knowledge transfer from a closely related domain having larger volumes of labelled data. we also suggest the use of a loss function to incorporate the asymmetric nature of knowledge transfer. our experiments demonstrate improvements on dailymail cnn and mctest datasets.
9,vashishtha-etal-2019-fine,"   Fine-Grained Temporal Relation Extraction"",
",we present a novel semantic framework for modeling temporal relations and event durations that maps pairs of events to real-valued scales. we use this framework to construct the largest temporal relations dataset to date covering the entirety of the universal dependencies english web treebank. we use this dataset to train models for jointly predicting fine-grained temporal relations and event durations. we report strong results on our data and show the efficacy of a transfer-learning approach for predicting categorical relations.
10,bentivogli-etal-2020-gender,"   Gender in Danger? Evaluating Speech Translation Technology on the {M}u{ST}-{SHE} Corpus"",
",translating from languages without productive grammatical gender like english into gender-marked languages is a well-known difficulty for machines. this difficulty is also due to the fact that the training data on which models are built typically reflect the asymmetries of natural languages gender bias included. exclusively fed with textual data machine translation is intrinsically constrained by the fact that the input sentence does not always contain clues about the gender identity of the referred human entities. but what happens with speech translation where the input is an audio signal? can audio provide additional information to reduce gender bias? we present the first thorough investigation of gender bias in speech translation contributing with: i) the release of a benchmark useful for future studies and ii) the comparison of different technologies (cascade and end-to-end) on two language directions (english-italian/french).
11,balalau-horincar-2021-stage,"   From the Stage to the Audience: Propaganda on {R}eddit"",
",political discussions revolve around ideological conflicts that often split the audience into two opposing parties. both parties try to win the argument by bringing forward information. however often this information is misleading and its dissemination employs propaganda techniques. in this work we analyze the impact of propaganda on six major political forums on reddit that target a diverse audience in two countries the us and the uk. we focus on three research questions: who is posting propaganda? how does propaganda differ across the political spectrum? and how is propaganda received on political forums?
12,catizone-etal-2006-evaluating,"   Evaluating Automatically Generated Timelines from the Web"",
",as web searches increase there is a need to represent the search results in the most comprehensible way possible. in particular we focus on search results from queries about people and places. the standard method for presentation of search results is an ordered list determined by the web search engine. although this is satisfactory in some cases when searching for people and places presenting the information indexed by time may be more desirable. we are developing a system called cronopath which generates a timeline of web search engine results by determining the time frame of each document in the collection and linking elements in the timeline to the relevant articles. in this paper we propose evaluation guidelines for judging the quality of automatically generated timelines based on a set of common features.
13,nieminen-2021-opus,"   {OPUS}-{CAT}: Desktop {NMT} with {CAT} integration and local fine-tuning"",
",opus-cat is a collection of software which enables translators to use neural machine translation in computer-assisted translation tools without exposing themselves to security and confidentiality risks inherent in online machine translation. opus-cat uses the public opus-mt machine translation models which are available for over a thousand language pairs. the generic opus-mt models can be fine-tuned with opus-cat on the desktop using data for a specific client or domain.
14,stern-sagot-2010-detection,"   D{\'e}tection et r{\'e}solution d{'}entit{\'e}s nomm{\'e}es dans des d{\'e}p{\^e}ches d{'}agence"",
",nous pr{\'e}sentons np un syst{\`e}me de reconnaissance d{'}entit{\'e}s nomm{\'e}es. comprenant un module de r{\'e}solution il permet d{'}associer {\`a} chaque occurrence d{'}entit{\'e} le r{\'e}f{\'e}rent qu{'}elle d{\'e}signe parmi les entr{\'e}es d{'}un r{\'e}f{\'e}rentiel d{\'e}di{\'e}. np apporte ainsi des informations pertinentes pour l{'}exploitation de l{'}extraction d{'}entit{\'e}s nomm{\'e}es en contexte applicatif. ce syst{\`e}me fait l{'}objet d{'}une {\'e}valuation gr{\^a}ce au d{\'e}veloppement d{'}un corpus annot{\'e} manuellement et adapt{\'e} aux t{\^a}ches de d{\'e}tection et de r{\'e}solution.
15,iwamoto-etal-2021-polar,"   Polar Embedding"",
",hierarchical relationships are invaluable information for many natural language processing (nlp) tasks. distributional representation has become a fundamental approach for encoding word relationships particularly embeddings in hyperbolic space showed great performance in representing hierarchies by taking advantage of their spatial properties. however most machine learning systems do not suppose to use in such complex non-euclidean geometries. to achieve hierarchy representations in commonly used euclidean space we propose polar embedding that learns word embeddings with the polar coordinate system. utilizing characteristics of polar coordinates the hierarchy of words is expressed with two independent variables: radius (generality) and angles (similarity) and their variables are optimized separately. polar embedding shows word hierarchies explicitly and allows us to use beneficial resources such as word frequencies or word generality annotations for computing radiuses. we introduce an optimization method for learning angles in limited ranges of polar coordinates which combining a loss function controlling gradient and distribution uniformization. experimental results on hypernymy datasets indicate that our approach outperforms other embeddings in low-dimensional euclidean space and competitively performs even with hyperbolic embeddings which possess a geometric advantage.
16,gunes-etal-2016-structured,"   Structured Aspect Extraction"",
",aspect extraction identifies relevant features from a textual description of an entity e.g. a phone and is typically targeted to product descriptions reviews and other short texts as an enabling task for e.g. opinion mining and information retrieval. current aspect extraction methods mostly focus on aspect terms and often neglect interesting modifiers of the term or embed them in the aspect term without proper distinction. moreover flat syntactic structures are often assumed resulting in inaccurate extractions of complex aspects. this paper studies the problem of structured aspect extraction a variant of traditional aspect extraction aiming at a fine-grained extraction of complex (i.e. hierarchical) aspects. we propose an unsupervised and scalable method for structured aspect extraction consisting of statistical noun phrase clustering cpmi-based noun phrase segmentation and hierarchical pattern induction. our evaluation shows a substantial improvement over existing methods in terms of both quality and computational efficiency.
17,chi-etal-2017-speaker,"   Speaker Role Contextual Modeling for Language Understanding and Dialogue Policy Learning"",
",language understanding (lu) and dialogue policy learning are two essential components in conversational systems. human-human dialogues are not well-controlled and often random and unpredictable due to their own goals and speaking habits. this paper proposes a role-based contextual model to consider different speaker roles independently based on the various speaking patterns in the multi-turn dialogues. the experiments on the benchmark dataset show that the proposed role-based model successfully learns role-specific behavioral patterns for contextual encoding and then significantly improves language understanding and dialogue policy learning tasks.
18,shin-etal-2020-autoprompt,"   {A}uto{P}rompt: {E}liciting {K}nowledge from {L}anguage {M}odels with {A}utomatically {G}enerated {P}rompts"",
",the remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. reformulating tasks as fill-in-the-blanks problems (e.g. cloze tests) is a natural approach for gauging such knowledge however its usage is limited by the manual effort and guesswork required to write suitable prompts. to address this we develop autoprompt an automated method to create prompts for a diverse set of tasks based on a gradient-guided search. using autoprompt we show that masked language models (mlms) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning sometimes achieving performance on par with recent state-of-the-art supervised models. we also show that our prompts elicit more accurate factual knowledge from mlms than the manually created prompts on the lama benchmark and that mlms can be used as relation extractors more effectively than supervised relation extraction models. these results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods and as pretrained lms become more sophisticated and capable potentially a replacement for finetuning.
19,le-titov-2019-distant,"   Distant Learning for Entity Linking with Automatic Noise Detection"",
",accurate entity linkers have been produced for domains and languages where annotated data (i.e. texts linked to a knowledge base) is available. however little progress has been made for the settings where no or very limited amounts of labeled data are present (e.g. legal or most scientific domains). in this work we show how we can learn to link mentions without having any labeled examples only a knowledge base and a collection of unannotated texts from the corresponding domain. in order to achieve this we frame the task as a multi-instance learning problem and rely on surface matching to create initial noisy labels. as the learning signal is weak and our surrogate labels are noisy we introduce a noise detection component in our model: it lets the model detect and disregard examples which are likely to be noisy. our method jointly learning to detect noise and link entities greatly outperforms the surface matching baseline. for a subset of entity categories it even approaches the performance of supervised learning.
20,wang-culotta-2020-identifying,"   Identifying Spurious Correlations for Robust Text Classification"",
",the predictions of text classifiers are often driven by spurious correlations {--} e.g. the term {``}spielberg{''} correlates with positively reviewed movies even though the term itself does not semantically convey a positive sentiment. in this paper we propose a method to distinguish spurious and genuine correlations in text classification. we treat this as a supervised classification problem using features derived from treatment effect estimators to distinguish spurious correlations from {``}genuine{''} ones. due to the generic nature of these features and their small dimensionality we find that the approach works well even with limited training examples and that it is possible to transport the word classifier to new domains. experiments on four datasets (sentiment classification and toxicity detection) suggest that using this approach to inform feature selection also leads to more robust classification as measured by improved worst-case accuracy on the samples affected by spurious correlations.
21,loffler-etal-2020-tag,"   Tag Me If You Can! Semantic Annotation of Biodiversity Metadata with the {QEMP} Corpus and the {B}iodiv{T}agger"",
",dataset retrieval is gaining importance due to a large amount of research data and the great demand for reusing scientific data. dataset retrieval is mostly based on metadata structured information about the primary data. enriching these metadata with semantic annotations based on linked open data (lod) enables datasets publications and authors to be connected and expands the search on semantically related terms. in this work we introduce the biodivtagger an ontology-based information extraction pipeline developed for metadata from biodiversity research. the system recognizes biological physical and chemical processes environmental terms data parameters and phenotypes as well as materials and chemical compounds and links them to concepts in dedicated ontologies. to evaluate our pipeline we created a gold standard of 50 metadata files (qemp corpus) selected from five different data repositories in biodiversity research. to the best of our knowledge this is the first annotated metadata corpus for biodiversity research data. the results reveal a mixed picture. while materials and data parameters are properly matched to ontological concepts in most cases some ontological issues occurred for processes and environmental terms.
22,baziotis-etal-2017-datastories-semeval,"   {D}ata{S}tories at {S}em{E}val-2017 Task 4: Deep {LSTM} with Attention for Message-level and Topic-based Sentiment Analysis"",
",in this paper we present two deep-learning systems that competed at semeval-2017 task 4 {``}sentiment analysis in twitter{''}. we participated in all subtasks for english tweets involving message-level and topic-based sentiment polarity classification and quantification. we use long short-term memory (lstm) networks augmented with two kinds of attention mechanisms on top of word embeddings pre-trained on a big collection of twitter messages. also we present a text processing tool suitable for social network messages which performs tokenization word normalization segmentation and spell correction. moreover our approach uses no hand-crafted features or sentiment lexicons. we ranked 1st (tie) in subtask a and achieved very competitive results in the rest of the subtasks. both the word embeddings and our text processing tool are available to the research community.
23,peinelt-etal-2017-classifierguesser,"   {C}lassifier{G}uesser: A Context-based Classifier Prediction System for {C}hinese Language Learners"",
",classifiers are function words that are used to express quantities in chinese and are especially difficult for language learners. in contrast to previous studies we argue that the choice of classifiers is highly contextual and train context-aware machine learning models based on a novel publicly available dataset outperforming previous baselines. we further present use cases for our database and models in an interactive demo system.
24,k-roy-2021-calling,"   {``}Are you calling for the vaporizer you ordered?{''} Combining Search and Prediction to Identify Orders in Contact Centers"",
",with the growing footprint of ecommerce worldwide the role of contact center is becoming increasingly crucial for customer satisfaction. to effectively handle scale and manage operational cost automation through chat-bots and voice-bots are getting rapidly adopted. with customers having multiple often long list of active orders - the first task of a voice-bot is to identify which one they are calling about. towards solving this problem which we refer to as order identification we propose a two-staged real-time technique by combining search and prediction in a sequential manner. in the first stage analogous to retrieval-based question-answering a fuzzy search technique uses customized textual similarity measures on noisy transcripts of calls to retrieve the order of interest. the coverage of fuzzy search is limited by no or limited response from customers to voice prompts. hence in the second stage a predictive solution that predict the most likely order a customer is calling about based on certain features of orders is introduced. we compare with multiple relevant techniques based on word embeddings as well as ecommerce product search to show that the proposed approach provides the best performance with 64{\%} coverage and 87{\%} accuracy on a large real-life data-set. a system based on the proposed technique is also deployed in production for a fraction of calls landing in the contact center of a large ecommerce provider; providing real evidence of operational benefits as well as increased customer delight.
25,casas-etal-2020-combining,"   Combining Subword Representations into Word-level Representations in the Transformer Architecture"",
",in neural machine translation using word-level tokens leads to degradation in translation quality. the dominant approaches use subword-level tokens but this increases the length of the sequences and makes it difficult to profit from word-level information such as pos tags or semantic dependencies. we propose a modification to the transformer model to combine subword-level representations into word-level ones in the first layers of the encoder reducing the effective length of the sequences in the following layers and providing a natural point to incorporate extra word-level information. our experiments show that this approach maintains the translation quality with respect to the normal transformer model when no extra word-level information is injected and that it is superior to the currently dominant method for incorporating word-level source language information to models based on subword-level vocabularies.
26,thorsteinsson-etal-2019-wide,"   Where{'}s My Head? {D}efinition, Data Set, and Models for Numeric Fused-Head Identification and Resolution"",
",we provide the first computational treatment of fused-heads constructions (fhs) focusing on the numeric fused-heads (nfhs). fhs constructions are noun phrases in which the head noun is missing and is said to be {``}fused{''} with its dependent modifier. this missing information is implicit and is important for sentence understanding. the missing references are easily filled in by humans but pose a challenge for computational models. we formulate the handling of fhs as a two stages process: identification of the fh construction and resolution of the missing head. we explore the nfh phenomena in large corpora of english text and create (1) a data set and a highly accurate method for nfh identification; (2) a 10k examples (1 m tokens) crowd-sourced data set of nfh resolution; and (3) a neural baseline for the nfh resolution task. we release our code and data set to foster further research into this challenging problem.
27,uban-etal-2021-understanding,"   Analysis and Evaluation of Language Models for Word Sense Disambiguation"",
",abstract transformer-based language models have taken many fields in nlp by storm. bert and its derivatives dominate most of the existing evaluation benchmarks including those for word sense disambiguation (wsd) thanks to their ability in capturing context-sensitive semantic nuances. however there is still little knowledge about their capabilities and potential limitations in encoding and recovering word senses. in this article we provide an in-depth quantitative and qualitative analysis of the celebrated bert model with respect to lexical ambiguity. one of the main conclusions of our analysis is that bert can accurately capture high-level sense distinctions even when a limited number of examples is available for each word sense. our analysis also reveals that in some cases language models come close to solving coarse-grained noun disambiguation under ideal conditions in terms of availability of training data and computing resources. however this scenario rarely occurs in real-world settings and hence many practical challenges remain even in the coarse-grained setting. we also perform an in-depth comparison of the two main language model-based wsd strategies namely fine-tuning and feature extraction finding that the latter approach is more robust with respect to sense bias and it can better exploit limited available training data. in fact the simple feature extraction strategy of averaging contextualized embeddings proves robust even using only three training sentences per word sense with minimal improvements obtained by increasing the size of this training data.
28,su-etal-2021-shot-table,"   Few-Shot Table-to-Text Generation with Prototype Memory"",
",neural table-to-text generation models have achieved remarkable progress on an array of tasks. however due to the data-hungry nature of neural models their performances strongly rely on large-scale training examples limiting their applicability in real-world applications. to address this we propose a new framework: prototype-to-generate (p2g) for table-to-text generation under the few-shot scenario. the proposed framework utilizes the retrieved prototypes which are jointly selected by an ir system and a novel prototype selector to help the model bridging the structural gap between tables and texts. experimental results on three benchmark datasets with three state-of-the-art models demonstrate that the proposed framework significantly improves the model performance across various evaluation metrics.
29,janssen-2016-teitok,"   {TEITOK}: Text-Faithful Annotated Corpora"",
",teitok is a web-based framework for corpus creation annotation and distribution that combines textual and linguistic annotation within a single tei based xml document. teitok provides several built-in nlp tools to automatically (pre)process texts and is highly customizable. it features multiple orthographic transcription layers and a wide range of user-defined token-based annotations. for searching teitok interfaces with a local cqp server. teitok can handle various types of additional resources including facsimile images and linked audio files making it possible to have a combined written/spoken corpus. it also has additional modules for psdx syntactic annotation and several types of stand-off annotation.
30,lu-etal-2020-iscas,"   {ISCAS} at {S}em{E}val-2020 Task 5: Pre-trained Transformers for Counterfactual Statement Modeling"",
",iscas participated in two subtasks of semeval 2020 task 5: detecting counterfactual statements and detecting antecedent and consequence. this paper describes our system which is based on pretrained transformers. for the first subtask we train several transformer-based classifiers for detecting counterfactual statements. for the second subtask we formulate antecedent and consequence extraction as a query-based question answering problem. the two subsystems both achieved third place in the evaluation. our system is openly released at https://github.com/casnlu/iscassemeval2020task5.
31,zhang-etal-2021-noahqa-numerical,"   {NOAHQA}: Numerical Reasoning with Interpretable Graph Question Answering Dataset"",
",while diverse question answering (qa) datasets have been proposed and contributed significantly to the development of deep learning models for qa tasks the existing datasets fall short in two aspects. first we lack qa datasets covering complex questions that involve answers as well as the reasoning processes to get them. as a result the state-of-the-art qa research on numerical reasoning still focuses on simple calculations and does not provide the mathematical expressions or evidence justifying the answers. second the qa community has contributed a lot of effort to improve the interpretability of qa models. however they fail to explicitly show the reasoning process such as the evidence order for reasoning and the interactions between different pieces of evidence. to address the above shortcoming we introduce noahqa a conversational and bilingual qa dataset with questions requiring numerical reasoning with compound mathematical expressions. with noahqa we develop an interpretable reasoning graph as well as the appropriate evaluation metric to measure the answer quality. we evaluate the state-of-the-art qa models trained using existing qa datasets on noahqa and show that the best among them can only achieve 55.5 exact match scores while the human performance is 89.7. we also present a new qa model for generating a reasoning graph where the reasoning graph metric still has a large gap compared with that of humans eg 28 scores.
32,gong-etal-2020-rich,"   Rich Syntactic and Semantic Information Helps Unsupervised Text Style Transfer"",
",text style transfer aims to change an input sentence to an output sentence by changing its text style while preserving the content. previous efforts on unsupervised text style transfer only use the surface features of words and sentences. as a result the transferred sentences may either have inaccurate or missing information compared to the inputs. we address this issue by explicitly enriching the inputs via syntactic and semantic structures from which richer features are then extracted to better capture the original information. experiments on two text-style-transfer tasks show that our approach improves the content preservation of a strong unsupervised baseline model thereby demonstrating improved transfer performance.
33,kalyan-sangeetha-2020-medical,"   Medical Concept Normalization in User-Generated Texts by Learning Target Concept Embeddings"",
",medical concept normalization helps in discovering standard concepts in free-form text i.e. maps health-related mentions to standard concepts in a clinical knowledge base. it is much beyond simple string matching and requires a deep semantic understanding of concept mentions. recent research approach concept normalization as either text classification or text similarity. the main drawback in existing a) text classification approach is ignoring valuable target concepts information in learning input concept mention representation b) text similarity approach is the need to separately generate target concept embeddings which is time and resource consuming. our proposed model overcomes these drawbacks by jointly learning the representations of input concept mention and target concepts. first we learn input concept mention representation using roberta. second we find cosine similarity between embeddings of input concept mention and all the target concepts. here embeddings of target concepts are randomly initialized and then updated during training. finally the target concept with maximum cosine similarity is assigned to the input concept mention. our model surpasses all the existing methods across three standard datasets by improving accuracy up to 2.31{\%}.
34,agirre-soroa-2008-using,"   Using the Multilingual Central Repository for Graph-Based Word Sense Disambiguation"",
",this paper presents the results of a graph-based method for performing knowledge-based word sense disambiguation (wsd). the technique exploits the structural properties of the graph underlying the chosen knowledge base. the method is general in the sense that it is not tied to any particular knowledge base but in this work we have applied it to the multilingual central repository (mcr). the evaluation has been performed on the senseval-3 all-words task. the main contributions of the paper are twofold: (1) we have evaluated the separate and combined performance of each type of relation in the mcr and thus indirectly validated the contents of the mcr and their potential for wsd. (2) we obtain state-of-the-art results and in fact yield the best results that can be obtained using publicly available data.
35,yuwono-etal-2019-learning,"   Learning from the Experience of Doctors: Automated Diagnosis of Appendicitis Based on Clinical Notes"",
",the objective of this work is to develop an automated diagnosis system that is able to predict the probability of appendicitis given a free-text emergency department (ed) note and additional structured information (e.g. lab test results). our clinical corpus consists of about 180000 ed notes based on ten years of patient visits to the accident and emergency (a{\&}e) department of the national university hospital (nuh) singapore. we propose a novel neural network approach that learns to diagnose acute appendicitis based on doctors{'} free-text ed notes without any feature engineering. on a test set of 2000 ed notes with equal number of appendicitis (positive) and non-appendicitis (negative) diagnosis and in which all the negative ed notes only consist of abdominal-related diagnosis our model is able to achieve a promising f{\_}0.5-score of 0.895 while ed doctors achieve f{\_}0.5-score of 0.900. visualization shows that our model is able to learn important features signs and symptoms of patients from unstructured free-text ed notes which will help doctors to make better diagnosis.
36,guo-etal-2016-unified,"   A Unified Architecture for Semantic Role Labeling and Relation Classification"",
",this paper describes a unified neural architecture for identifying and classifying multi-typed semantic relations between words in a sentence. we investigate two typical and well-studied tasks: semantic role labeling (srl) which identifies the relations between predicates and arguments and relation classification (rc) which focuses on the relation between two entities or nominals. while mostly studied separately in prior work we show that the two tasks can be effectively connected and modeled using a general architecture. experiments on conll-2009 benchmark datasets show that our srl models significantly outperform state-of-the-art approaches. our rc models also yield competitive performance with the best published records. furthermore we show that the two tasks can be trained jointly with multi-task learning resulting in additive significant improvements for srl.
37,jin-schuler-2020-grounded,"   Grounded {PCFG} Induction with Images"",
",recent work in unsupervised parsing has tried to incorporate visual information into learning but results suggest that these models need linguistic bias to compete against models that only rely on text. this work proposes grammar induction models which use visual information from images for labeled parsing and achieve state-of-the-art results on grounded grammar induction on several languages. results indicate that visual information is especially helpful in languages where high frequency words are more broadly distributed. comparison between models with and without visual information shows that the grounded models are able to use visual information for proposing noun phrases gathering useful information from images for unknown words and achieving better performance at prepositional phrase attachment prediction.
38,dong-etal-2021-data,"   Data Augmentation with Adversarial Training for Cross-Lingual {NLI}"",
",due to recent pretrained multilingual representation models it has become feasible to exploit labeled data from one language to train a cross-lingual model that can then be applied to multiple new languages. in practice however we still face the problem of scarce labeled data leading to subpar results. in this paper we propose a novel data augmentation strategy for better cross-lingual natural language inference by enriching the data to reflect more diversity in a semantically faithful way. to this end we propose two methods of training a generative model to induce synthesized examples and then leverage the resulting data using an adversarial training regimen for more robustness. in a series of detailed experiments we show that this fruitful combination leads to substantial gains in cross-lingual inference.
39,luo-etal-2019-reading,"   Reading Like {HER}: Human Reading Inspired Extractive Summarization"",
",in this work we re-examine the problem of extractive text summarization for long documents. we observe that the process of extracting summarization of human can be divided into two stages: 1) a rough reading stage to look for sketched information and 2) a subsequent careful reading stage to select key sentences to form the summary. by simulating such a two-stage process we propose a novel approach for extractive summarization. we formulate the problem as a contextual-bandit problem and solve it with policy gradient. we adopt a convolutional neural network to encode gist of paragraphs for rough reading and a decision making policy with an adapted termination mechanism for careful reading. experiments on the cnn and dailymail datasets show that our proposed method can provide high-quality summaries with varied length and significantly outperform the state-of-the-art extractive methods in terms of rouge metrics.
40,xiao-etal-2019-grammatical,"   Grammatical Sequence Prediction for Real-Time Neural Semantic Parsing"",
",while sequence-to-sequence (seq2seq) models achieve state-of-the-art performance in many natural language processing tasks they can be too slow for real-time applications. one performance bottleneck is predicting the most likely next token over a large vocabulary; methods to circumvent this bottleneck are a current research topic. we focus specifically on using seq2seq models for semantic parsing where we observe that grammars often exist which specify valid formal representations of utterance semantics. by developing a generic approach for restricting the predictions of a seq2seq model to grammatically permissible continuations we arrive at a widely applicable technique for speeding up semantic parsing. the technique leads to a 74{\%} speed-up on an in-house dataset with a large vocabulary compared to the same neural model without grammatical restrictions
41,gong-etal-2019-enhanced,"   Enhanced Transformer Model for Data-to-Text Generation"",
",neural models have recently shown significant progress on data-to-text generation tasks in which descriptive texts are generated conditioned on database records. in this work we present a new transformer-based data-to-text generation model which learns content selection and summary generation in an end-to-end fashion. we introduce two extensions to the baseline transformer model: first we modify the latent representation of the input which helps to significantly improve the content correctness of the output summary; second we include an additional learning objective that accounts for content selection modelling. in addition we propose two data augmentation methods that succeed to further improve performance of the resulting generation models. evaluation experiments show that our final model outperforms current state-of-the-art systems as measured by different metrics: bleu content selection precision and content ordering. we made publicly available the transformer extension presented in this paper.
42,gretter-2012-focusing,"   Focusing language models for automatic speech recognition"",
",this paper describes a method for selecting text data from a corpus with the aim of training auxiliary language models (lms) for an automatic speech recognition (asr) system. a novel similarity score function is proposed which allows to score each document belonging to the corpus in order to select those with the highest scores for training auxiliary lms which are linearly interpolated with the baseline one. the similarity score function makes use of {''}similarity models{''} built from the automatic transcriptions furnished by earlier stages of the asr system while the documents selected for training auxiliary lms are drawn from the same set of data used to train the baseline lm used in the asr system. in this way the resulting interpolated lms are {''}focused{''} towards the output of the recognizer itself. the approach allows to improve word error rate measured on a task of spontaneous speech of about 3{\%} relative. it is important to note that a similar improvement has been obtained using an {''}in-domain{''} set of texts data not contained in the sources used to train the baseline lm. in addition we compared the proposed similarity score function with two other ones based on perplexity (pp) and on tfxidf (term frequency x inverse document frequency) vector space model. the proposed approach provides about the same performance as that based on tfxidf model but requires both lower computation and occupation memory.
43,cornelisse-2020-inferring,"   Inferring Neuroticism of {T}witter Users by Utilizing their Following Interests"",
",twitter is a medium where when used adequately users{'} interests can be derived from what he follows. this characteristic can make it attractive for a source of personality derivation. we set out to test the hypothesis that analogous to the lexical hypothesis which posits that word use should reveal personality following behavior on social media should reveal personality aspects. we used a two-step approach wherein the first stage we selected accounts for whom it was possible to infer personality profiles to some extent using available literature on personality and interests. on these accounts we trained a regression model and segmented the derived features using hierarchical cluster analysis. in the second stage we obtained a small sample of users{'} personalities via a questionnaire and tested whether the model from stage 1 correlated with the users from step 2. the the explained variance for the neurotic and neutral neuroticism groups indicated significant results (r2 = .131 p = .0205; r2 = .22 p = .0044). confirming the hypothesis that following behavior should be correlated with one{'}s interests and that interests are correlated with the neuroticism personality dimension.
44,bhattacharjee-etal-2020-bert,"   To {BERT} or Not to {BERT}: Comparing Task-specific and Task-agnostic Semi-Supervised Approaches for Sequence Tagging"",
",leveraging large amounts of unlabeled data using transformer-like architectures like bert has gained popularity in recent times owing to their effectiveness in learning general representations that can then be further fine-tuned for downstream tasks to much success. however training these models can be costly both from an economic and environmental standpoint. in this work we investigate how to effectively use unlabeled data: by exploring the task-specific semi-supervised approach cross-view training (cvt) and comparing it with task-agnostic bert in multiple settings that include domain and task relevant english data. cvt uses a much lighter model architecture and we show that it achieves similar performance to bert on a set of sequence tagging tasks with lesser financial and environmental impact.
45,denis-sagot-2010-exploitation,"   Exploitation d{'}une ressource lexicale pour la construction d{'}un {\'e}tiqueteur morpho-syntaxique {\'e}tat-de-l{'}art du fran{\c{c}}ais"",
",cet article pr{\'e}sente meltfr un {\'e}tiqueteur morpho-syntaxique automatique du fran{\c{c}}ais. il repose sur un mod{\`e}le probabiliste s{\'e}quentiel qui b{\'e}n{\'e}ficie d{'}informations issues d{'}un lexique exog{\`e}ne {\`a} savoir le lefff. evalu{\'e} sur le ftb meltfr atteint un taux de pr{\'e}cision de 97.75{\%} (91.36{\%} sur les mots inconnus) sur un jeu de 29 {\'e}tiquettes. ceci correspond {\`a} une diminution du taux d{'}erreur de 18{\%} (36.1{\%} sur les mots inconnus) par rapport au m{\^e}me mod{\`e}le sans couplage avec le lefff. nous {\'e}tudions plus en d{\'e}tail la contribution de cette ressource au travers de deux s{\'e}ries d{'}exp{\'e}riences. celles-ci font appara{\^\i}tre en particulier que la contribution des traits issus du lefff est de permettre une meilleure couverture ainsi qu{'}une mod{\'e}lisation plus fine du contexte droit des mots.
46,antoine-etal-2002-corpus,"   Corpus {OTG} et {ECOLE}{\_}{MASSY} : vers la constitution d{'}une collection de corpus francophones de dialogue oral diffus{\'e}s librement"",
",cet article pr{\'e}sente deux corpus francophones de dialogue oral (otg et ecole{\_}massy) mis librement {\`a} la disposition de la communaut{\'e} scientifique. ces deux corpus constituent la premi{\`e}re livraison du projet parole publique initi{\'e} par le laboratoire valoria. ce projet vise la constitution d{'}une collection de corpus de dialogue oral enrichis par annotation morpho-syntaxique. ces corpus de dialogue finalis{\'e} sont essentiellement destin{\'e}s {\`a} une utilisation en communication homme-machine.
47,suzuki-etal-2006-web,"   On the Web Trilingual Sign Language Dictionary to Learn the foreign Sign Language without Learning a Target Spoken Language"",
",this paper describes a trilingual sign language dictionary (japanese sign language and american sign language and korean sign language) which helps those who learn each sign language directly from their mother sign language. our discussion covers two main points. the first describes the necessity of a trilingual dictionary. since there is no universal sign language or real international sign language deaf people should learn at least four languages: they want to talk to people whose mother tongue is different from their owns the mother sign language the mother spoken language as the first intermediate language the target spoken language as the second intermediate language and the sign language in which they want to communicate. those two spoken languages become language barriers for deaf people and our trilingual dictionary will remove the barrier. the second describes the use of computer. as the use of computers becomes widespread it is increasingly convenient to study through computer software or internet facilities. our www dictionary system provides deaf people with an easy means of access using their mother-sign language which means they don't have to overcome the barrier of learning a foreign spoken language. it also provides a way for people who are going to learn three sign languages to look up new vocabulary. we are further planning to examine how our dictionary system could be used to educate and assist deaf people.
48,chandrahas-etal-2018-towards,"   Towards Understanding the Geometry of Knowledge Graph Embeddings"",
",knowledge graph (kg) embedding has emerged as a very active area of research over the last few years resulting in the development of several embedding methods. these kg embedding methods represent kg entities and relations as vectors in a high-dimensional space. despite this popularity and effectiveness of kg embeddings in various tasks (e.g. link prediction) geometric understanding of such embeddings (i.e. arrangement of entity and relation vectors in vector space) is unexplored {--} we fill this gap in the paper. we initiate a study to analyze the geometry of kg embeddings and correlate it with task performance and other hyperparameters. to the best of our knowledge this is the first study of its kind. through extensive experiments on real-world datasets we discover several insights. for example we find that there are sharp differences between the geometry of embeddings learnt by different classes of kg embeddings methods. we hope that this initial study will inspire other follow-up research on this important but unexplored problem.
49,bagherbeygi-shamsfard-2012-corpus,"   Corpus based Semi-Automatic Extraction of {P}ersian Compound Verbs and their Relations"",
",nowadays wordnet is used in natural language processing as one of the major linguistic resources. having such a resource for persian language helps researchers in computational linguistics and natural language processing fields to develop more accurate systems with higher performances. in this research we propose a model for semi-automatic construction of persian wordnet of verbs. compound verbs are a very productive structure in persian and number of compound verbs is much greater than simple verbs in this language this research is aimed at finding the structure of persian compound verbs and the relations between verb components. the main idea behind developing this system is using the wordnet of other pos categories (here means noun and adjective) to extract persian compound verbs their synsets and their relations. this paper focuses on three main tasks: 1.extracting compound verbs 2.extracting verbal synsets and 3.extracting the relations among verbal synsets such as hypernymy antonymy and cause.
50,higashiyama-etal-2019-incorporating,"   Incorporating Word Attention into Character-Based Word Segmentation"",
",neural network models have been actively applied to word segmentation especially chinese because of the ability to minimize the effort in feature engineering. typical segmentation models are categorized as character-based for conducting exact inference or word-based for utilizing word-level information. we propose a character-based model utilizing word information to leverage the advantages of both types of models. our model learns the importance of multiple candidate words for a character on the basis of an attention mechanism and makes use of it for segmentation decisions. the experimental results show that our model achieves better performance than the state-of-the-art models on both japanese and chinese benchmark datasets.
51,yim-etal-2017-annotation,"   Annotation of pain and anesthesia events for surgery-related processes and outcomes extraction"",
",pain and anesthesia information are crucial elements to identifying surgery-related processes and outcomes. however pain is not consistently recorded in the electronic medical record. even when recorded the rich complex granularity of the pain experience may be lost. similarly anesthesia information is recorded using local electronic collection systems; though the accuracy and completeness of the information is unknown. we propose an annotation schema to capture pain pain management and anesthesia event information.
52,nakov-etal-2017-semeval,"   {S}em{E}val-2017 Task 3: Community Question Answering"",
",we describe semeval{--}2017 task 3 on community question answering. this year we reran the four subtasks from semeval-2016: (a) question{--}comment similarity (b) question{--}question similarity (c) question{--}external comment similarity and (d) rerank the correct answers for a new question in arabic providing all the data from 2015 and 2016 for training and fresh data for testing. additionally we added a new subtask e in order to enable experimentation with multi-domain question duplicate detection in a larger-scale scenario using stackexchange subforums. a total of 23 teams participated in the task and submitted a total of 85 runs (36 primary and 49 contrastive) for subtasks a{--}d. unfortunately no teams participated in subtask e. a variety of approaches and features were used by the participating systems to address the different subtasks. the best systems achieved an official score (map) of 88.43 47.22 15.46 and 61.16 in subtasks a b c and d respectively. these scores are better than the baselines especially for subtasks a{--}c.
53,marie-max-2013-study,"   A study in greedy oracle improvement of translation hypotheses"",
",this paper describes a study of translation hypotheses that can be obtained by iterative greedy oracle improvement from the best hypothesis of a state-of-the-art phrase-based statistical machine translation system. the factors that we consider include the influence of the rewriting operations target languages and training data sizes. analysis of our results provide new insights into some previously unanswered questions which include the reachability of previously unreachable hypotheses via indirect translation (thanks to the introduction of a rewrite operation on the source text) and the potential translation performance of systems relying on pruned phrase tables.
54,tavernier-etal-2008-holy,"   Holy {M}oses! Leveraging Existing Tools and Resources for Entity Translation"",
",recently there has been an emphasis on creating shared resources for natural language processing applications. this has resulted in the development of high-quality tools and data which can then be leveraged by the research community as components for novel systems. in this paper we reuse an open source machine translation framework to create an arabic-to-english entity translation system. the system first translates known entity mentions using a standard phrase-based statistical machine translation framework which is then reused to perform name transliteration on unknown mentions. in order to transliterate names more accurately we introduce an algorithm to augment a names database with name origin and frequency information from existing data resources. origin information is used to learn name origin classifiers and origin-specific transliteration models while frequency information is used to select amongst n-best transliteration candidates. this work demonstrates the feasibility and benefit of adapting such data resources and shows how off-the-shelf tools and data resources can be repurposed to rapidly create a system outside their original domain.
55,fan-etal-2018-multi,"   Multi-grained Attention Network for Aspect-Level Sentiment Classification"",
",we propose a novel multi-grained attention network (mgan) model for aspect level sentiment classification. existing approaches mostly adopt coarse-grained attention mechanism which may bring information loss if the aspect has multiple words or larger context. we propose a fine-grained attention mechanism which can capture the word-level interaction between aspect and context. and then we leverage the fine-grained and coarse-grained attention mechanisms to compose the mgan framework. moreover unlike previous works which train each aspect with its context separately we design an aspect alignment loss to depict the aspect-level interactions among the aspects that have the same context. we evaluate the proposed approach on three datasets: laptop and restaurant are from semeval 2014 and the last one is a twitter dataset. experimental results show that the multi-grained attention network consistently outperforms the state-of-the-art methods on all three datasets. we also conduct experiments to evaluate the effectiveness of aspect alignment loss which indicates the aspect-level interactions can bring extra useful information and further improve the performance.
56,sirajzade-etal-2020-annotation,"   An Annotation Framework for {L}uxembourgish Sentiment Analysis"",
",the aim of this paper is to present a framework developed for crowdsourcing sentiment annotation for the low-resource language luxembourgish. our tool is easily accessible through a web interface and facilitates sentence-level annotation of several annotators in parallel. in the heart of our framework is an xml database which serves as central part linking several components. the corpus in the database consists of news articles and user comments. one of the components is luna a tool for linguistic preprocessing of the data set. it tokenizes the text splits it into sentences and assigns pos-tags to the tokens. after that the preprocessed text is stored in xml format into the database. the sentiment annotation tool which is a browser-based tool then enables the annotation of split sentences from the database. the sentiment engine a separate module is trained with this material in order to annotate the whole data set and analyze the sentiment of the comments over time and in relationship to the news articles. the gained knowledge can again be used to improve the sentiment classification on the one hand and on the other hand to understand the sentiment phenomenon from the linguistic point of view.
57,zhao-tsujii-1999-transfer,"   Transfer in experience-guided machine translation"",
",experience-guided machine translation (egmt) seeks to represent the translators' knowledge of translation as experiences and translates by analogy. the transfer in egmt finds the experiences most similar to a new text and its parts segments it into units of translation and translates them by analogy to the experiences and then assembles them into a whole. a research prototype of analogical transfer from chinese to english is built to prove the viability of the approach in the exploration of new architecture of machine translation. the paper discusses how the experiences are represented and selected with respect to a new text. it describes how units of translation are defined partial translation is derived and composed into a whole.
58,vempala-etal-2018-determining,"   Determining Event Durations: Models and Error Analysis"",
",this paper presents models to predict event durations. we introduce aspectual features that capture deeper linguistic information than previous work and experiment with neural networks. our analysis shows that tense aspect and temporal structure of the clause provide useful clues and that an lstm ensemble captures relevant context around the event.
59,popovic-2021-nature,"   On nature and causes of observed {MT} errors"",
",this work describes analysis of nature and causes of mt errors observed by different evaluators under guidance of different quality criteria: adequacy and comprehension and and a not specified generic mixture of adequacy and fluency. we report results for three language pairs and two domains and eleven mt systems. our findings indicate that and despite the fact that some of the identified phenomena depend on domain and/or language and the following set of phenomena can be considered as generally challenging for modern mt systems: rephrasing groups of words and translation of ambiguous source words and translating noun phrases and and mistranslations. furthermore and we show that the quality criterion also has impact on error perception. our findings indicate that comprehension and adequacy can be assessed simultaneously by different evaluators and so that comprehension and as an important quality criterion and can be included more often in human evaluations.
60,yu-etal-2016-even,"   If You {E}ven Don{'}t Have a Bit of {B}ible: Learning Delexicalized {POS} Taggers"",
",part-of-speech (pos) induction is one of the most popular tasks in research on unsupervised nlp. various unsupervised and semi-supervised methods have been proposed to tag an unseen language. however many of them require some partial understanding of the target language because they rely on dictionaries or parallel corpora such as the bible. in this paper we propose a different method named delexicalized tagging for which we only need a raw corpus of the target language. we transfer tagging models trained on annotated corpora of one or more resource-rich languages. we employ language-independent features such as word length frequency neighborhood entropy character classes (alphabetic vs. numeric vs. punctuation) etc. we demonstrate that such features can to certain extent serve as predictors of the part of speech represented by the universal pos tag.
61,stein-donatelli-2021-representing,"   Representing Implicit Positive Meaning of Negated Statements in {AMR}"",
",abstract meaning representation (amr) has become popular for representing the meaning of natural language in graph structures. however amr does not represent scope information posing a problem for its overall expressivity and specifically for drawing inferences from negated statements. this is the case with so-called {``}positive interpretations{''} of negated statements in which implicit positive meaning is identified by inferring the opposite of the negation{'}s focus. in this work we investigate how potential positive interpretations (ppis) can be represented in amr. we propose a logically motivated amr structure for ppis that makes the focus of negation explicit and sketch an initial proposal for a systematic methodology to generate this more expressive structure.
62,vulic-etal-2017-word,"   Word Vector Space Specialisation"",
",specialising vector spaces to maximise their content with respect to one key property of vector space models (e.g. semantic similarity vs. relatedness or lexical entailment) while mitigating others has become an active and attractive research topic in representation learning. such specialised vector spaces support different classes of nlp problems. proposed approaches fall into two broad categories: a) unsupervised methods which learn from raw textual corpora in more sophisticated ways (e.g. using context selection extracting co-occurrence information from word patterns attending over contexts); and b) knowledge-base driven approaches which exploit available resources to encode external information into distributional vector spaces injecting knowledge from semantic lexicons (e.g. wordnet framenet ppdb). in this tutorial we will introduce researchers to state-of-the-art methods for constructing vector spaces specialised for a broad range of downstream nlp applications. we will deliver a detailed survey of the proposed methods and discuss best practices for intrinsic and application-oriented evaluation of such vector spaces.throughout the tutorial we will provide running examples reaching beyond english as the only (and probably the easiest) use-case language in order to demonstrate the applicability and modelling challenges of current representation learning architectures in other languages.
63,ambroselli-etal-2018-prediction,"   Prediction for the Newsroom: Which Articles Will Get the Most Comments?"",
",the overwhelming success of the web and mobile technologies has enabled millions to share their opinions publicly at any time. but the same success also endangers this freedom of speech due to closing down of participatory sites misused by individuals or interest groups. we propose to support manual moderation by proactively drawing the attention of our moderators to article discussions that most likely need their intervention. to this end we predict which articles will receive a high number of comments. in contrast to existing work we enrich the article with metadata extract semantic and linguistic features and exploit annotated data from a foreign language corpus. our logistic regression model improves f1-scores by over 80{\%} in comparison to state-of-the-art approaches.
64,chowdhury-etal-2020-multi,"   A Multi-Platform {A}rabic News Comment Dataset for Offensive Language Detection"",
",access to social media often enables users to engage in conversation with limited accountability. this allows a user to share their opinions and ideology especially regarding public content occasionally adopting offensive language. this may encourage hate crimes or cause mental harm to targeted individuals or groups. hence it is important to detect offensive comments in social media platforms. typically most studies focus on offensive commenting in one platform only even though the problem of offensive language is observed across multiple platforms. therefore in this paper we introduce and make publicly available a new dialectal arabic news comment dataset collected from multiple social media platforms including twitter facebook and youtube. we follow two-step crowd-annotator selection criteria for low-representative language annotation task in a crowdsourcing platform. furthermore we analyze the distinctive lexical content along with the use of emojis in offensive comments. we train and evaluate the classifiers using the annotated multi-platform dataset along with other publicly available data. our results highlight the importance of multiple platform dataset for (a) cross-platform (b) cross-domain and (c) cross-dialect generalization of classifier performance.
65,polignano-etal-2019-swap,"   {SWAP} at {S}em{E}val-2019 Task 3: Emotion detection in conversations through Tweets, {CNN} and {LSTM} deep neural networks"",
",emotion detection from user-generated contents is growing in importance in the area of natural language processing. the approach we proposed for the emocontext task is based on the combination of a cnn and an lstm using a concatenation of word embeddings. a stack of convolutional neural networks (cnn) is used for capturing the hierarchical hidden relations among embedding features. meanwhile a long short-term memory network (lstm) is used for capturing information shared among words of the sentence. each conversation has been formalized as a list of word embeddings in particular during experimental runs pre-trained glove and google word embeddings have been evaluated. surface lexical features have been also considered but they have been demonstrated to be not usefully for the classification in this specific task. the final system configuration achieved a micro f1 score of 0.7089. the python code of the system is fully available at https://github.com/marcopoli/emocontext2019
66,hashimoto-tsuruoka-2017-neural,"   Neural Machine Translation with Source-Side Latent Graph Parsing"",
",this paper presents a novel neural machine translation model which jointly learns translation and source-side latent graph representations of sentences. unlike existing pipelined approaches using syntactic parsers our end-to-end model learns a latent graph parser as part of the encoder of an attention-based neural machine translation model and thus the parser is optimized according to the translation objective. in experiments we first show that our model compares favorably with state-of-the-art sequential and pipelined syntax-based nmt models. we also show that the performance of our model can be further improved by pre-training it with a small amount of treebank annotations. our final ensemble model significantly outperforms the previous best models on the standard english-to-japanese translation dataset.
67,skeppstedt-etal-2017-automatic,"   Automatic detection of stance towards vaccination in online discussion forums"",
",a classifier for automatic detection of stance towards vaccination in online forums was trained and evaluated. debate posts from six discussion threads on the british parental website mumsnet were manually annotated for stance {`}against{'} or {`}for{'} vaccination or as {`}undecided{'}. a support vector machine trained to detect the three classes achieved a macro f-score of 0.44 while a macro f-score of 0.62 was obtained by the same type of classifier on the binary classification task of distinguishing stance {`}against{'} vaccination from stance {`}for{'} vaccination. these results show that vaccine stance detection in online forums is a difficult task at least for the type of model investigated and for the relatively small training corpus that was used. future work will therefore include an expansion of the training data and an evaluation of other types of classifiers and features.
68,gupta-etal-2020-compositionality,"   Compositionality and Capacity in Emergent Languages"",
",recent works have discussed the extent to which emergent languages can exhibit properties of natural languages particularly learning compositionality. in this paper we investigate the learning biases that affect the efficacy and compositionality in multi-agent communication in addition to the communicative bandwidth. our foremost contribution is to explore how the capacity of a neural network impacts its ability to learn a compositional language. we additionally introduce a set of evaluation metrics with which we analyze the learned languages. our hypothesis is that there should be a specific range of model capacity and channel bandwidth that induces compositional structure in the resulting language and consequently encourages systematic generalization. while we empirically see evidence for the bottom of this range we curiously do not find evidence for the top part of the range and believe that this is an open question for the community.
69,leaman-lu-2020-comprehensive,"   A Comprehensive Dictionary and Term Variation Analysis for {COVID}-19 and {SARS}-{C}o{V}-2"",
",the number of unique terms in the scientific literature used to refer to either sars-cov-2 or covid-19 is remarkably large and has continued to increase rapidly despite well-established standardized terms. this high degree of term variation makes high recall identification of these important entities difficult. in this manuscript we present an extensive dictionary of terms used in the literature to refer to sars-cov-2 and covid-19. we use a rule-based approach to iteratively generate new term variants then locate these variants in a large text corpus. we compare our dictionary to an extensive collection of terminological resources demonstrating that our resource provides a substantial number of additional terms. we use our dictionary to analyze the usage of sars-cov-2 and covid-19 terms over time and show that the number of unique terms continues to grow rapidly. our dictionary is freely available at https://github.com/ncbi-nlp/covidtermvar.
70,weiss-meurers-2018-modeling,"   Modeling the Readability of {G}erman Targeting Adults and Children: An empirically broad analysis and its cross-corpus validation"",
",we analyze two novel data sets of german educational media texts targeting adults and children. the analysis is based on 400 automatically extracted measures of linguistic complexity from a wide range of linguistic domains. we show that both data sets exhibit broad linguistic adaptation to the target audience which generalizes across both data sets. our most successful binary classification model for german readability robustly shows high accuracy between 89.4{\%}{--}98.9{\%} for both data sets. to our knowledge this comprehensive german readability model is the first for which robust cross-corpus performance has been shown. the research also contributes resources for german readability assessment that are externally validated as successful for different target audiences: we compiled a new corpus of german news broadcast subtitles the tagesschau/logo corpus and crawled a geo/geolino corpus substantially enlarging the data compiled by hancke et al. 2012.
71,vuppuluri-etal-2017-ice,"   {ICE}: Idiom and Collocation Extractor for Research and Education"",
",collocation and idiom extraction are well-known challenges with many potential applications in natural language processing (nlp). our experimental open-source software system called ice is a python package for flexibly extracting collocations and idioms currently in english. it also has a competitive pos tagger that can be used alone or as part of collocation/idiom extraction. ice is available free of cost for research and educational uses in two user-friendly formats. this paper gives an overview of ice and its performance and briefly describes the research underlying the extraction algorithms.
72,gervits-scheutz-2018-pardon,"   Pardon the Interruption: Managing Turn-Taking through Overlap Resolution in Embodied Artificial Agents"",
",speech overlap is a common phenomenon in natural conversation and in task-oriented interactions. as human-robot interaction (hri) becomes more sophisticated the need to effectively manage turn-taking and resolve overlap becomes more important. in this paper we introduce a computational model for speech overlap resolution in embodied artificial agents. the model identifies when overlap has occurred and uses timing information dialogue history and the agent{'}s goals to generate context-appropriate behavior. we implement this model in a nao robot using the diarc cognitive robotic architecture. the model is evaluated on a corpus of task-oriented human dialogue and we find that the robot can replicate many of the most common overlap resolution behaviors found in the human data.
73,liu-etal-2019-rhetorically,"   Rhetorically Controlled Encoder-Decoder for {M}odern {C}hinese Poetry Generation"",
",rhetoric is a vital element in modern poetry and plays an essential role in improving its aesthetics. however to date it has not been considered in research on automatic poetry generation. in this paper we propose a rhetorically controlled encoder-decoder for modern chinese poetry generation. our model relies on a continuous latent variable as a rhetoric controller to capture various rhetorical patterns in an encoder and then incorporates rhetoric-based mixtures while generating modern chinese poetry. for metaphor and personification an automated evaluation shows that our model outperforms state-of-the-art baselines by a substantial margin while human evaluation shows that our model generates better poems than baseline methods in terms of fluency coherence meaningfulness and rhetorical aesthetics.
74,koufakou-etal-2020-hurtbert,"   {H}urt{BERT}: Incorporating Lexical Features with {BERT} for the Detection of Abusive Language"",
",the detection of abusive or offensive remarks in social texts has received significant attention in research. in several related shared tasks bert has been shown to be the state-of-the-art. in this paper we propose to utilize lexical features derived from a hate lexicon towards improving the performance of bert in such tasks. we explore different ways to utilize the lexical features in the form of lexicon-based encodings at the sentence level or embeddings at the word level. we provide an extensive dataset evaluation that addresses in-domain as well as cross-domain detection of abusive content to render a complete picture. our results indicate that our proposed models combining bert with lexical features help improve over a baseline bert model in many of our in-domain and cross-domain experiments.
75,chang-etal-2018-distributional,"   Distributional Inclusion Vector Embedding for Unsupervised Hypernymy Detection"",
",modeling hypernymy such as poodle is-a dog is an important generalization aid to many nlp tasks such as entailment relation extraction and question answering. supervised learning from labeled hypernym sources such as wordnet limits the coverage of these models which can be addressed by learning hypernyms from unlabeled text. existing unsupervised methods either do not scale to large vocabularies or yield unacceptably poor accuracy. this paper introduces distributional inclusion vector embedding (dive) a simple-to-implement unsupervised method of hypernym discovery via per-word non-negative vector embeddings which preserve the inclusion property of word contexts. in experimental evaluations more comprehensive than any previous literature of which we are aware{---}evaluating on 11 datasets using multiple existing as well as newly proposed scoring functions{---}we find that our method provides up to double the precision of previous unsupervised methods and the highest average performance using a much more compact word representation and yielding many new state-of-the-art results.
76,korre-etal-2021-elerrant,"   {ELERRANT}: Automatic Grammatical Error Type Classification for {G}reek"",
",in this paper we introduce the greek version of the automatic annotation tool errant (bryant et al. 2017) which we named elerrant. errant functions as a rule-based error type classifier and was used as the main evaluation tool of the systems participating in the bea-2019 (bryant et al. 2019) shared task. here we discuss grammatical and morphological differences between english and greek and how these differences affected the development of elerrant. we also introduce the first greek native corpus (gnc) and the greek wikiedits corpus (gwe) two new evaluation datasets with errors from native greek learners and wikipedia talk pages edits respectively. these two datasets are used for the evaluation of elerrant. this paper is a sole fragment of a bigger picture which illustrates the attempt to solve the problem of low-resource languages in nlp in our case greek.
77,jayannavar-etal-2020-learning,"   Learning to execute instructions in a {M}inecraft dialogue"",
",the minecraft collaborative building task is a two-player game in which an architect (a) instructs a builder (b) to construct a target structure in a simulated blocks world environment. we define the subtask of predicting correct action sequences (block placements and removals) in a given game context and show that capturing b{'}s past actions as well as b{'}s perspective leads to a significant improvement in performance on this challenging language understanding problem.
78,munoz-bravo-marquez-2021-interventions,"   Interventions Recommendation: Professionals{'} Observations Analysis in Special Needs Education"",
",we present a new task in educational nlp recommend the best interventions to help special needs education professionals to work with students with different disabilities. we use the professionals{'} observations of the students together with the students diagnosis and other chosen interventions to predict the best interventions for chilean special needs students.
79,mesnard-etal-2016-construction,"   Construction automatis{\'e}e d{'}une base de connaissances (Automated Building a Knowledge Base)"",
",le syst{\`e}me pr{\'e}sent{\'e} permet la construction automatis{\'e}e d{'}une base de connaissances sur des personnes et des organisations {\`a} partir d{'}une collection de documents. il s{'}appuie sur de l{'}apprentissage distant pour l{'}extraction d{'}hypoth{\`e}ses de relations entre mentions d{'}entit{\'e}s qu{'}il consolide avec des informations orient{\'e}es graphe.
80,gittens-etal-2017-skip,"   Skip-Gram − {Z}ipf + Uniform = Vector Additivity"",
",in recent years word-embedding models have gained great popularity due to their remarkable performance on several tasks including word analogy questions and caption generation. an unexpected {``}side-effect{''} of such models is that their vectors often exhibit compositionality i.e. \textit{adding}two word-vectors results in a vector that is only a small angle away from the vector of a word representing the semantic composite of the original words e.g. {``}man{''} + {``}royal{''} = {``}king{''}. this work provides a theoretical justification for the presence of additive compositionality in word vectors learned using the skip-gram model. in particular it shows that additive compositionality holds in an even stricter sense (small distance rather than small angle) under certain assumptions on the process generating the corpus. as a corollary it explains the success of vector calculus in solving word analogies. when these assumptions do not hold this work describes the correct non-linear composition operator. finally this work establishes a connection between the skip-gram model and the sufficient dimensionality reduction (sdr) framework of globerson and tishby: the parameters of sdr models can be obtained from those of skip-gram models simply by adding information on symbol frequencies. this shows that skip-gram embeddings are optimal in the sense of globerson and tishby and further implies that the heuristics commonly used to approximately fit skip-gram models can be used to fit sdr models.
81,sandoval-etal-2008-developing,"   Developing a Phonemic and Syllabic Frequency Inventory for Spontaneous Spoken Castilian {S}panish and their Comparison to Text-Based Inventories"",
",in this paper we present our recent work to develop phonemic and syllabic inventories for castilian spanish based on the c-oral-rom corpus a spontaneous spoken resource with varying degrees of naturalness and in different communicative contexts. these inventories have been developed by means of a phonemic and syllabic automatic transcriptor whose output has been assessed by manually reviewing most of the transcriptions. the inventories include absolute frequencies of occurrence of the different phones and syllables. these frequencies have been contrasted against an inventory extracted from a comparable textual corpus finding evidence that the available inventories based mainly on text do not provide an accurate description of spontaneously spoken castilian spanish.
82,pareti-prodanof-2010-annotating,"   Annotating Attribution Relations: Towards an {I}talian Discourse Treebank"",
",in this paper we describe the development of a schema for the annotation of attribution relations and present the first findings and some relevant issues concerning this phenomenon. following the d-ltag approach to discourse we have developed a lexically anchored description of attribution considering this relation contrary to the approach in the pdtb independently from other discourse relations. this approach has allowed us to deal with the phenomenon in a broader perspective than previous studies reaching therefore a more accurate description of it and making it possible to raise some still unaddressed issues. following this analysis we propose an annotation schema and discuss the first results concerning its applicability. the schema has been applied to a pilot portion of the isst corpus of italian and represents the initial phase of a project aiming at the creation of an italian discourse treebank. we believe this work will raise some awareness concerning the fundamental importance of attribution relations. the identification of the source has in fact strong implications for the attributed material. moreover it will make overt the complexity of a phenomenon for long underestimated.
83,aloraini-poesio-2020-anaphoric,"   Anaphoric Zero Pronoun Identification: A Multilingual Approach"",
",pro-drop languages such as arabic chinese italian or japanese allow morphologically null but referential arguments in certain syntactic positions called anaphoric zero-pronouns. much nlp work on anaphoric zero-pronouns (azp) is based on gold mentions but models for their identification are a fundamental prerequisite for their resolution in real-life applications. such identification requires complex language understanding and knowledge of real-world entities. transfer learning models such as bert have recently shown to learn surface syntactic and semantic informationwhich can be very useful in recognizing azps. we propose a bert-based multilingual model for azp identification from predicted zero pronoun positions and evaluate it on the arabic and chinese portions of ontonotes 5.0. as far as we know this is the first neural network model of azp identification for arabic; and our approach outperforms the stateof-the-art for chinese. experiment results suggest that bert implicitly encode information about azps through their surrounding context.
84,doukhan-etal-2012-designing,"   Designing {F}rench Tale Corpora for Entertaining Text To Speech Synthesis"",
",text and speech corpora for training a tale telling robot have been designed recorded and annotated. the aim of these corpora is to study expressive storytelling behaviour and to help in designing expressive prosodic and co-verbal variations for the artificial storyteller). a set of 89 children tales in french serves as a basis for this work. the tales annotation principles and scheme are described together with the corpus description in terms of coverage and inter-annotator agreement. automatic analysis of a new tale with the help of this corpus and machine learning is discussed. metrics for evaluation of automatic annotation methods are discussed. a speech corpus of about 1 hour with 12 tales has been recorded and aligned and annotated. this corpus is used for predicting expressive prosody in children tales above the level of the sentence.
85,forcada-etal-2018-exploring,"   Exploring gap filling as a cheaper alternative to reading comprehension questionnaires when evaluating machine translation for gisting"",
",a popular application of machine translation (mt) is \textit{gisting}: mt is consumed \textit{as is} to make sense of text in a foreign language. evaluation of the usefulness of mt for gisting is surprisingly uncommon. the classical method uses \textit{reading comprehension questionnaires} (rcq) in which informants are asked to answer professionally-written questions in their language about a foreign text that has been machine-translated into their language. recently \textit{gap-filling} (gf) a form of \textit{cloze} testing has been proposed as a cheaper alternative to rcq. in gf certain words are removed from reference translations and readers are asked to fill the gaps left using the machine-translated text as a hint. this paper reports for the first time a comparative evaluation using both rcq and gf of translations from multiple mt systems for the same foreign texts and a systematic study on the effect of variables such as gap density gap-selection strategies and document context in gf. the main findings of the study are: (a) both rcq and gf clearly identify mt to be useful; (b) global rcq and gf rankings for the mt systems are mostly in agreement; (c) gf scores vary very widely across informants making comparisons among mt systems hard and (d) unlike rcq which is framed around documents gf evaluation can be framed at the sentence level. these findings support the use of gf as a cheaper alternative to rcq.
86,dai-futrell-2021-simple,"   Simple induction of (deterministic) probabilistic finite-state automata for phonotactics by stochastic gradient descent"",
",we introduce a simple and highly general phonotactic learner which induces a probabilistic finite-state automaton from word-form data. we describe the learner and show how to parameterize it to induce unrestricted regular languages as well as how to restrict it to certain subregular classes such as strictly k-local and strictly k-piecewise languages. we evaluate the learner on its ability to learn phonotactic constraints in toy examples and in datasets of quechua and navajo. we find that an unrestricted learner is the most accurate overall when modeling attested forms not seen in training; however only the learner restricted to the strictly piecewise language class successfully captures certain nonlocal phonotactic constraints. our learner serves as a baseline for more sophisticated methods.
87,simonarson-etal-2021-mideinds,"   Mi{\dh}eind{'}s {WMT} 2021 Submission"",
",we present mi{\dh}eind{'}s submission for the english→icelandic and icelandic→english subsets of the 2021 wmt news translation task. transformer-base models are trained for translation on parallel data to generate backtranslations teratively. a pretrained mbart-25 model is then adapted for translation using parallel data as well as the last backtranslation iteration. this adapted pretrained model is then used to re-generate backtranslations and the training of the adapted model is continued.
88,zhao-etal-2021-sparta,"   {SPARTA}: Efficient Open-Domain Question Answering via Sparse Transformer Matching Retrieval"",
",we introduce sparta a novel neural retrieval method that shows great promise in performance generalization and interpretability for open-domain question answering. unlike many neural ranking methods that use dense vector nearest neighbor search sparta learns a sparse representation that can be efficiently implemented as an inverted index. the resulting representation enables scalable neural retrieval that does not require expensive approximate vector search and leads to better performance than its dense counterpart. we validated our approaches on 4 open-domain question answering (openqa) tasks and 11 retrieval question answering (reqa) tasks. sparta achieves new state-of-the-art results across a variety of open-domain question answering tasks in both english and chinese datasets including open squad cmrc and etc. analysis also confirms that the proposed method creates human interpretable representation and allows flexible control over the trade-off between performance and efficiency.
89,he-etal-2018-sequence,"   Modeling Speech Acts in Asynchronous Conversations: A Neural-{CRF} Approach"",
",participants in an asynchronous conversation (e.g. forum e-mail) interact with each other at different times performing certain communicative acts called speech acts (e.g. question request). in this article we propose a hybrid approach to speech act recognition in asynchronous conversations. our approach works in two main steps: a long short-term memory recurrent neural network (lstm-rnn) first encodes each sentence separately into a task-specific distributed representation and this is then used in a conditional random field (crf) model to capture the conversational dependencies between sentences. the lstm-rnn model uses pretrained word embeddings learned from a large conversational corpus and is trained to classify sentences into speech act types. the crf model can consider arbitrary graph structures to model conversational dependencies in an asynchronous conversation. in addition to mitigate the problem of limited annotated data in the asynchronous domains we adapt the lstm-rnn model to learn from synchronous conversations (e.g. meetings) using domain adversarial training of neural networks. empirical evaluation shows the effectiveness of our approach over existing ones: (i) lstm-rnns provide better task-specific representations (ii) conversational word embeddings benefit the lstm-rnns more than the off-the-shelf ones (iii) adversarial training gives better domain-invariant representations and (iv) the global crf model improves over local models.
90,sysoev-mayorov-2018-texterra,"   Texterra at {S}em{E}val-2018 Task 7: Exploiting Syntactic Information for Relation Extraction and Classification in Scientific Papers"",
",in this work we evaluate applicability of entity pair models and neural network architectures for relation extraction and classification in scientific papers at semeval-2018. we carry out experiments with representing entity pairs through sentence tokens and through shortest path in dependency tree comparing approaches based on convolutional and recurrent neural networks. with convolutional network applied to shortest path in dependency tree we managed to be ranked eighth in subtask 1.1 ({``}clean data{''}) ninth in 1.2 ({``}noisy data{''}). similar model applied to separate parts of the shortest path was mounted to ninth (extraction track) and seventh (classification track) positions in subtask 2 ranking.
91,ding-etal-2020-chinese,"   {C}hinese Content Scoring: Open-Access Datasets and Features on Different Segmentation Levels"",
",in this paper we analyse the challenges of chinese content scoring in comparison to english. as a review of prior work for chinese content scoring shows a lack of open-access data in the field we present two short-answer data sets for chinese. the chinese educational short answers data set (cesa) contains 1800 student answers for five science-related questions. as a second data set we collected asap-zh with 942 answers by re-using three existing prompts from the asap data set. we adapt a state-of-the-art content scoring system for chinese and evaluate it in several settings on these data sets. results show that features on lower segmentation levels such as character n-grams tend to have better performance than features on token level.
92,srivastava-etal-2018-zero,"   Zero-shot Learning of Classifiers from Natural Language Quantification"",
",humans can efficiently learn new concepts using language. we present a framework through which a set of explanations of a concept can be used to learn a classifier without access to any labeled examples. we use semantic parsing to map explanations to probabilistic assertions grounded in latent class labels and observed attributes of unlabeled data and leverage the differential semantics of linguistic quantifiers (e.g. {`}usually{'} vs {`}always{'}) to drive model training. experiments on three domains show that the learned classifiers outperform previous approaches for learning with limited data and are comparable with fully supervised classifiers trained from a small number of labeled examples.
93,mehri-eskenazi-2020-usr,"   {USR}: An Unsupervised and Reference Free Evaluation Metric for Dialog Generation"",
",the lack of meaningful automatic evaluation metrics for dialog has impeded open-domain dialog research. standard language generation metrics have been shown to be ineffective for evaluating dialog models. to this end this paper presents usr an unsupervised and reference-free evaluation metric for dialog. usr is a reference-free metric that trains unsupervised models to measure several desirable qualities of dialog. usr is shown to strongly correlate with human judgment on both topical-chat (turn-level: 0.42 system-level: 1.0) and personachat (turn-level: 0.48 and system-level: 1.0). usr additionally produces interpretable measures for several desirable properties of dialog.
94,gombert-bartsch-2020-multivitaminbooster,"   {M}ulti{V}itamin{B}ooster at {PARSEME} Shared Task 2020: Combining Window- and Dependency-Based Features with Multilingual Contextualised Word Embeddings for {VMWE} Detection"",
",in this paper we present multivitaminbooster a system implemented for the parseme shared task on semi-supervised identification of verbal multiword expressions - edition 1.2. for our approach we interpret detecting verbal multiword expressions as a token classification task aiming to decide whether a token is part of a verbal multiword expression or not. for this purpose we train gradient boosting-based models. we encode tokens as feature vectors combining multilingual contextualized word embeddings provided by the xlm-roberta language model with a more traditional linguistic feature set relying on context windows and dependency relations. our system was ranked 7th in the official open track ranking of the shared task evaluations with an encoding-related bug distorting the results. for this reason we carry out further unofficial evaluations. unofficial versions of our systems would have achieved higher ranks.
95,soldaini-etal-2018-helping,"   Helping or Hurting? Predicting Changes in Users{'} Risk of Self-Harm Through Online Community Interactions"",
",in recent years online communities have formed around suicide and self-harm prevention. while these communities offer support in moment of crisis they can also normalize harmful behavior discourage professional treatment and instigate suicidal ideation. in this work we focus on how interaction with others in such a community affects the mental state of users who are seeking support. we first build a dataset of conversation threads between users in a distressed state and community members offering support. we then show how to construct a classifier to predict whether distressed users are helped or harmed by the interactions in the thread and we achieve a macro-f1 score of up to 0.69.
96,peters-martins-2021-smoothing,"   Smoothing and Shrinking the Sparse {S}eq2{S}eq Search Space"",
",current sequence-to-sequence models are trained to minimize cross-entropy and use softmax to compute the locally normalized probabilities over target sequences. while this setup has led to strong results in a variety of tasks one unsatisfying aspect is its length bias: models give high scores to short inadequate hypotheses and often make the empty string the argmax{---}the so-called cat got your tongue problem. recently proposed entmax-based sparse sequence-to-sequence models present a possible solution since they can shrink the search space by assigning zero probability to bad hypotheses but their ability to handle word-level tasks with transformers has never been tested. in this work we show that entmax-based models effectively solve the cat got your tongue problem removing a major source of model error for neural machine translation. in addition we generalize label smoothing a critical regularization technique to the broader family of fenchel-young losses which includes both cross-entropy and the entmax losses. our resulting label-smoothed entmax loss models set a new state of the art on multilingual grapheme-to-phoneme conversion and deliver improvements and better calibration properties on cross-lingual morphological inflection and machine translation for 7 language pairs.
97,markov-etal-2019-anglicized,"   Anglicized Words and Misspelled Cognates in Native Language Identification"",
",in this paper we present experiments that estimate the impact of specific lexical choices of people writing in a second language (l2). in particular we look at misspelled words that indicate lexical uncertainty on the part of the author and separate them into three categories: misspelled cognates {``}l2-ed{''} (in our case anglicized) words and all other spelling errors. we test the assumption that such errors contain clues about the native language of an essay{'}s author through the task of native language identification. the results of the experiments show that the information brought by each of these categories is complementary. we also note that while the distribution of such features changes with the proficiency level of the writer their contribution towards native language identification remains significant at all levels.
98,chen-etal-2018-pre,"   Pre- and In-Parsing Models for Neural Empty Category Detection"",
",motivated by the positive impact of empty category on syntactic parsing we study neural models for pre- and in-parsing detection of empty category which has not previously been investigated. we find several non-obvious facts: (a) bilstm can capture non-local contextual information which is essential for detecting empty categories (b) even with a bilstm syntactic information is still able to enhance the detection and (c) automatic detection of empty categories improves parsing quality for overt words. our neural ecd models outperform the prior state-of-the-art by significant margins.
99,yao-etal-2019-docred,"   {D}oc{RED}: A Large-Scale Document-Level Relation Extraction Dataset"",
",multiple entities in a document generally exhibit complex inter-sentence relations and cannot be well handled by existing relation extraction (re) methods that typically focus on extracting intra-sentence relations for single entity pairs. in order to accelerate the research on document-level re we introduce docred a new dataset constructed from wikipedia and wikidata with three features: (1) docred annotates both named entities and relations and is the largest human-annotated dataset for document-level re from plain text; (2) docred requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document; (3) along with the human-annotated data we also offer large-scale distantly supervised data which enables docred to be adopted for both supervised and weakly supervised scenarios. in order to verify the challenges of document-level re we implement recent state-of-the-art methods for re and conduct a thorough evaluation of these methods on docred. empirical results show that docred is challenging for existing re methods which indicates that document-level re remains an open problem and requires further efforts. based on the detailed analysis on the experiments we discuss multiple promising directions for future research. we make docred and the code for our baselines publicly available at https://github.com/thunlp/docred.
100,marvin-linzen-2018-targeted,"   Targeted Syntactic Evaluation of Language Models"",
",we present a data set for evaluating the grammaticality of the predictions of a language model. we automatically construct a large number of minimally different pairs of english sentences each consisting of a grammatical and an ungrammatical sentence. the sentence pairs represent different variations of structure-sensitive phenomena: subject-verb agreement reflexive anaphora and negative polarity items. we expect a language model to assign a higher probability to the grammatical sentence than the ungrammatical one. in an experiment using this data set an lstm language model performed poorly on many of the constructions. multi-task training with a syntactic objective (ccg supertagging) improved the lstm{'}s accuracy but a large gap remained between its performance and the accuracy of human participants recruited online. this suggests that there is considerable room for improvement over lstms in capturing syntax in a language model.
101,shinoda-etal-2021-question,"   Can Question Generation Debias Question Answering Models? A Case Study on Question{--}Context Lexical Overlap"",
",question answering (qa) models for reading comprehension have been demonstrated to exploit unintended dataset biases such as question{--}context lexical overlap. this hinders qa models from generalizing to under-represented samples such as questions with low lexical overlap. question generation (qg) a method for augmenting qa datasets can be a solution for such performance degradation if qg can properly debias qa datasets. however we discover that recent neural qg models are biased towards generating questions with high lexical overlap which can amplify the dataset bias. moreover our analysis reveals that data augmentation with these qg models frequently impairs the performance on questions with low lexical overlap while improving that on questions with high lexical overlap. to address this problem we use a synonym replacement-based approach to augment questions with low lexical overlap. we demonstrate that the proposed data augmentation approach is simple yet effective to mitigate the degradation problem with only 70k synthetic examples.
102,scherrer-ljubesic-2021-social,"   Social Media Variety Geolocation with geo{BERT}"",
",this paper describes the helsinki{--}ljubljana contribution to the vardial 2021 shared task on social media variety geolocation. following our successful participation at vardial 2020 we again propose constrained and unconstrained systems based on the bert architecture. in this paper we report experiments with different tokenization settings and different pre-trained models and we contrast our parameter-free regression approach with various classification schemes proposed by other participants at vardial 2020. both the code and the best-performing pre-trained models are made freely available.
103,ryu-etal-2018-domain,"   Out-of-domain Detection based on Generative Adversarial Network"",
",the main goal of this paper is to develop out-of-domain (ood) detection for dialog systems. we propose to use only in-domain (ind) sentences to build a generative adversarial network (gan) of which the discriminator generates low scores for ood sentences. to improve basic gans we apply feature matching loss in the discriminator use domain-category analysis as an additional task in the discriminator and remove the biases in the generator. thereby we reduce the huge effort of collecting ood sentences for training ood detection. for evaluation we experimented ood detection on a multi-domain dialog system. the experimental results showed the proposed method was most accurate compared to the existing methods.
104,balazs-etal-2017-refining,"   Refining Raw Sentence Representations for Textual Entailment Recognition via Attention"",
",in this paper we present the model used by the team rivercorners for the 2017 repeval shared task. first our model separately encodes a pair of sentences into variable-length representations by using a bidirectional lstm. later it creates fixed-length raw representations by means of simple aggregation functions which are then refined using an attention mechanism. finally it combines the refined representations of both sentences into a single vector to be used for classification. with this model we obtained test accuracies of 72.057{\%} and 72.055{\%} in the matched and mismatched evaluation tracks respectively outperforming the lstm baseline and obtaining performances similar to a model that relies on shared information between sentences (esim). when using an ensemble both accuracies increased to 72.247{\%} and 72.827{\%} respectively.
105,pedinotti-etal-2021-cat,"   Did the Cat Drink the Coffee? Challenging Transformers with Generalized Event Knowledge"",
",prior research has explored the ability of computational models to predict a word semantic fit with a given predicate. while much work has been devoted to modeling the typicality relation between verbs and arguments in isolation in this paper we take a broader perspective by assessing whether and to what extent computational approaches have access to the information about the typicality of entire events and situations described in language (generalized event knowledge). given the recent success of transformers language models (tlms) we decided to test them on a benchmark for the dynamic estimation of thematic fit. the evaluation of these models was performed in comparison with sdm a framework specifically designed to integrate events in sentence meaning representations and we conducted a detailed error analysis to investigate which factors affect their behavior. our results show that tlms can reach performances that are comparable to those achieved by sdm. however additional analysis consistently suggests that tlms do not capture important aspects of event knowledge and their predictions often depend on surface linguistic features such as frequent words collocations and syntactic patterns thereby showing sub-optimal generalization abilities.
106,duong-etal-2017-multilingual,"   Multilingual Training of Crosslingual Word Embeddings"",
",crosslingual word embeddings represent lexical items from different languages using the same vector space enabling crosslingual transfer. most prior work constructs embeddings for a pair of languages with english on one side. we investigate methods for building high quality crosslingual word embeddings for many languages in a unified vector space.in this way we can exploit and combine strength of many languages. we obtained high performance on bilingual lexicon induction monolingual similarity and crosslingual document classification tasks.
107,wang-etal-2021-want-reduce,"   Want To Reduce Labeling Cost? {GPT}-3 Can Help"",
",data annotation is a time-consuming and labor-intensive process for many nlp tasks. although there exist various methods to produce pseudo data labels they are often task-specific and require a decent amount of labeled data to start with. recently the immense language model gpt-3 with 170 billion parameters has achieved tremendous improvement across many few-shot learning tasks. in this paper we explore ways to leverage gpt-3 as a low-cost data labeler to train other models. we find that to make the downstream model achieve the same performance on a variety of nlu and nlg tasks it costs 50{\%} to 96{\%} less to use labels from gpt-3 than using labels from humans. furthermore we propose a novel framework of combining pseudo labels from gpt-3 with human labels which leads to even better performance. these results present a cost-effective data labeling methodology that is generalizable to many practical applications.
108,mariotti-etal-2020-towards,"   Towards Harnessing Natural Language Generation to Explain Black-box Models"",
",the opaque nature of many machine learning techniques prevents the wide adoption of powerful information processing tools for high stakes scenarios. the emerging field explainable artificial intelligence (xai) aims at providing justifications for automatic decision-making systems in order to ensure reliability and trustworthiness in the users. for achieving this vision we emphasize the importance of a natural language textual modality as a key component for a future intelligent interactive agent. we outline the challenges of xai and review a set of publications that work in this direction.
109,pimentel-2012-identifying,"   Identifying equivalents of specialized verbs in a bilingual comparable corpus of judgments: A frame-based methodology"",
",multilingual terminological resources do not always include the equivalents of specialized verbs that occur in legal texts. this study aims to bridge that gap by proposing a methodology to assign the equivalents of this kind of predicative units. we use a comparable corpus of judgments produced by the supreme court of canada and by the supremo tribunal de justi{\c{c}}a de portugal. from this corpus 200 english and portuguese verbs are selected. the description of the verbs is based on the theory of frame semantics (fillmore 1977 1977 1982 1985) as well as on the framenet methodology (ruppenhofer et al. 2010). specialized verbs are said to evoke a semantic frame a sort of conceptual scenario in which a number of mandatory elements play specific roles (e.g. the role of judge the role of defendant). given that semantic frames are language independent to a fair degree (boas 2005; baker 2009) the labels attributed to each of the 76 identified frames (e.g. [crime] [regulations]) were used to group together 165 pairs of candidate equivalents. 71{\%} of them are full equivalents whereas 29{\%} are only partial equivalents.
110,sheng-etal-2016-dataset,"   A Dataset for Multimodal Question Answering in the Cultural Heritage Domain"",
",multimodal question answering in the cultural heritage domain allows visitors to ask questions in a more natural way and thus provides better user experiences with cultural objects while visiting a museum landmark or any other historical site. in this paper we introduce the construction of a golden standard dataset that will aid research of multimodal question answering in the cultural heritage domain. the dataset which will be soon released to the public contains multimodal content including images of typical artworks from the fascinating old-egyptian amarna period related image-containing documents of the artworks and over 800 multimodal queries integrating visual and textual questions. the multimodal questions and related documents are all in english. the multimodal questions are linked to relevant paragraphs in the related documents that contain the answer to the multimodal query.
111,ferreira-freitas-2020-natural,"   Natural Language Premise Selection: Finding Supporting Statements for Mathematical Text"",
",mathematical text is written using a combination of words and mathematical expressions. this combination along with a specific way of structuring sentences makes it challenging for state-of-art nlp tools to understand and reason on top of mathematical discourse. in this work we propose a new nlp task the natural premise selection which is used to retrieve supporting definitions and supporting propositions that are useful for generating an informal mathematical proof for a particular statement. we also make available a dataset nl-ps which can be used to evaluate different approaches for the natural premise selection task. using different baselines we demonstrate the underlying interpretation challenges associated with the task.
112,zhu-etal-2020-efficient,"   Efficient Context and Schema Fusion Networks for Multi-Domain Dialogue State Tracking"",
",dialogue state tracking (dst) aims at estimating the current dialogue state given all the preceding conversation. for multi-domain dst the data sparsity problem is a major obstacle due to increased numbers of state candidates and dialogue lengths. to encode the dialogue context efficiently we utilize the previous dialogue state (predicted) and the current dialogue utterance as the input for dst. to consider relations among different domain-slots the schema graph involving prior knowledge is exploited. in this paper a novel context and schema fusion network is proposed to encode the dialogue context and schema graph by using internal and external attention mechanisms. experiment results show that our approach can outperform strong baselines and the previous state-of-the-art method (som-dst) can also be improved by our proposed schema graph.
113,wible-tsao-2016-word,"   Word {M}idas Powered by {S}tring{N}et: Discovering Lexicogrammatical Constructions in Situ"",
",adult second language learners face the daunting but underappreciated task of mastering patterns of language use that are neither products of fully productive grammar rules nor frozen items to be memorized. word midas a web browser extention targets this uncharted territory of lexicogrammar by detecting multiword tokens of lexicogrammatical patterning in real time in situ within the noisy digital texts from the user{'}s unscripted web browsing or other digital venues. the language model powering word midas is stringnet a densely cross-indexed navigable network of one billion lexicogrammatical patterns of english. these resources are described and their functionality is illustrated with a detailed scenario.
114,maehlum-etal-2021-negation,"   Negation in {N}orwegian: an annotated dataset"",
",this paper introduces norecneg {--} the first annotated dataset of negation for norwegian. negation cues and their in-sentence scopes have been annotated across more than 11k sentences spanning more than 400 documents for a subset of the norwegian review corpus (norec). in addition to providing in-depth discussion of the annotation guidelines we also present a first set of benchmark results based on a graph-parsing approach.
115,girardi-etal-2018-patient,"   Patient Risk Assessment and Warning Symptom Detection Using Deep Attention-Based Neural Networks"",
",we present an operational component of a real-world patient triage system. given a specific patient presentation the system is able to assess the level of medical urgency and issue the most appropriate recommendation in terms of best \textit{point of care} and \textit{time to treat}. we use an attention-based convolutional neural network architecture trained on 600000 doctor notes in german. we compare two approaches one that uses the full text of the medical notes and one that uses only a selected list of medical entities extracted from the text. these approaches achieve 79{\%} and 66{\%} precision respectively but on a confidence threshold of 0.6 precision increases to 85{\%} and 75{\%} respectively. in addition a method to detect \textit{warning symptoms} is implemented to render the classification task transparent from a medical perspective. the method is based on the learning of attention scores and a method of automatic validation using the same data.
116,almeida-matos-2020-frugal,"   Frugal neural reranking: evaluation on the Covid-19 literature"",
",the covid-19 pandemic urged the scientific community to join efforts at an unprecedented scale leading to faster than ever dissemination of data and results which in turn motivated more research works. this paper presents and discusses information retrieval models aimed at addressing the challenge of searching the large number of publications that stem from these studies. the model presented based on classical baselines followed by an interaction based neural ranking model was evaluated and evolved within the trec covid challenge setting. results on this dataset show that when starting with a strong baseline our light neural ranking model can achieve results that are comparable to other model architectures that use very large number of parameters.
117,park-tyers-2019-new,"   A New Annotation Scheme for the {S}ejong Part-of-speech Tagged Corpus"",
",in this paper we present a new annotation scheme for the sejong part-of-speech tagged corpus based on universal dependencies style annotation. by using a new annotation scheme we can produce sejong-style morphological analysis and part-of-speech tagging results which have been the \textit{de facto} standard for korean language processing. we also explore the possibility of doing named-entity recognition and semantic-role labelling for korean using the new annotation scheme.
118,yuan-etal-2020-graph,"   Graph Attention Network with Memory Fusion for Aspect-level Sentiment Analysis"",
",aspect-level sentiment analysis(asc) predicts each specific aspect term{'}s sentiment polarity in a given text or review. recent studies used attention-based methods that can effectively improve the performance of aspect-level sentiment analysis. these methods ignored the syntactic relationship between the aspect and its corresponding context words leading the model to focus on syntactically unrelated words mistakenly. one proposed solution the graph convolutional network (gcn) cannot completely avoid the problem. while it does incorporate useful information about syntax it assigns equal weight to all the edges between connected words. it may still incorrectly associate unrelated words to the target aspect through the iterations of graph convolutional propagation. in this study a graph attention network with memory fusion is proposed to extend gcn{'}s idea by assigning different weights to edges. syntactic constraints can be imposed to block the graph convolutional propagation of unrelated words. a convolutional layer and a memory fusion were applied to learn and exploit multiword relations and draw different weights of words to improve performance further. experimental results on five datasets show that the proposed method yields better performance than existing methods.
119,runge-hovy-2020-exploring,"   Exploring Neural Entity Representations for Semantic Information"",
",neural methods for embedding entities are typically extrinsically evaluated on downstream tasks and more recently intrinsically using probing tasks. downstream task-based comparisons are often difficult to interpret due to differences in task structure while probing task evaluations often look at only a few attributes and models. we address both of these issues by evaluating a diverse set of eight neural entity embedding methods on a set of simple probing tasks demonstrating which methods are able to remember words used to describe entities learn type relationship and factual information and identify how frequently an entity is mentioned. we also compare these methods in a unified framework on two entity linking tasks and discuss how they generalize to different model architectures and datasets.
120,he-etal-2020-open,"   Open-source Multi-speaker Speech Corpora for Building {G}ujarati, {K}annada, {M}alayalam, {M}arathi, {T}amil and {T}elugu Speech Synthesis Systems"",
",we present free high quality multi-speaker speech corpora for gujarati kannada malayalam marathi tamil and telugu which are six of the twenty two official languages of india spoken by 374 million native speakers. the datasets are primarily intended for use in text-to-speech (tts) applications such as constructing multilingual voices or being used for speaker or language adaptation. most of the corpora (apart from marathi which is a female-only database) consist of at least 2000 recorded lines from female and male native speakers of the language. we present the methodological details behind corpora acquisition which can be scaled to acquiring data for other languages of interest. we describe the experiments in building a multilingual text-to-speech model that is constructed by combining our corpora. our results indicate that using these corpora results in good quality voices with mean opinion scores (mos) {\textgreater} 3.6 for all the languages tested. we believe that these resources released with an open-source license and the described methodology will help in the progress of speech applications for the languages described and aid corpora development for other smaller languages of india and beyond.
121,li-etal-2019-compositional,"   Compositional Generalization for Primitive Substitutions"",
",compositional generalization is a basic mechanism in human language learning but current neural networks lack such ability. in this paper we conduct fundamental research for encoding compositionality in neural networks. conventional methods use a single representation for the input sentence making it hard to apply prior knowledge of compositionality. in contrast our approach leverages such knowledge with two representations one generating attention maps and the other mapping attended input words to output symbols. we reduce the entropy in each representation to improve generalization. our experiments demonstrate significant improvements over the conventional methods in five nlp tasks including instruction learning and machine translation. in the scan domain it boosts accuracies from 14.0{\%} to 98.8{\%} in jump task and from 92.0{\%} to 99.7{\%} in turnleft task. it also beats human performance on a few-shot learning task. we hope the proposed approach can help ease future research towards human-level compositional language learning.
122,svetla-2021-towards,"   Towards Expanding {W}ord{N}et with Conceptual Frames"",
",the paper presents the project semantic network with a wide range of semantic relations and its main achievements. the ultimate objective of the project is to expand princeton wordnet with conceptual frames that define the syntagmatic relations of verb synsets and the semantic classes of nouns felicitous to combine with particular verbs. at this stage of the work: a) over 5000 wordnet verb synsets have been supplied with manually evaluated framenet semantic frames b) 253 semantic types have been manually mapped to the appropriate wordnet concepts providing detailed ontological representation of the semantic classes of nouns.
123,detrez-etal-2014-sharing,"   Sharing resources between free/open-source rule-based machine translation systems: Grammatical Framework and Apertium"",
",in this paper we describe two methods developed for sharing linguistic data between two free and open source rule based machine translation systems: apertium a shallow-transfer system; and grammatical framework (gf) which performs a deeper syntactic transfer. in the first method we describe the conversion of lexical data from apertium to gf while in the second one we automatically extract apertium shallow-transfer rules from a gf bilingual grammar. we evaluated the resulting systems in a english-spanish translation context and results showed the usefulness of the resource sharing and confirmed the a-priori strong and weak points of the systems involved. {\textbackslash}{\textbackslash}
124,jon-etal-2021-end,"   End-to-End Lexically Constrained Machine Translation for Morphologically Rich Languages"",
",lexically constrained machine translation allows the user to manipulate the output sentence by enforcing the presence or absence of certain words and phrases. although current approaches can enforce terms to appear in the translation they often struggle to make the constraint word form agree with the rest of the generated output. our manual analysis shows that 46{\%} of the errors in the output of a baseline constrained model for english to czech translation are related to agreement. we investigate mechanisms to allow neural machine translation to infer the correct word inflection given lemmatized constraints. in particular we focus on methods based on training the model with constraints provided as part of the input sequence. our experiments on english-czech language pair show that this approach improves translation of constrained terms in both automatic and manual evaluation by reducing errors in agreement. our approach thus eliminates inflection errors without introducing new errors or decreasing overall quality of the translation.
125,germann-1999-deterministic,"   A deterministic dependency parser for {J}apanese"",
",we present a rule-based deterministic dependency parser for japanese. it was implemented in c++ using object classes that reflect linguistic concepts and thus facilitate the transfer of linguistic intuitions into code. the parser first chunks morphemes into one-word phrases and then parses from the right to the left. the average parsing accuracy is 83.6{\%}.
126,kaushik-lipton-2018-much,"   How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks"",
",many recent papers address reading comprehension where examples consist of (question passage answer) tuples. presumably a model must combine information from both questions and passages to predict corresponding answers. however despite intense interest in the topic with hundreds of published papers vying for leaderboard dominance basic questions about the difficulty of many popular benchmarks remain unanswered. in this paper we establish sensible baselines for the babi squad cbt cnn and who-did-what datasets finding that question- and passage-only models often perform surprisingly well. on 14 out of 20 babi tasks passage-only models achieve greater than 50{\%} accuracy sometimes matching the full model. interestingly while cbt provides 20-sentence passages only the last is needed for accurate prediction. by comparison squad and cnn appear better-constructed.
127,nivre-2003-efficient,"   An Efficient Algorithm for Projective Dependency Parsing"",
",this paper presents a deterministic parsing algorithm for projective dependency grammar. the running time of the algorithm is linear in the length of the input string and the dependency graph produced is guaranteed to be projective and acyclic. the algorithm has been experimentally evaluated in parsing unrestricted swedish text achieving an accuracy above 85{\%} with a very simple grammar.
128,ono-etal-2008-construction,"   Construction and Analysis of Word-level Time-aligned Simultaneous Interpretation Corpus"",
",in this paper quantitative analyses of the delay in japanese-to-english (j-e) and english-to-japanese (e-j) interpretations are described. the simultaneous interpretation database of nagoya university (sidb) was used for the analyses. beginning time and end time of each word were provided to the corpus using hmm-based phoneme segmentation and the time lag between the corresponding words was calculated as the word-level delay. word-level delay was calculated for 3722 pairs and 4932 pairs of words for j-e and e-j interpretations respectively. the analyses revealed that j-e interpretation has much larger delay than e-j interpretation and that the difference of word order between japanese and english affect the degree of delay.
129,donahue-etal-2017-humorhawk,"   {H}umor{H}awk at {S}em{E}val-2017 Task 6: Mixing Meaning and Sound for Humor Recognition"",
",this paper describes the winning system for semeval-2017 task 6: {\#}hashtagwars: learning a sense of humor. humor detection has up until now been predominantly addressed using feature-based approaches. our system utilizes recurrent deep learning methods with dense embeddings to predict humorous tweets from the @midnight show {\#}hashtagwars. in order to include both meaning and sound in the analysis glove embeddings are combined with a novel phonetic representation to serve as input to an lstm component. the output is combined with a character-based cnn model and an xgboost component in an ensemble model which achieves 0.675 accuracy on the evaluation data.
130,guerreiro-martins-2021-spectra,"   {SPECTRA}: Sparse Structured Text Rationalization"",
",selective rationalization aims to produce decisions along with rationales (e.g. text highlights or word alignments between two sentences). commonly rationales are modeled as stochastic binary masks requiring sampling-based gradient estimators which complicates training and requires careful hyperparameter tuning. sparse attention mechanisms are a deterministic alternative but they lack a way to regularize the rationale extraction (e.g. to control the sparsity of a text highlight or the number of alignments). in this paper we present a unified framework for deterministic extraction of structured explanations via constrained inference on a factor graph forming a differentiable layer. our approach greatly eases training and rationale regularization generally outperforming previous work on what comes to performance and plausibility of the extracted rationales. we further provide a comparative study of stochastic and deterministic methods for rationale extraction for classification and natural language inference tasks jointly assessing their predictive power quality of the explanations and model variability.
131,saha-etal-2021-hate,"   Hate-Alert@{D}ravidian{L}ang{T}ech-{EACL}2021: Ensembling strategies for Transformer-based Offensive language Detection"",
",social media often acts as breeding grounds for different forms of offensive content. for low resource languages like tamil the situation is more complex due to the poor performance of multilingual or language-specific models and lack of proper benchmark datasets. based on this shared task {``}offensive language identification in dravidian languages{''} at eacl 2021; we present an exhaustive exploration of different transformer models we also provide a genetic algorithm technique for ensembling different models. our ensembled models trained separately for each language secured the first position in tamil the second position in kannada and the first position in malayalam sub-tasks. the models and codes are provided.
132,schulte-im-walde-etal-2016-ghost,"   {G}ho{S}t-{NN}: A Representative Gold Standard of {G}erman Noun-Noun Compounds"",
",this paper presents a novel gold standard of german noun-noun compounds (ghost-nn) including 868 compounds annotated with corpus frequencies of the compounds and their constituents productivity and ambiguity of the constituents semantic relations between the constituents and compositionality ratings of compound-constituent pairs. moreover a subset of the compounds containing 180 compounds is balanced for the productivity of the modifiers (distinguishing low/mid/high productivity) and the ambiguity of the heads (distinguishing between heads with 1 2 and {\textgreater}2 senses
133,bhat-etal-2019-margin,"   A Margin-based Loss with Synthetic Negative Samples for Continuous-output Machine Translation"",
",neural models that eliminate the softmax bottleneck by generating word embeddings (rather than multinomial distributions over a vocabulary) attain faster training with fewer learnable parameters. these models are currently trained by maximizing densities of pretrained target embeddings under von mises-fisher distributions parameterized by corresponding model-predicted embeddings. this work explores the utility of margin-based loss functions in optimizing such models. we present syn-margin loss a novel margin-based loss that uses a synthetic negative sample constructed from only the predicted and target embeddings at every step. the loss is efficient to compute and we use a geometric analysis to argue that it is more consistent and interpretable than other margin-based losses. empirically we find that syn-margin provides small but significant improvements over both vmf and standard margin-based losses in continuous-output neural machine translation.
134,bilu-etal-2019-argument,"   Argument Invention from First Principles"",
",competitive debaters often find themselves facing a challenging task {--} how to debate a topic they know very little about with only minutes to prepare and without access to books or the internet? what they often do is rely on {''}first principles{''} commonplace arguments which are relevant to many topics and which they have refined in past debates. in this work we aim to explicitly define a taxonomy of such principled recurring arguments and given a controversial topic to automatically identify which of these arguments are relevant to the topic. as far as we know this is the first time that this approach to argument invention is formalized and made explicit in the context of nlp. the main goal of this work is to show that it is possible to define such a taxonomy. while the taxonomy suggested here should be thought of as a {''}first attempt{''} it is nonetheless coherent covers well the relevant topics and coincides with what professional debaters actually argue in their speeches and facilitates automatic argument invention for new topics.
135,guillaume-etal-2016-crowdsourcing,"   Crowdsourcing Complex Language Resources: Playing to Annotate Dependency Syntax"",
",this article presents the results we obtained on a complex annotation task (that of dependency syntax) using a specifically designed game with a purpose zombilingo. we show that with suitable mechanisms (decomposition of the task training of the players and regular control of the annotation quality during the game) it is possible to obtain annotations whose quality is significantly higher than that obtainable with a parser provided that enough players participate. the source code of the game and the resulting annotated corpora (for french) are freely available.
136,chia-etal-2020-red,"   Nested Named Entity Recognition via Second-best Sequence Learning and Decoding"",
",when an entity name contains other names within it the identification of all combinations of names can become difficult and expensive. we propose a new method to recognize not only outermost named entities but also inner nested ones. we design an objective function for training a neural model that treats the tag sequence for nested entities as the second best path within the span of their parent entity. in addition we provide the decoding method for inference that extracts entities iteratively from outermost ones to inner ones in an outside-to-inside way. our method has no additional hyperparameters to the conditional random field based model widely used for flat named entity recognition tasks. experiments demonstrate that our method performs better than or at least as well as existing methods capable of handling nested entities achieving f1-scores of 85.82{\%} 84.34{\%} and 77.36{\%} on ace-2004 ace-2005 and genia datasets respectively.
137,tongchim-etal-2008-dependency,"   A Dependency Parser for {T}hai"",
",this paper presents some preliminary results of our dependency parser for thai. it is part of an ongoing project in developing a syntactically annotated thai corpus. the parser has been trained and tested by using the complete part of the corpus. the parser achieves 83.64{\%} as the root accuracy 78.54{\%} as the dependency accuracy and 53.90{\%} as the complete sentence accuracy. the trained parser will be used as a preprocessing step in our corpus annotation workflow in order to accelerate the corpus development.
138,xiang-etal-2020-sina,"   Sina {M}andarin Alphabetical Words:A Web-driven Code-mixing Lexical Resource"",
",mandarin alphabetical word (maw) is one indispensable component of modern chinese that demonstrates unique code-mixing idiosyncrasies influenced by language exchanges. yet this interesting phenomenon has not been properly addressed and is mostly excluded from the chinese language system. this paper addresses the core problem of maw identification and proposes to construct a large collection of maws from sina weibo (smaw) using an automatic web-based technique which includes rule-based identification informatics-based extraction as well as baidu search engine validation. a collection of 16207 qualified smaws are obtained using this technique along with an annotated corpus of more than 200000 sentences for linguistic research and applicable inquiries.
139,thorsteinsson-etal-2019-wide,"   Natural Questions: A Benchmark for Question Answering Research"",
",we present the natural questions corpus a question answering data set. questions consist of real anonymized aggregated queries issued to the google search engine. an annotator is presented with a question along with a wikipedia page from the top 5 search results and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page or marks null if no long/short answer is present. the public release consists of 307373 training examples with single annotations; 7830 examples with 5-way annotations for development data; and a further 7842 examples with 5-way annotated sequestered as test data. we present experiments validating quality of the data. we also describe analysis of 25-way annotations on 302 examples giving insights into human variability on the annotation task. we introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.
140,singh-etal-2021-cfilt,"   {CFILT} {IIT} {B}ombay@{LT}-{EDI}-{EACL}2021: Hope Speech Detection for Equality, Diversity, and Inclusion using Multilingual Representation from{T}ransformers"",
",with the internet becoming part and parcel of our lives engagement in social media has increased a lot. identifying and eliminating offensive content from social media has become of utmost priority to prevent any kind of violence. however detecting encouraging supportive and positive content is equally important to prevent misuse of censorship targeted to attack freedom of speech. this paper presents our system for the shared task hope speech detection for equality diversity and inclusion at lt-edi eacl 2021. the data for this shared task is provided in english tamil and malayalam which was collected from youtube comments. it is a multiclass classification problem where each data instance is categorized into one of the three classes: {`}hope speech{'} {`}not hope speech{'} and {`}not in intended language{'}. we propose a system that employs multilingual transformer models to obtain the representation of text and classifies it into one of the three classes. we explored the use of multilingual models trained specifically for indian languages along with generic multilingual models. our system was ranked 2nd for english 2nd for malayalam and 7th for the tamil language in the final leader board published by organizers and obtained a weighted f1-score of 0.92 0.84 0.55 respectively on the hidden test dataset used for the competition. we have made our system publicly available at github.
141,lu-2017-unified,"   A Unified Framework for Structured Prediction: From Theory to Practice"",
",structured prediction is one of the most important topics in various fields including machine learning computer vision natural language processing (nlp) and bioinformatics. in this tutorial we present a novel framework that unifies various structured prediction models.the hidden markov model (hmm) and the probabilistic context-free grammars (pcfgs) are two classic generative models used for predicting outputs with linear-chain and tree structures respectively. as hmm{'}s discriminative counterpart the linear-chain conditional random fields (crfs) (lafferty et al. 2001) model was later proposed. such a model was shown to yield good performance on standard nlp tasks such as information extraction. several extensions to such a model were then proposed afterward including the semi-markov crfs (sarawagi and cohen 2004) tree crfs (cohn and blunsom 2005) as well as discriminative parsing models and their latent variable variants (petrov and klein 2007). on the other hand utilizing a slightly different loss function one could arrive at the structured support vector machines (tsochantaridis et al. 2004) and its latent variable variant (yu and joachims 2009) as well. furthermore new models that integrate neural networks and graphical models such as neural crfs (do et al. 2010) were also proposed.in this tutorial we will be discussing how such a wide spectrum of existing structured prediction models can all be implemented under a unified framework (available at here) that involves some basic building blocks. based on such a framework we show how some seemingly complicated structured prediction models such as a semantic parsing model (lu et al. 2008; lu 2014) can be implemented conveniently and quickly. furthermore we also show that the framework can be used to solve certain structured prediction problems that otherwise cannot be easily handled by conventional structured prediction models. specifically we show how to use such a framework to construct models that are capable of predicting non-conventional structures such as overlapping structures (lu and roth 2015; muis and lu 2016a). we will also discuss how to make use of the framework to build other related models such as topic models and highlight its potential applications in some recent popular tasks (e.g. amr parsing (flanigan et al. 2014)).the framework has been extensively used by our research group for developing various structured prediction models including models for information extraction (lu and roth 2015; muis and lu 2016a; jie et al. 2017) noun phrase chunking (muis and lu 2016b) semantic parsing (lu 2015; susanto and lu 2017) and sentiment analysis (li and lu 2017). it is our hope that this tutorial will be helpful for many natural language processing researchers who are interested in designing their own structured prediction models rapidly. we also hope this tutorial allows researchers to strengthen their understandings on the connections between various structured prediction models and that the open release of the framework will bring value to the nlp research community and enhance its overall productivity.the material associated with this tutorial will be available at the tutorial web site: https://web.archive.org/web/20180427113151/http://statnlp.org/tutorials/.
142,shah-etal-2020-nlp,"   {NLP} Service {API}s and Models for Efficient Registration of New Clients"",
",state-of-the-art nlp inference uses enormous neural architectures and models trained for gpu-months well beyond the reach of most consumers of nlp. this has led to one-size-fits-all public api-based nlp service models by major ai companies serving millions of clients. they cannot afford traditional fine tuning for individual clients. many clients cannot even afford significant fine tuning and own little or no labeled data. recognizing that word usage and salience diversity across clients leads to reduced accuracy we initiate a study of practical and lightweight adaptation of centralized nlp services to clients. each client uses an unsupervised corpus-based sketch to register to the service. the server modifies its network mildly to accommodate client sketches and occasionally trains the augmented network over existing clients. when a new client registers with its sketch it gets immediate accuracy benefits. we demonstrate the proposed architecture using sentiment labeling ner and predictive language modeling.
143,caines-etal-2016-crowdsourcing,"   Crowdsourcing a Multi-lingual Speech Corpus: Recording, Transcription and Annotation of the {C}rowd{IS} Corpora"",
",we announce the release of the crowded corpus: a pair of speech corpora collected via crowdsourcing containing a native speaker corpus of english (crowded{\_}english) and a corpus of german/english bilinguals (crowded{\_}bilingual). release 1 of the crowded corpus contains 1000 recordings amounting to 33400 tokens collected from 80 speakers and is freely available to other researchers. we recruited participants via the crowdee application for android. recruits were prompted to respond to business-topic questions of the type found in language learning oral tests. we then used the crowdflower web application to pass these recordings to crowdworkers for transcription and annotation of errors and sentence boundaries. finally the sentences were tagged and parsed using standard natural language processing tools. we propose that crowdsourcing is a valid and economical method for corpus collection and discuss the advantages and disadvantages of this approach.
144,heyman-etal-2019-learning,"   Learning Unsupervised Multilingual Word Embeddings with Incremental Multilingual Hubs"",
",recent research has discovered that a shared bilingual word embedding space can be induced by projecting monolingual word embedding spaces from two languages using a self-learning paradigm without any bilingual supervision. however it has also been shown that for distant language pairs such fully unsupervised self-learning methods are unstable and often get stuck in poor local optima due to reduced isomorphism between starting monolingual spaces. in this work we propose a new robust framework for learning unsupervised multilingual word embeddings that mitigates the instability issues. we learn a shared multilingual embedding space for a variable number of languages by incrementally adding new languages one by one to the current multilingual space. through the gradual language addition the method can leverage the interdependencies between the new language and all other languages in the current multilingual space. we find that it is beneficial to project more distant languages later in the iterative process. our fully unsupervised multilingual embedding spaces yield results that are on par with the state-of-the-art methods in the bilingual lexicon induction (bli) task and simultaneously obtain state-of-the-art scores on two downstream tasks: multilingual document classification and multilingual dependency parsing outperforming even supervised baselines. this finding also accentuates the need to establish evaluation protocols for cross-lingual word embeddings beyond the omnipresent intrinsic bli task in future work.
145,elgabou-kazakov-2017-building,"   Building Dialectal {A}rabic Corpora"",
",the aim of this research is to identify local arabic dialects in texts from social media (twitter) and link them to specific geographic areas. dialect identification is studied as a subset of the task of language identification. the proposed method is based on unsupervised learning using simultaneously lexical and geographic distance. while this study focusses on libyan dialects the approach is general and could produce resources to support human translators and interpreters when dealing with vernaculars rather than standard arabic.
146,hazem-morin-2018-leveraging,"   Leveraging Meta-Embeddings for Bilingual Lexicon Extraction from Specialized Comparable Corpora"",
",recent evaluations on bilingual lexicon extraction from specialized comparable corpora have shown contrasted performance while using word embedding models. this can be partially explained by the lack of large specialized comparable corpora to build efficient representations. within this context we try to answer the following questions: first (i) among the state-of-the-art embedding models whether trained on specialized corpora or pre-trained on large general data sets which one is the most appropriate model for bilingual terminology extraction? second (ii) is it worth it to combine multiple embeddings trained on different data sets? for that purpose we propose the first systematic evaluation of different word embedding models for bilingual terminology extraction from specialized comparable corpora. we emphasize how the character-based embedding model outperforms other models on the quality of the extracted bilingual lexicons. further more we propose a new efficient way to combine different embedding models learned from specialized and general-domain data sets. our approach leads to higher performance than the best individual embedding model.
147,genereux-2002-example,"   An Example-Based Semantic Parser for Natural Language"",
",this paper presents a method for guiding semantic parsers based on a statistical model. the parser is example driven that is it learns how to interpret a new utterance by looking at some examples. it is mainly predicated on the idea that similarities exist between contexts in which individual parsing actions take place. those similarities are then used to compute the degree of certainty of a particular parse. the treatment of word order and the disambiguation of meanings can therefore be learned.
148,huber-carenini-2020-sentiment,"   From Sentiment Annotations to Sentiment Prediction through Discourse Augmentation"",
",sentiment analysis especially for long documents plausibly requires methods capturing complex linguistics structures. to accommodate this we propose a novel framework to exploit task-related discourse for the task of sentiment analysis. more specifically we are combining the large-scale sentiment-dependent mega-dt treebank with a novel neural architecture for sentiment prediction based on a hybrid treelstm hierarchical attention model. experiments show that our framework using sentiment-related discourse augmentations for sentiment prediction enhances the overall performance for long documents even beyond previous approaches using well-established discourse parsers trained on human annotated data. we show that a simple ensemble approach can further enhance performance by selectively using discourse depending on the document length.
149,wei-etal-2019-unsupervised,"   Unsupervised Neural Machine Translation with Future Rewarding"",
",in this paper we alleviate the local optimality of back-translation by learning a policy (takes the form of an encoder-decoder and is defined by its parameters) with future rewarding under the reinforcement learning framework which aims to optimize the global word predictions for unsupervised neural machine translation. to this end we design a novel reward function to characterize high-quality translations from two aspects: n-gram matching and semantic adequacy. the n-gram matching is defined as an alternative for the discrete bleu metric and the semantic adequacy is used to measure the adequacy of conveying the meaning of the source sentence to the target. during training our model strives for earning higher rewards by learning to produce grammatically more accurate and semantically more adequate translations. besides a variational inference network (vin) is proposed to constrain the corresponding sentences in two languages have the same or similar latent semantic code. on the widely used wmt{'}14 english-french wmt{'}16 english-german and nist chinese-to-english benchmarks our models respectively obtain 27.59/27.15 19.65/23.42 and 22.40 bleu points without using any labeled data demonstrating consistent improvements over previous unsupervised nmt models.
150,aga-etal-2016-learning,"   Learning Thesaurus Relations from Distributional Features"",
",in distributional semantics words are represented by aggregated context features. the similarity of words can be computed by comparing their feature vectors. thus we can predict whether two words are synonymous or similar with respect to some other semantic relation. we will show on six different datasets of pairs of similar and non-similar words that a supervised learning algorithm on feature vectors representing pairs of words outperforms cosine similarity between vectors representing single words. we compared different methods to construct a feature vector representing a pair of words. we show that simple methods like pairwise addition or multiplication give better results than a recently proposed method that combines different types of features. the semantic relation we consider is relatedness of terms in thesauri for intellectual document classification. thus our findings can directly be applied for the maintenance and extension of such thesauri. to the best of our knowledge this relation was not considered before in the field of distributional semantics.
151,baziotis-etal-2020-language,"   Language Model Prior for Low-Resource Neural Machine Translation"",
",the scarcity of large parallel corpora is an important obstacle for neural machine translation. a common solution is to exploit the knowledge of language models (lm) trained on abundant monolingual data. in this work we propose a novel approach to incorporate a lm as prior in a neural translation model (tm). specifically we add a regularization term which pushes the output distributions of the tm to be probable under the lm prior while avoiding wrong predictions when the tm {``}disagrees{''} with the lm. this objective relates to knowledge distillation where the lm can be viewed as teaching the tm about the target language. the proposed approach does not compromise decoding speed because the lm is used only at training time unlike previous work that requires it during inference. we present an analysis of the effects that different methods have on the distributions of the tm. results on two low-resource machine translation datasets show clear improvements even with limited monolingual data.
152,mueller-etal-2019-answering,"   Answering Conversational Questions on Structured Data without Logical Forms"",
",we present a novel approach to answering sequential questions based on structured objects such as knowledge bases or tables without using a logical form as an intermediate representation. we encode tables as graphs using a graph neural network model based on the transformer architecture. the answers are then selected from the encoded graph using a pointer network. this model is appropriate for processing conversations around structured data where the attention mechanism that selects the answers to a question can also be used to resolve conversational references. we demonstrate the validity of this approach with competitive results on the sequential question answering (sqa) task.
153,willemsen-etal-2018-context,"   Context-sensitive Natural Language Generation for robot-assisted second language tutoring"",
",this paper describes the l2tor intelligent tutoring system (its) focusing primarily on its output generation module. the l2tor its is developed for the purpose of investigating the efficacy of robot-assisted second language tutoring in early childhood. we explain the process of generating contextually-relevant utterances such as task-specific feedback messages and discuss challenges regarding multimodality and multilingualism for situated natural language generation from a robot tutoring perspective.
154,wang-etal-2017-sentence,"   Sentence Embedding for Neural Machine Translation Domain Adaptation"",
",although new corpora are becoming increasingly available for machine translation only those that belong to the same or similar domains are typically able to improve translation performance. recently neural machine translation (nmt) has become prominent in the field. however most of the existing domain adaptation methods only focus on phrase-based machine translation. in this paper we exploit the nmt{'}s internal embedding of the source sentence and use the sentence embedding similarity to select the sentences which are close to in-domain data. the empirical adaptation results on the iwslt english-french and nist chinese-english tasks show that the proposed methods can substantially improve nmt performance by 2.4-9.0 bleu points outperforming the existing state-of-the-art baseline by 2.3-4.5 bleu points.
155,chen-qian-2020-enhancing,"   Enhancing Aspect Term Extraction with Soft Prototypes"",
",aspect term extraction (ate) aims to extract aspect terms from a review sentence that users have expressed opinions on. existing studies mostly focus on designing neural sequence taggers to extract linguistic features from the token level. however since the aspect terms and context words usually exhibit long-tail distributions these taggers often converge to an inferior state without enough sample exposure. in this paper we propose to tackle this problem by correlating words with each other through soft prototypes. these prototypes generated by a soft retrieval process can introduce global knowledge from internal or external data and serve as the supporting evidence for discovering the aspect terms. our proposed model is a general framework and can be combined with almost all sequence taggers. experiments on four semeval datasets show that our model boosts the performance of three typical ate methods by a large margin.
156,garcia-fernandez-etal-2009-collecte,"   Collecte et analyses de r{\'e}ponses naturelles pour les syst{\`e}mes de questions-r{\'e}ponses"",
",notre travail se situe dans le cadre des syst{\`e}mes de r{\'e}ponse a une question et {\`a} pour but de fournir une r{\'e}ponse en langue naturelle aux questions pos{\'e}es en langue naturelle. cet article pr{\'e}sente une exp{\'e}rience permettant d{'}analyser les r{\'e}ponses de locuteurs du fran{\c{c}}ais {\`a} des questions que nous leur posons. l{'}exp{\'e}rience se d{\'e}roule {\`a} l{'}{\'e}crit comme {\`a} l{'}oral et propose {\`a} des locuteurs fran{\c{c}}ais des questions relevant de diff{\'e}rents types s{\'e}mantiques et syntaxiques. nous mettons en valeur une large variabilit{\'e} dans les formes de r{\'e}ponses possibles en langue fran{\c{c}}aise. d{'}autre part nous {\'e}tablissons un certain nombre de liens entre formulation de question et formulation de r{\'e}ponse. nous proposons d{'}autre part une comparaison des r{\'e}ponses selon la modalit{\'e} oral / {\'e}crit. ces r{\'e}sultats peuvent {\^e}tre int{\'e}gr{\'e}s {\`a} des syst{\`e}mes existants pour produire une r{\'e}ponse en langue naturelle de fa{\c{c}}on dynamique.
157,fukuda-etal-2020-naists,"   {NAIST}{'}s Machine Translation Systems for {IWSLT} 2020 Conversational Speech Translation Task"",
",this paper describes naist{'}s nmt system submitted to the iwslt 2020 conversational speech translation task. we focus on the translation disfluent speech transcripts that include asr errors and non-grammatical utterances. we tried a domain adaptation method by transferring the styles of out-of-domain data (united nations parallel corpus) to be like in-domain data (fisher transcripts). our system results showed that the nmt model with domain adaptation outperformed a baseline. in addition slight improvement by the style transfer was observed.
158,feyisetan-kasiviswanathan-2021-private,"   Private Release of Text Embedding Vectors"",
",ensuring strong theoretical privacy guarantees on text data is a challenging problem which is usually attained at the expense of utility. however to improve the practicality of privacy preserving text analyses it is essential to design algorithms that better optimize this tradeoff. to address this challenge we propose a release mechanism that takes any (text) embedding vector as input and releases a corresponding private vector. the mechanism satisfies an extension of differential privacy to metric spaces. our idea based on first randomly projecting the vectors to a lower-dimensional space and then adding noise in this projected space generates private vectors that achieve strong theoretical guarantees on its utility. we support our theoretical proofs with empirical experiments on multiple word embedding models and nlp datasets achieving in some cases more than 10{\%} gains over the existing state-of-the-art privatization techniques.
159,soni-etal-2017-post,"   Post-Processing Techniques for Improving Predictions of Multilabel Learning Approaches"",
",in multilabel learning (mll) each training instance is associated with a set of labels and the task is to learn a function that maps an unseen instance to its corresponding label set. in this paper we present a suite of {--} mll algorithm independent {--} post-processing techniques that utilize the conditional and directional label-dependences in order to make the predictions from any mll approach more coherent and precise. we solve constraint optimization problem over the output produced by any mll approach and the result is a refined version of the input predicted label set. using proposed techniques we show absolute improvement of 3{\%} on english news and 10{\%} on chinese e-commerce datasets for p@k metric.
160,zhou-etal-2021-rica,"   {RICA}: Evaluating Robust Inference Capabilities Based on Commonsense Axioms"",
",pre-trained language models (ptlms) have achieved impressive performance on commonsense inference benchmarks but their ability to employ commonsense to make robust inferences which is crucial for effective communications with humans is debated. in the pursuit of advancing fluid human-ai communication we propose a new challenge rica: robust inference using commonsense axioms that evaluates robust commonsense inference despite textual perturbations. to generate data for this challenge we develop a systematic and scalable procedure using commonsense knowledge bases and probe ptlms across two different evaluation settings. extensive experiments on our generated probe sets with more than 10k statements show that ptlms perform no better than random guessing on the zero-shot setting are heavily impacted by statistical biases and are not robust to perturbation attacks. we also find that fine-tuning on similar statements offer limited gains as ptlms still fail to generalize to unseen inferences. our new large-scale benchmark exposes a significant gap between ptlms and human-level language understanding and offers a new challenge for ptlms to demonstrate commonsense.
161,sengupta-etal-2021-gated-transformer,"   {G}ated {T}ransformer for {R}obust {D}e-noised {S}equence-to-{S}equence {M}odelling"",
",robust sequence-to-sequence modelling is an essential task in the real world where the inputs are often noisy. both user-generated and machine generated inputs contain various kinds of noises in the form of spelling mistakes grammatical errors character recognition errors all of which impact downstream tasks and affect interpretability of texts. in this work we devise a novel sequence-to-sequence architecture for detecting and correcting different real world and artificial noises (adversarial attacks) from english texts. towards that we propose a modified transformer-based encoder-decoder architecture that uses a gating mechanism to detect types of corrections required and accordingly corrects texts. experimental results show that our gated architecture with pre-trained language models perform significantly better that the non-gated counterparts and other state-of-the-art error correction models in correcting spelling and grammatical errors. extrinsic evaluation of our model on machine translation (mt) and summarization tasks show the competitive performance of the model against other generative sequence-to-sequence models under noisy inputs.
162,hanselowski-etal-2018-retrospective,"   A Retrospective Analysis of the Fake News Challenge Stance-Detection Task"",
",the 2017 fake news challenge stage 1 (fnc-1) shared task addressed a stance classification task as a crucial first step towards detecting fake news. to date there is no in-depth analysis paper to critically discuss fnc-1{'}s experimental setup reproduce the results and draw conclusions for next-generation stance classification methods. in this paper we provide such an in-depth analysis for the three top-performing systems. we first find that fnc-1{'}s proposed evaluation metric favors the majority class which can be easily classified and thus overestimates the true discriminative power of the methods. therefore we propose a new f1-based metric yielding a changed system ranking. next we compare the features and architectures used which leads to a novel feature-rich stacked lstm model that performs on par with the best systems but is superior in predicting minority classes. to understand the methods{'} ability to generalize we derive a new dataset and perform both in-domain and cross-domain experiments. our qualitative and quantitative study helps interpreting the original fnc-1 scores and understand which features help improving performance and why. our new dataset and all source code used during the reproduction study are publicly available for future research.
163,danlos-roze-2011-traduction,"   Traduction (automatique) des connecteurs de discours ((Machine) Translation of discourse connectors)"",
",en nous appuyant sur des donn{\'e}es fournies par le concordancier bilingue transsearch qui int{\`e}gre un alignement statistique au niveau des mots nous avons effectu{\'e} une annotation semi-manuelle de la traduction anglaise de deux connecteurs du fran{\c{c}}ais. les r{\'e}sultats de cette annotation montrent que les traductions de ces connecteurs ne correspondent pas aux « transpots » identifi{\'e}s par transsearch et encore moins {\`a} ce qui est propos{\'e} dans les dictionnaires bilingues.
164,nerima-wehrli-2020-la,"   La r{\'e}solution d{'}anaphores au-del{\`a} de la fronti{\`e}re de la phrase (The Anaphora Resolution Beyond Sentence Boundary)"",
",cette d{\'e}monstration pr{\'e}sente une extension de nos outils d{'}analyse syntaxique et d{'}{\'e}tiquetage morphosyntaxique qui prend en compte la r{\'e}solution d{'}anaphores pronominales non seulement {\`a} l{'}int{\'e}rieur d{'}une phrase mais {\'e}galement si l{'}ant{\'e}c{\'e}dent se trouve dans la phrase pr{\'e}c{\'e}dente. autant l{'}analyseur que l{'}{\'e}tiqueteur effectuant une analyse syntaxique compl{\`e}te des phrases ces outils affichent {\'e}galement les fonctions grammaticales des constituants (sujet objet direct etc.) et les arguments des verbes. une version de cette d{\'e}monstration est disponible sur le web.
165,ruiz-etal-2012-fbks,"   {FBK}{'}s machine translation systems for {IWSLT} 2012{'}s {TED} lectures"",
",this paper reports on fbk{'}s machine translation (mt) submissions at the iwslt 2012 evaluation on the ted talk translation tasks. we participated in the english-french and the arabic- dutch- german- and turkish-english translation tasks. several improvements are reported over our last year baselines. in addition to using fill-up combinations of phrase-tables for domain adaptation we explore the use of corpora filtering based on cross-entropy to produce concise and accurate translation and language models. we describe challenges encountered in under-resourced languages (turkish) and language-specific preprocessing needs.
166,nooralahzadeh-ovrelid-2018-sirius,"   {SIRIUS}-{LTG}: An Entity Linking Approach to Fact Extraction and Verification"",
",this article presents the sirius-ltg system for the fact extraction and verification (fever) shared task. it consists of three components: 1) \textit{wikipedia page retrieval}: first we extract the entities in the claim then we find potential wikipedia uri candidates for each of the entities using a sparql query over dbpedia 2) \textit{sentence selection}: we investigate various techniques i.e. smooth inverse frequency (sif) word mover{'}s distance (wmd) soft-cosine similarity cosine similarity with unigram term frequency inverse document frequency (tf-idf) to rank sentences by their similarity to the claim. 3) \textit{textual entailment}: we compare three models for the task of claim classification. we apply a decomposable attention (da) model (parikh et al. 2016) a decomposed graph entailment (dge) model (khot et al. 2018) and a gradient-boosted decision trees (talostree) model (sean et al. 2017) for this task. the experiments show that the pipeline with simple cosine similarity using tfidf in sentence selection along with da model as labelling model achieves the best results on the development set (f1 evidence: 32.17 label accuracy: 59.61 and fever score: 0.3778). furthermore it obtains 30.19 48.87 and 36.55 in terms of f1 evidence label accuracy and fever score respectively on the test set. our system ranks 15th among 23 participants in the shared task prior to any human-evaluation of the evidence.
167,nourbakhsh-etal-2019-sthruggle,"   sthruggle at {S}em{E}val-2019 Task 5: An Ensemble Approach to Hate Speech Detection"",
",in this paper we present our approach to detection of hate speech against women and immigrants in tweets for our participation in the semeval-2019 task 5. we trained an svm and an rf classifier using character bi- and trigram features and a bilstm pre-initialized with external word embeddings. we combined the predictions of the svm rf and bilstm in two different ensemble models. the first was a majority vote of the binary values and the second used the average of the confidence scores. for development we got the highest accuracy (75{\%}) by the final ensemble model with majority voting. for testing all models scored substantially lower and the scores between the classifiers varied more. we believe that these large differences between the higher accuracies in the development phase and the lower accuracies we obtained in the testing phase have partly to do with differences between the training development and testing data.
168,yeung-lee-2018-personalized,"   Personalized Text Retrieval for Learners of {C}hinese as a Foreign Language"",
",this paper describes a personalized text retrieval algorithm that helps language learners select the most suitable reading material in terms of vocabulary complexity. the user first rates their knowledge of a small set of words chosen by a graph-based active learning model. the system trains a complex word identification model on this set and then applies the model to find texts that contain the desired proportion of new challenging and familiar vocabulary. in an evaluation on learners of chinese as a foreign language we show that this algorithm is effective in identifying simpler texts for low-proficiency learners and more challenging ones for high-proficiency learners.
169,zhuang-wang-2019-token,"   Token-level Dynamic Self-Attention Network for Multi-Passage Reading Comprehension"",
",multi-passage reading comprehension requires the ability to combine cross-passage information and reason over multiple passages to infer the answer. in this paper we introduce the dynamic self-attention network (dynsan) for multi-passage reading comprehension task which processes cross-passage information at token-level and meanwhile avoids substantial computational costs. the core module of the dynamic self-attention is a proposed gated token selection mechanism which dynamically selects important tokens from a sequence. these chosen tokens will attend to each other via a self-attention mechanism to model long-range dependencies. besides convolutional layers are combined with the dynamic self-attention to enhance the model{'}s capacity of extracting local semantic. the experimental results show that the proposed dynsan achieves new state-of-the-art performance on the searchqa quasar-t and wikihop datasets. further ablation study also validates the effectiveness of our model components.
170,oliver-2016-extending,"   Extending the {WN}-Toolkit: dealing with polysemous words in the dictionary-based strategy"",
",in this paper we present an extension of the dictionary-based strategy for wordnet construction implemented in the wn-toolkit. this strategy allows the extraction of information for polysemous english words if definitions and/or semantic relations are present in the dictionary. the wn-toolkit is a freely available set of programs for the creation and expansion of wordnets using dictionary-based and parallel-corpus based strategies. in previous versions of the toolkit the dictionary-based strategy was only used for translating monosemous english variants. in the experiments we have used omegawiki and wiktionary and we present automatic evaluation results for 24 languages that have wordnets in the open multilingual wordnet project. we have used these existing versions of the wordnet to perform an automatic evaluation.
171,trajanovski-etal-2021-text,"   When does text prediction benefit from additional context? An exploration of contextual signals for chat and email messages"",
",email and chat communication tools are increasingly important for completing daily tasks. accurate real-time phrase completion can save time and bolster productivity. modern text prediction algorithms are based on large language models which typically rely on the prior words in a message to predict a completion. we examine how additional contextual signals (from previous messages time and subject) affect the performance of a commercial text prediction model. we compare contextual text prediction in chat and email messages from two of the largest commercial platforms microsoft teams and outlook finding that contextual signals contribute to performance differently between these scenarios. on emails time context is most beneficial with small relative gains of 2{\%} over baseline. whereas in chat scenarios using a tailored set of previous messages as context yields relative improvements over the baseline between 9.3{\%} and 18.6{\%} across various critical service-oriented text prediction metrics.
172,baruah-etal-2019-abaruah,"   {ABARUAH} at {S}em{E}val-2019 Task 5 : Bi-directional {LSTM} for Hate Speech Detection"",
",in this paper we present the results obtained using bi-directional long short-term memory (bilstm) with and without attention and logistic regression (lr) models for semeval-2019 task 5 titled {''}hateval: multilingual detection of hate speech against immigrants and women in twitter{''}. this paper presents the results obtained for subtask a for english language. the results of the bilstm and lr models are compared for two different types of preprocessing. one with no stemming performed and no stopwords removed. the other with stemming performed and stopwords removed. the bilstm model without attention performed the best for the first test while the lr model with character n-grams performed the best for the second test. the bilstm model obtained an f1 score of 0.51 on the test set and obtained an official ranking of 8/71.
173,lees-etal-2021-capturing,"   Capturing Covertly Toxic Speech via Crowdsourcing"",
",we study the task of labeling covert or veiled toxicity in online conversations. prior research has highlighted the difficulty in creating language models that recognize nuanced toxicity such as microaggressions. our investigations further underscore the difficulty in parsing such labels reliably from raters via crowdsourcing. we introduce an initial dataset coverttoxicity which aims to identify and categorize such comments from a refined rater template. finally we fine-tune a comment-domain bert model to classify covertly offensive comments and compare against existing baselines.
174,kim-schubert-2019-type,"   A Type-coherent, Expressive Representation as an Initial Step to Language Understanding"",
",a growing interest in tasks involving language understanding by the nlp community has led to the need for effective semantic parsing and inference. modern nlp systems use semantic representations that do not quite fulfill the nuanced needs for language understanding: adequately modeling language semantics enabling general inferences and being accurately recoverable. this document describes underspecified logical forms (ulf) for episodic logic (el) which is an initial form for a semantic representation that balances these needs. ulfs fully resolve the semantic type structure while leaving issues such as quantifier scope word sense and anaphora unresolved; they provide a starting point for further resolution into el and enable certain structural inferences without further resolution. this document also presents preliminary results of creating a hand-annotated corpus of ulfs for the purpose of training a precise ulf parser showing a three-person pairwise interannotator agreement of 0.88 on confident annotations. we hypothesize that a divide-and-conquer approach to semantic parsing starting with derivation of ulfs will lead to semantic analyses that do justice to subtle aspects of linguistic meaning and will enable construction of more accurate semantic parsers.
175,sun-etal-2018-super,"   Super Characters: A Conversion from Sentiment Classification to Image Classification"",
",we propose a method named super characters for sentiment classification. this method converts the sentiment classification problem into image classification problem by projecting texts into images and then applying cnn models for classification. text features are extracted automatically from the generated super characters images hence there is no need of any explicit step of embedding the words or characters into numerical vector representations. experimental results on large social media corpus show that the super characters method consistently outperforms other methods for sentiment classification and topic classification tasks on ten large social media datasets of millions of contents in four different languages including chinese japanese korean and english.
176,chrupala-etal-2020-analyzing,"   Analyzing analytical methods: The case of phonology in neural models of spoken language"",
",given the fast development of analysis techniques for nlp and speech processing systems few systematic studies have been conducted to compare the strengths and weaknesses of each method. as a step in this direction we study the case of representations of phonology in neural network models of spoken language. we use two commonly applied analytical techniques diagnostic classifiers and representational similarity analysis to quantify to what extent neural activation patterns encode phonemes and phoneme sequences. we manipulate two factors that can affect the outcome of analysis. first we investigate the role of learning by comparing neural activations extracted from trained versus randomly-initialized models. second we examine the temporal scope of the activations by probing both local activations corresponding to a few milliseconds of the speech signal and global activations pooled over the whole utterance. we conclude that reporting analysis results with randomly initialized models is crucial and that global-scope methods tend to yield more consistent and interpretable results and we recommend their use as a complement to local-scope diagnostic methods.
177,richard-bollans-etal-2020-categorisation,"   Categorisation, Typicality {\&} Object-Specific Features in Spatial Referring Expressions"",
",various accounts of cognition and semantic representations have highlighted that for some concepts different factors may influence category and typicality judgements. in particular some features may be more salient in categorisation tasks while other features are more salient when assessing typicality. in this paper we explore the extent to which this is the case for english spatial prepositions and discuss the implications for pragmatic strategies and semantic models. we hypothesise that object-specific features {---} related to object properties and affordances {---} are more salient in categorisation while geometric and physical relationships between objects are more salient in typicality judgements. in order to test this hypothesis we conducted a study using virtual environments to collect both category and typicality judgements in 3d scenes. based on the collected data we cannot verify the hypothesis and conclude that object-specific features appear to be salient in both category and typicality judgements further evidencing the need to include these types of features in semantic models.
178,thakkar-etal-2021-understanding,"   Understanding Unintended Memorization in Language Models Under Federated Learning"",
",recent works have shown that language models (lms) e.g. for next word prediction (nwp) have a tendency to memorize rare or unique sequences in the training data. since useful lms are often trained on sensitive data it is critical to identify and mitigate such \textit{unintended} memorization. federated learning (fl) has emerged as a novel framework for large-scale distributed learning tasks. it differs in many aspects from the well-studied \textit{central learning} setting where all the data is stored at the central server and minibatch stochastic gradient descent is used to conduct training. this work is motivated by our observation that nwp models trained under fl exhibited remarkably less propensity to such memorization compared to the central learning setting. thus we initiate a formal study to understand the effect of different components of fl on unintended memorization in trained nwp models. our results show that several differing components of fl play an important role in reducing unintended memorization. first we discover that the clustering of data according to users{---}which happens by design in fl{---}has the most significant effect in reducing such memorization. using the federated averaging optimizer with larger effective minibatch sizes for training causes a further reduction. we also demonstrate that training in fl with a user-level differential privacy guarantee results in models that can provide high utility while being resilient to memorizing \textit{out-of-distribution} phrases with thousands of insertions across over a hundred users in the training set.
179,ganu-p-2018-fast,"   Fast Query Expansion on an Accounting Corpus using Sub-Word Embeddings"",
",we present early results from a system under development which uses sub-word embeddings for query expansion in presence of mis-spelled words and other aberrations. we work for a company which creates accounting software and the end goal is to improve customer experience when they search for help on our {``}customer care{''} portal. our customers use colloquial language non-standard acronyms and sometimes mis-spell words when they use our search portal or interact over other channels. however our knowledge base has curated content which leverages technical terms and is in language which is quite formal. this results in the answer not being retrieved even though the answer might actually be present in the documentation (as assessed by a human). we address this problem by creating equivalence classes of words with similar meanings (with the additional property that the mappings to these equivalence classes are robust to mis-spellings) using sub-word embeddings and then use them to fine tune an elasticsearch index to improve recall. we demonstrate through an end-end system that using sub-word embeddings leads to a significant lift in correct answers retrieved for an accounting corpus available in the public domain.
180,tanase-etal-2020-upb,"   {UPB} at {S}em{E}val-2020 Task 12: Multilingual Offensive Language Detection on Social Media by Fine-tuning a Variety of {BERT}-based Models"",
",offensive language detection is one of the most challenging problem in the natural language processing field being imposed by the rising presence of this phenomenon in online social media. this paper describes our transformer-based solutions for identifying offensive language on twitter in five languages (i.e. english arabic danish greek and turkish) which was employed in subtask a of the offenseval 2020 shared task. several neural architectures (i.e. bert mbert roberta xlm-roberta and albert) pre-trained using both single-language and multilingual corpora were fine-tuned and compared using multiple combinations of datasets. finally the highest-scoring models were used for our submissions in the competition which ranked our team 21st of 85 28th of 53 19th of 39 16th of 37 and 10th of 46 for english arabic danish greek and turkish respectively.
181,oard-och-2003-rapid,"   Rapid-response machine translation for unexpected languages"",
",statistical techniques for machine translation offer promise for rapid development in response to unexpected requirements but realizing that potential requires rapid acquisition of required resources as well. this paper reports the results of experiments with resources collected in ten days; about 1.3 million words of parallel text from five types of sources and a bilingual term list with about 20000 term pairs. systems were trained with resources individually and in combination using an approach based on alignment templates. the use of all available resources was found to yield the best results in an automatic evaluation using the bleu measure but a single resource (the bible) coupled with a small amount of in-domain manual translation (less than 6000 words) achieved more than 85{\%} of that upper baseline. with a concerted effort such a system could be built in a single day.
182,tian-etal-2018-polarity,"   Polarity and Intensity: the Two Aspects of Sentiment Analysis"",
",current multimodal sentiment analysis frames sentiment score prediction as a general machine learning task. however what the sentiment score actually represents has often been overlooked. as a measurement of opinions and affective states a sentiment score generally consists of two aspects: polarity and intensity. we decompose sentiment scores into these two aspects and study how they are conveyed through individual modalities and combined multimodal models in a naturalistic monologue setting. in particular we build unimodal and multimodal multi-task learning models with sentiment score prediction as the main task and polarity and/or intensity classification as the auxiliary tasks. our experiments show that sentiment analysis benefits from multi-task learning and individual modalities differ when conveying the polarity and intensity aspects of sentiment.
183,niklaus-etal-2019-minwikisplit,"   {M}in{W}iki{S}plit: A Sentence Splitting Corpus with Minimal Propositions"",
",we compiled a new sentence splitting corpus that is composed of 203k pairs of aligned complex source and simplified target sentences. contrary to previously proposed text simplification corpora which contain only a small number of split examples we present a dataset where each input sentence is broken down into a set of minimal propositions i.e. a sequence of sound self-contained utterances with each of them presenting a minimal semantic unit that cannot be further decomposed into meaningful propositions. this corpus is useful for developing sentence splitting approaches that learn how to transform sentences with a complex linguistic structure into a fine-grained representation of short sentences that present a simple and more regular structure which is easier to process for downstream applications and thus facilitates and improves their performance.
184,melly-etal-2020-consolidated,"   A Consolidated Dataset for Knowledge-based Question Generation using Predicate Mapping of Linked Data"",
",in this paper we present the forwardquestions data set made of human-generated questions related to knowledge triples. this data set results from the conversion and merger of the existing simpledbpediaqa and simplequestionswikidata data sets including the mapping of predicates from dbpedia to wikidata and the selection of {`}forward{'} questions as opposed to {`}backward{'} ones. the new data set can be used to generate novel questions given an unseen wikidata triple by replacing the subjects of existing questions with the new one and then selecting the best candidate questions using semantic and syntactic criteria. evaluation results indicate that the question generation method using forwardquestions improves the quality of questions by about 20{\%} with respect to a baseline not using ranking criteria.
185,wuebker-etal-2014-comparison,"   Comparison of data selection techniques for the translation of video lectures"",
",for the task of online translation of scientific video lectures using huge models is not possible. in order to get smaller and efficient models we perform data selection. in this paper we perform a qualitative and quantitative comparison of several data selection techniques based on cross-entropy and infrequent n-gram criteria. in terms of bleu a combination of translation and language model cross-entropy achieves the most stable results. as another important criterion for measuring translation quality in our application we identify the number of out-of-vocabulary words. here infrequent n-gram recovery shows superior performance. finally we combine the two selection techniques in order to benefit from both their strengths.
186,ma-etal-2021-eventplus,"   {E}vent{P}lus: A Temporal Event Understanding Pipeline"",
",we present eventplus a temporal event understanding pipeline that integrates various state-of-the-art event understanding components including event trigger and type detection event argument detection event duration and temporal relation extraction. event information especially event temporal knowledge is a type of common sense knowledge that helps people understand how stories evolve and provides predictive hints for future events. eventplus as the first comprehensive temporal event understanding pipeline provides a convenient tool for users to quickly obtain annotations about events and their temporal information for any user-provided document. furthermore we show eventplus can be easily adapted to other domains (e.g. biomedical domain). we make eventplus publicly available to facilitate event-related information extraction and downstream applications.
187,misra-etal-2020-exploring,"   Exploring {BERT}{'}s Sensitivity to Lexical Cues using Tests from Semantic Priming"",
",models trained to estimate word probabilities in context have become ubiquitous in natural language processing. how do these models use lexical cues in context to inform their word probabilities? to answer this question we present a case study analyzing the pre-trained bert model with tests informed by semantic priming. using english lexical stimuli that show priming in humans we find that bert too shows {``}priming{''} predicting a word with greater probability when the context includes a related word versus an unrelated one. this effect decreases as the amount of information provided by the context increases. follow-up analysis shows bert to be increasingly distracted by related prime words as context becomes more informative assigning lower probabilities to related words. our findings highlight the importance of considering contextual constraint effects when studying word prediction in these models and highlight possible parallels with human processing.
188,fadaee-monz-2018-back,"   Back-Translation Sampling by Targeting Difficult Words in Neural Machine Translation"",
",neural machine translation has achieved state-of-the-art performance for several language pairs using a combination of parallel and synthetic data. synthetic data is often generated by back-translating sentences randomly sampled from monolingual data using a reverse translation model. while back-translation has been shown to be very effective in many cases it is not entirely clear why. in this work we explore different aspects of back-translation and show that words with high prediction loss during training benefit most from the addition of synthetic data. we introduce several variations of sampling strategies targeting difficult-to-predict words using prediction losses and frequencies of words. in addition we also target the contexts of difficult words and sample sentences that are similar in context. experimental results for the wmt news translation task show that our method improves translation quality by up to 1.7 and 1.2 bleu points over back-translation using random sampling for german-english and english-german respectively.
189,piao-etal-2016-lexical,"   Lexical Coverage Evaluation of Large-scale Multilingual Semantic Lexicons for Twelve Languages"",
",the last two decades have seen the development of various semantic lexical resources such as wordnet (miller 1995) and the usas semantic lexicon (rayson et al. 2004) which have played an important role in the areas of natural language processing and corpus-based studies. recently increasing efforts have been devoted to extending the semantic frameworks of existing lexical knowledge resources to cover more languages such as eurowordnet and global wordnet. in this paper we report on the construction of large-scale multilingual semantic lexicons for twelve languages which employ the unified lancaster semantic taxonomy and provide a multilingual lexical knowledge base for the automatic ucrel semantic annotation system (usas). our work contributes towards the goal of constructing larger-scale and higher-quality multilingual semantic lexical resources and developing corpus annotation tools based on them. lexical coverage is an important factor concerning the quality of the lexicons and the performance of the corpus annotation tools and in this experiment we focus on evaluating the lexical coverage achieved by the multilingual lexicons and semantic annotation tools based on them. our evaluation shows that some semantic lexicons such as those for finnish and italian have achieved lexical coverage of over 90{\%} while others need further expansion.
190,wang-etal-2019-improving-back,"   Improving Back-Translation with Uncertainty-based Confidence Estimation"",
",while back-translation is simple and effective in exploiting abundant monolingual corpora to improve low-resource neural machine translation (nmt) the synthetic bilingual corpora generated by nmt models trained on limited authentic bilingual data are inevitably noisy. in this work we propose to quantify the confidence of nmt model predictions based on model uncertainty. with word- and sentence-level confidence measures based on uncertainty it is possible for back-translation to better cope with noise in synthetic bilingual corpora. experiments on chinese-english and english-german translation tasks show that uncertainty-based confidence estimation significantly improves the performance of back-translation.
191,murarka-etal-2021-classification,"   Classification of mental illnesses on social media using {R}o{BERT}a"",
",given the current social distancing regulations across the world social media has become the primary mode of communication for most people. this has isolated millions suffering from mental illnesses who are unable to receive assistance in person. they have increasingly turned to online platforms to express themselves and to look for guidance in dealing with their illnesses. keeping this in mind we propose a solution to classify mental illness posts on social media thereby enabling users to seek appropriate help. in this work we classify five prominent kinds of mental illnesses- depression anxiety bipolar disorder adhd and ptsd by analyzing unstructured user data on reddit. in addition we share a new high-quality dataset1 to drive research on this topic. the dataset consists of the title and post texts from 17159 posts and 13 subreddits each associated with one of the five mental illnesses listed above or a none class indicating the absence of any mental illness. our model is trained on reddit data but is easily extensible to other social media platforms as well as demonstrated in our results.we believe that our work is the first multi-class model that uses a transformer based architecture such as roberta to analyze people{'}s emotions and psychology. we also demonstrate how we stress test our model using behavioral testing. our dataset is publicly available and we encourage researchers to utilize this to advance research in this arena. we hope that this work contributes to the public health system by automating some of the detection process and alerting relevant authorities about users that need immediate help.
192,laskar-etal-2021-enkhcorp1,"   {E}n{K}h{C}orp1.0: An {E}nglish{--}{K}hasi Corpus"",
",in machine translation corpus preparation is one of the crucial tasks particularly for lowresource pairs. in multilingual countries like india machine translation plays a vital role in communication among people with various linguistic backgrounds. there are available online automatic translation systems by google and microsoft which include various languages which lack support for the khasi language which can hence be considered lowresource. this paper overviews the development of enkhcorp1.0 a corpus for english{--}khasi pair and implemented baseline systems for englishtokhasi and khasitoenglish translation based on the neural machine translation approach.
193,soldner-etal-2019-uphill,"   Uphill from here: Sentiment patterns in videos from left- and right-wing {Y}ou{T}ube news channels"",
",news consumption exhibits an increasing shift towards online sources which bring platforms such as youtube more into focus. thus the distribution of politically loaded news is easier receives more attention but also raises the concern of forming isolated ideological communities. understanding how such news is communicated and received is becoming increasingly important. to expand our understanding in this domain we apply a linguistic temporal trajectory analysis to analyze sentiment patterns in english-language videos from news channels on youtube. we examine transcripts from videos distributed through eight channels with pro-left and pro-right political leanings. using unsupervised clustering we identify seven different sentiment patterns in the transcripts. we found that the use of two sentiment patterns differed significantly depending on political leaning. furthermore we used predictive models to examine how different sentiment patterns relate to video popularity and if they differ depending on the channel{'}s political leaning. no clear relations between sentiment patterns and popularity were found. however results indicate that videos from pro-right news channels are more popular and that a negative sentiment further increases that popularity when sentiments are averaged for each video.
194,smets-etal-2003-french-amalgam,"   {F}rench Amalgam: A machine-learned sentence realization system"",
",this paper presents the french implementation of amalgam a machine-learned sentence realization system. it presents in some detail two of the machine-learned models employed in amalgam and shows how linguistic intuition and knowledge can be combined with statistical techniques to improve the performance of the models.
195,christodoulides-2020-variation,"   Variation prosodique des styles de parole et interface syntaxe-prosodie: {\'E}tude sur corpus {\`a} grande {\'e}chelle (Speaking Style Prosodic Variation and the Prosody-Syntax Interface : A Large-Scale Corpus)"",
",la mutualisation et diffusion des grands corpus de parole permet de r{\'e}examiner des analyses pr{\'e}c{\'e}dentes effectu{\'e}es sur des corpus plus petits afin de v{\'e}rifier si les conclusions de ces analyses se g{\'e}n{\'e}ralisent aux nouvelles donn{\'e}es. dans cette {\'e}tude nous pr{\'e}sentons les r{\'e}sultats pr{\'e}liminaires d{'}une analyse de la variation des styles de parole en fran{\c{c}}ais bas{\'e}e sur un corpus {\`a} grande {\'e}chelle (300 heures 2500 locuteurs). le corpus a {\'e}t{\'e} r{\'e}align{\'e} au niveau des phones syllabes et mots et une annotation morphosyntaxique et syntaxique a {\'e}t{\'e} ajout{\'e} en am{\'e}liorant les annotations existantes. plusieurs caract{\'e}ristiques acoustiques et prosodiques sont automatiquement extraites et une analyse statistique (analyse en composantes principales acp) est effectu{\'e}e afin d{'}explorer les caract{\'e}ristiques des styles de parole et leur variance. nous explorons aussi la relation entre fronti{\`e}res prosodique et syntaxiques comme m{\'e}thode pour discriminer les styles de parole. 1
196,honda-etal-2021-removing,"   Removing Word-Level Spurious Alignment between Images and Pseudo-Captions in Unsupervised Image Captioning"",
",unsupervised image captioning is a challenging task that aims at generating captions without the supervision of image-sentence pairs but only with images and sentences drawn from different sources and object labels detected from the images. in previous work pseudo-captions i.e. sentences that contain the detected object labels were assigned to a given image. the focus of the previous work was on the alignment of input images and pseudo-captions at the sentence level. however pseudo-captions contain many words that are irrelevant to a given image. in this work we investigate the effect of removing mismatched words from image-sentence alignment to determine how they make this task difficult. we propose a simple gating mechanism that is trained to align image features with only the most reliable words in pseudo-captions: the detected object labels. the experimental results show that our proposed method outperforms the previous methods without introducing complex sentence-level learning objectives. combined with the sentence-level alignment method of previous work our method further improves its performance. these results confirm the importance of careful alignment in word-level details.
197,branco-etal-2020-shared,"   A Shared Task of a New, Collaborative Type to Foster Reproducibility: A First Exercise in the Area of Language Science and Technology with {REPROLANG}2020"",
",n this paper we introduce a new type of shared task {---} which is collaborative rather than competitive {---} designed to support and fosterthe reproduction of research results. we also describe the first event running such a novel challenge present the results obtained discussthe lessons learned and ponder on future undertakings.
198,bhandari-etal-2020-metrics,"   Metrics also Disagree in the Low Scoring Range: Revisiting Summarization Evaluation Metrics"",
",in text summarization evaluating the efficacy of automatic metrics without human judgments has become recently popular. one exemplar work (peyrard 2019) concludes that automatic metrics strongly disagree when ranking high-scoring summaries. in this paper we revisit their experiments and find that their observations stem from the fact that metrics disagree in ranking summaries from any narrow scoring range. we hypothesize that this may be because summaries are similar to each other in a narrow scoring range and are thus difficult to rank. apart from the width of the scoring range of summaries we analyze three other properties that impact inter-metric agreement - ease of summarization abstractiveness and coverage.
199,zeldes-etal-2019-disrpt,"   The {DISRPT} 2019 Shared Task on Elementary Discourse Unit Segmentation and Connective Detection"",
",in 2019 we organized the first iteration of a shared task dedicated to the underlying units used in discourse parsing across formalisms: the disrpt shared task on elementary discourse unit segmentation and connective detection. in this paper we review the data included in the task which cover 2.6 million manually annotated tokens from 15 datasets in 10 languages survey and compare submitted systems and report on system performance on each task for both annotated and plain-tokenized versions of the data.
200,moiron-2004-discarding,"   Discarding Noise in an Automatically Acquired Lexicon of Support verb Constructions"",
",we applied data-driven methods to carry out automatic acquisition of dutch prepositional support verb constructions (svcs) in corpora (e.g. iets in de gaten houden (``keep an eye on something'')). this paper addresses the question whether linguistic diagnostics help to discard noise from the nbest lists and how to (semi-)automatically apply such linguistic diagnostics to parsed corpora. we show that some of the linguistic diagnostics proposed in hollebrandse (1993) effectively identify svcs and contribute a modest error rate decrease.
201,uban-etal-2021-understanding,"   Interpretability Analysis for Named Entity Recognition to Understand System Predictions and How They Can Improve"",
",abstract named entity recognition systems achieve remarkable performance on domains such as english news. it is natural to ask: what are these models actually learning to achieve this? are they merely memorizing the names themselves? or are they capable of interpreting the text and inferring the correct entity type from the linguistic context? we examine these questions by contrasting the performance of several variants of architectures for named entity recognition with some provided only representations of the context as features. we experiment with glove-based bilstm-crf as well as bert. we find that context does influence predictions but the main factor driving high performance is learning the named tokens themselves. furthermore we find that bert is not always better at recognizing predictive contexts compared to a bilstm-crf model. we enlist human annotators to evaluate the feasibility of inferring entity types from context alone and find that humans are also mostly unable to infer entity types for the majority of examples on which the context-only system made errors. however there is room for improvement: a system should be able to recognize any named entity in a predictive context correctly and our experiments indicate that current systems may be improved by such capability. our human study also revealed that systems and humans do not always learn the same contextual clues and context-only systems are sometimes correct even when humans fail to recognize the entity type from the context. finally we find that one issue contributing to model errors is the use of {``}entangled{''} representations that encode both contextual and local token information into a single vector which can obscure clues. our results suggest that designing models that explicitly operate over representations of local inputs and context respectively may in some cases improve performance. in light of these and related findings we highlight directions for future work.
202,garg-moschitti-2021-will,"   Will this Question be Answered? Question Filtering via Answer Model Distillation for Efficient Question Answering"",
",in this paper we propose a novel approach towards improving the efficiency of question answering (qa) systems by filtering out questions that will not be answered by them. this is based on an interesting new finding: the answer confidence scores of state-of-the-art qa systems can be approximated well by models solely using the input question text. this enables preemptive filtering of questions that are not answered by the system due to their answer confidence scores being lower than the system threshold. specifically we learn transformer-based question models by distilling transformer-based answering models. our experiments on three popular qa datasets and one industrial qa benchmark demonstrate the ability of our question models to approximate the precision/recall curves of the target qa system well. these question models when used as filters can effectively trade off lower computation cost of qa systems for lower recall e.g. reducing computation by {\textasciitilde}60{\%} while only losing {\textasciitilde}3-4{\%} of recall.
203,prabhakaran-etal-2019-perturbation,"   Perturbation Sensitivity Analysis to Detect Unintended Model Biases"",
",data-driven statistical natural language processing (nlp) techniques leverage large amounts of language data to build models that can understand language. however most language data reflect the public discourse at the time the data was produced and hence nlp models are susceptible to learning incidental associations around named referents at a particular point in time in addition to general linguistic meaning. an nlp system designed to model notions such as sentiment and toxicity should ideally produce scores that are independent of the identity of such entities mentioned in text and their social associations. for example in a general purpose sentiment analysis system a phrase such as i hate katy perry should be interpreted as having the same sentiment as i hate taylor swift. based on this idea we propose a generic evaluation framework perturbation sensitivity analysis which detects unintended model biases related to named entities and requires no new annotations or corpora. we demonstrate the utility of this analysis by employing it on two different nlp models {---} a sentiment model and a toxicity model {---} applied on online comments in english language from four different genres.
204,bellengier-priego-valverde-2004-modelisation,"   Mod{\'e}lisation de la modulation"",
",le dialogue est un processus interactif pendant lequel les diff{\'e}rents agents impliqu{\'e}s vont s{'}engager sur un certain nombre d{'}{\'e}l{\'e}ments propositionnels. la modulation implique des ajouts propositionnels - r{\'e}vis{\'e}s et att{\'e}nu{\'e}s - qui ne constituent pas n{\'e}cessairement une base pour un accord. l{'}objectif de cet article est donc de proposer une description formelle du ph{\'e}nom{\`e}ne de modulation dans le cadre du mod{\`e}le de j. ginzburg.
205,tamburini-2019-quantum,"   A Quantum-Like Approach to Word Sense Disambiguation"",
",this paper presents a novel algorithm for word sense disambiguation (wsd) based on quantum probability theory. the quantum wsd algorithm requires concepts representations as vectors in the complex domain and thus we have developed a technique for computing complex word and sentence embeddings based on the paragraph vectors algorithm. despite the proposed method is quite simple and that it does not require long training phases when it is evaluated on a standardized benchmark for this task it exhibits state-of-the-art (sota) performances.
206,plum-etal-2019-rgcl,"   {RGCL}-{WLV} at {S}em{E}val-2019 Task 12: Toponym Detection"",
",this article describes the system submitted by the rgcl-wlv team to the semeval 2019 task 12: toponym resolution in scientific papers. the system detects toponyms using a bootstrapped machine learning (ml) approach which classifies names identified using gazetteers extracted from the geonames geographical database. the paper evaluates the performance of several ml classifiers as well as how the gazetteers influence the accuracy of the system. several runs were submitted. the highest precision achieved for one of the submissions was 89{\%} albeit it at a relatively low recall of 49{\%}.
207,moro-etal-2014-annotating,"   Annotating the {MASC} Corpus with {B}abel{N}et"",
",in this paper we tackle the problem of automatically annotating with both word senses and named entities the masc 3.0 corpus a large english corpus covering a wide range of genres of written and spoken text. we use babelnet 2.0 a multilingual semantic network which integrates both lexicographic and encyclopedic knowledge as our sense/entity inventory together with its semantic structure to perform the aforementioned annotation task. word sense annotated corpora have been around for more than twenty years helping the development of word sense disambiguation algorithms by providing both training and testing grounds. more recently entity linking has followed the same path with the creation of huge resources containing annotated named entities. however to date there has been no resource that contains both kinds of annotation. in this paper we present an automatic approach for performing this annotation together with its output on the masc corpus. we use this corpus because its goal of integrating different types of annotations goes exactly in our same direction. our overall aim is to stimulate research on the joint exploitation and disambiguation of word senses and named entities. finally we estimate the quality of our annotations using both manually-tagged named entities and word senses obtaining an accuracy of roughly 70{\%} for both named entities and word sense annotations.
208,gorelick-etal-2021-syntax,"   Syntax and Themes: How Context Free Grammar Rules and Semantic Word Association Influence Book Success"",
",in this paper we attempt to improve upon the state-of-the-art in predicting a novel{'}s success by modeling the lexical semantic relationships of its contents. we created the largest dataset used in such a project containing lexical data from 17962 books from project gutenberg. we utilized domain specific feature reduction techniques to implement the most accurate models to date for predicting book success with our best model achieving an average accuracy of 94.0{\%}. by analyzing the model parameters we extracted the successful semantic relationships from books of 12 different genres. we finally mapped those semantic relations to a set of themes as defined in roget{'}s thesaurus and discovered the themes that successful books of a given genre prioritize. at the end of the paper we further showed that our model demonstrate similar performance for book success prediction even when goodreads rating was used instead of download count to measure success.
209,seljan-etal-2010-corpus,"   Corpus Aligner ({C}or{A}l) Evaluation on {E}nglish-{C}roatian Parallel Corpora"",
",an increasing demand for new language resources of recent eu members and accessing countries has in turn initiated the development of different language tools and resources such as alignment tools and corresponding translation memories for new languages pairs. the primary goal of this paper is to provide a description of a free sentence alignment tool coral (corpus aligner) developed at the faculty of electrical engineering and computing university of zagreb. the tool performs paragraph alignment at the first step of the alignment process which is followed by sentence alignment. description of the tool is followed by its evaluation. the paper describes an experiment with applying the coral aligner to a english-croatian parallel corpus of legislative domain using metrics of precision recall and f1-measure. results are discussed and the concluding sections discuss future directions of coral development.
210,straka-etal-2021-character,"   Character Transformations for Non-Autoregressive {GEC} Tagging"",
",we propose a character-based non-autoregressive gec approach with automatically generated character transformations. recently per-word classification of correction edits has proven an efficient parallelizable alternative to current encoder-decoder gec systems. we show that word replacement edits may be suboptimal and lead to explosion of rules for spelling diacritization and errors in morphologically rich languages and propose a method for generating character transformations from gec corpus. finally we train character transformation models for czech german and russian reaching solid results and dramatic speedup compared to autoregressive systems. the source code is released at https://github.com/ufal/wnut2021{\_}character{\_}transformations{\_}gec.
211,yang-etal-2021-rethinking,"   Rethinking Stealthiness of Backdoor Attack against {NLP} Models"",
",recent researches have shown that large natural language processing (nlp) models are vulnerable to a kind of security threat called the backdoor attack. backdoor attacked models can achieve good performance on clean test sets but perform badly on those input sentences injected with designed trigger words. in this work we point out a potential problem of current backdoor attacking research: its evaluation ignores the stealthiness of backdoor attacks and most of existing backdoor attacking methods are not stealthy either to system deployers or to system users. to address this issue we first propose two additional stealthiness-based metrics to make the backdoor attacking evaluation more credible. we further propose a novel word-based backdoor attacking method based on negative data augmentation and modifying word embeddings making an important step towards achieving stealthy backdoor attacking. experiments on sentiment analysis and toxic detection tasks show that our method is much stealthier while maintaining pretty good attacking performance. our code is available at https://github.com/lancopku/sos.
212,attia-etal-2016-power,"   The Power of Language Music: {A}rabic Lemmatization through Patterns"",
",the interaction between roots and patterns in arabic has intrigued lexicographers and morphologists for centuries. while roots provide the consonantal building blocks patterns provide the syllabic vocalic moulds. while roots provide abstract semantic classes patterns realize these classes in specific instances. in this way both roots and patterns are indispensable for understanding the derivational morphological and to some extent the cognitive aspects of the arabic language. in this paper we perform lemmatization (a high-level lexical processing) without relying on a lookup dictionary. we use a hybrid approach that consists of a machine learning classifier to predict the lemma pattern for a given stem and mapping rules to convert stems to their respective lemmas with the vocalization defined by the pattern.
213,bar-haim-etal-2021-project,"   Project {D}ebater {API}s: {D}ecomposing the {AI} Grand Challenge"",
",project debater was revealed in 2019 as the first ai system that can debate human experts on complex topics. engaging in a live debate requires a diverse set of skills and project debater has been developed accordingly as a collection of components each designed to perform a specific subtask. project debater apis provide access to many of these capabilities as well as to more recently developed ones. this diverse set of web services publicly available for academic use includes core nlp services argument mining and analysis capabilities and higher-level services for content summarization. we describe these apis and their performance and demonstrate how they can be used for building practical solutions. in particular we will focus on key point analysis a novel technology that identifies the main points and their prevalence in a collection of texts such as survey responses and user reviews.
214,jang-etal-2018-interpretable,"   Interpretable Word Embedding Contextualization"",
",in this paper we propose a method of calibrating a word embedding so that the semantic it conveys becomes more relevant to the context. our method is novel because the output shows clearly which senses that were originally presented in a target word embedding become stronger or weaker. this is possible by utilizing the technique of using sparse coding to recover senses that comprises a word embedding.
215,roy-etal-2020-topic,"   A Topic-Aligned Multilingual Corpus of {W}ikipedia Articles for Studying Information Asymmetry in Low Resource Languages"",
",wikipedia is the largest web-based open encyclopedia covering more than three hundred languages. however different language editions of wikipedia differ significantly in terms of their information coverage. we present a systematic comparison of information coverage in english wikipedia (most exhaustive) and wikipedias in eight other widely spoken languages (arabic german hindi korean portuguese russian spanish and turkish). we analyze the content present in the respective wikipedias in terms of the coverage of topics as well as the depth of coverage of topics included in these wikipedias. our analysis quantifies and provides useful insights about the information gap that exists between different language editions of wikipedia and offers a roadmap for the ir community to bridge this gap.
216,barbedette-eshkol-taravella-2020-predire,"   Pr{\'e}dire automatiquement les intentions du locuteur dans des questions issues du discours oral spontan{\'e} (Automatically predicting the speaker{'}s intentions in questions from spontaneous oral speech)"",
",cette {\'e}tude porte sur la classification automatique des intentions exprim{\'e}es dans des questions issues d{'}un corpus d{'}{\'e}changes oraux spontan{\'e}s. nous proposons une typologie dans laquelle nous distinguons trois classes d{'}intentions (avis volont{\'e} et doute). apr{\`e}s plusieurs pr{\'e}traitements et ajouts de traits lexicaux aux donn{\'e}es (lexiques nombre de mots et de caract{\`e}res) nous impl{\'e}mentons un algorithme de classification automatique et nous en pr{\'e}sentons et {\'e}valuons les r{\'e}sultats qui atteignent une f-mesure de 062. nous proposons ensuite une interpr{\'e}tation de ceux-ci bas{\'e}e sur une comparaison entre les exp{\'e}riences men{\'e}es et des mesures li{\'e}es aux traits linguistiques int{\'e}gr{\'e}s avant la t{\^a}che de classification.
217,erdmann-habash-2018-complementary,"   Complementary Strategies for Low Resourced Morphological Modeling"",
",morphologically rich languages are challenging for natural language processing tasks due to data sparsity. this can be addressed either by introducing out-of-context morphological knowledge or by developing machine learning architectures that specifically target data sparsity and/or morphological information. we find these approaches to complement each other in a morphological paradigm modeling task in modern standard arabic which in addition to being morphologically complex features ubiquitous ambiguity exacerbating sparsity with noise. given a small number of out-of-context rules describing closed class morphology we combine them with word embeddings leveraging subword strings and noise reduction techniques. the combination outperforms both approaches individually by about 20{\%} absolute. while morphological resources already exist for modern standard arabic our results inform how comparable resources might be constructed for non-standard dialects or any morphologically rich low resourced language given scarcity of time and funding.
218,rashkin-etal-2019-towards,"   Towards Empathetic Open-domain Conversation Models: A New Benchmark and Dataset"",
",one challenge for dialogue agents is recognizing feelings in the conversation partner and replying accordingly a key communicative skill. while it is straightforward for humans to recognize and acknowledge others{'} feelings in a conversation this is a significant challenge for ai systems due to the paucity of suitable publicly-available datasets for training and evaluation. this work proposes a new benchmark for empathetic dialogue generation and empatheticdialogues a novel dataset of 25k conversations grounded in emotional situations. our experiments indicate that dialogue models that use our dataset are perceived to be more empathetic by human evaluators compared to models merely trained on large-scale internet conversation data. we also present empirical comparisons of dialogue model adaptations for empathetic responding leveraging existing models or datasets without requiring lengthy re-training of the full model.
219,suadaa-etal-2021-metric,"   Metric-Type Identification for Multi-Level Header Numerical Tables in Scientific Papers"",
",numerical tables are widely used to present experimental results in scientific papers. for table understanding a metric-type is essential to discriminate numbers in the tables. we introduce a new information extraction task metric-type identification from multi-level header numerical tables and provide a dataset extracted from scientific papers consisting of header tables captions and metric-types. we then propose two joint-learning neural classification and generation schemes featuring pointer-generator-based and bert-based models. our results show that the joint models can handle both in-header and out-of-header metric-type identification problems.
220,alqaisi-okeefe-2019-en,"   En-Ar Bilingual Word Embeddings without Word Alignment: Factors Effects"",
",this paper introduces the first attempt to investigate morphological segmentation on en-ar bilingual word embeddings using bilingual word embeddings model without word alignment (bilbowa). we investigate the effect of sentence length and embedding size on the learning process. our experiment shows that using the d3 segmentation scheme improves the accuracy of learning bilingual word embeddings up to 10 percentage points compared to the atb and d0 schemes in all different training settings.
221,atallah-2015-la,"   La ressource {EXPLICADIS}, un corpus annot{\'e} sp{\'e}cifiquement pour l{'}{\'e}tude des relations de discours causales"",
",dans le but de proposer une caract{\'e}risation des relations de discours li{\'e}es {\`a} la causalit{\'e} nous avons {\'e}t{\'e} amen{\'e}e {\`a} constituer et annoter notre propre corpus d{'}{\'e}tude : la ressource explicadis (explication et argumentation en discours). cette ressource a {\'e}t{\'e} construite dans la continuit{\'e} d{'}une ressource d{\'e}j{\`a} disponible le corpus annodis. proposant une annotation plus pr{\'e}cise des relations causales sur un ensemble de textes diversifi{\'e}s en genres textuels explicadis est le premier corpus de ce type constitu{\'e} sp{\'e}cifiquement pour l{'}{\'e}tude des relations de discours causales.
222,patra-moniz-2019-weakly,"   Weakly Supervised Attention Networks for Entity Recognition"",
",the task of entity recognition has traditionally been modelled as a sequence labelling task. however this usually requires a large amount of fine-grained data annotated at the token level which in turn can be expensive and cumbersome to obtain. in this work we aim to circumvent this requirement of word-level annotated data. to achieve this we propose a novel architecture for entity recognition from a corpus containing weak binary presence/absence labels which are relatively easier to obtain. we show that our proposed weakly supervised model trained solely on a multi-label classification task performs reasonably well on the task of entity recognition despite not having access to any token-level ground truth data.
223,bao-etal-2016-constraint,"   Constraint-Based Question Answering with Knowledge Graph"",
",webquestions and simplequestions are two benchmark data-sets commonly used in recent knowledge-based question answering (kbqa) work. most questions in them are {`}simple{'} questions which can be answered based on a single relation in the knowledge base. such data-sets lack the capability of evaluating kbqa systems on complicated questions. motivated by this issue we release a new data-set namely complexquestions aiming to measure the quality of kbqa systems on {`}multi-constraint{'} questions which require multiple knowledge base relations to get the answer. beside we propose a novel systematic kbqa approach to solve multi-constraint questions. compared to state-of-the-art methods our approach not only obtains comparable results on the two existing benchmark data-sets but also achieves significant improvements on the complexquestions.
224,mao-etal-2020-multi,"   Multi-document Summarization with Maximal Marginal Relevance-guided Reinforcement Learning"",
",while neural sequence learning methods have made significant progress in single-document summarization (sds) they produce unsatisfactory results on multi-document summarization (mds). we observe two major challenges when adapting sds advances to mds: (1) mds involves larger search space and yet more limited training data setting obstacles for neural methods to learn adequate representations; (2) mds needs to resolve higher information redundancy among the source documents which sds methods are less effective to handle. to close the gap we present rl-mmr maximal margin relevance-guided reinforcement learning for mds which unifies advanced neural sds methods and statistical measures used in classical mds. rl-mmr casts mmr guidance on fewer promising candidates which restrains the search space and thus leads to better representation learning. additionally the explicit redundancy measure in mmr helps the neural representation of the summary to better capture redundancy. extensive experiments demonstrate that rl-mmr achieves state-of-the-art performance on benchmark mds datasets. in particular we show the benefits of incorporating mmr into end-to-end learning when adapting sds to mds in terms of both learning effectiveness and efficiency.
225,guillen-1998-reusing,"   Reusing translated terms to expand a multilingual thesaurus"",
",multilingual thesauri play a key role in multilingual text retrieval. at present only a small number of on-line thesauri contain translations of terms in languages other than english. this is the case of the unified medical language system (umls) metathesaurus that includes the same term in different languages (e.g. english and spanish). however only a subset of terms in english have a corresponding translation in spanish. in this work i present an approach and some experimental results for reusing translated terms to expand the metathesaurus. the approach includes two main tasks: finding patterns and formulating rules to automate the translation of english terms into spanish terms. the approach is based on pattern matching morphological rules and word order inversion.
226,heuer-buschek-2021-methods,"   Methods for the Design and Evaluation of {HCI}+{NLP} Systems"",
",hci and nlp traditionally focus on different evaluation methods. while hci involves a small number of people directly and deeply nlp traditionally relies on standardized benchmark evaluations that involve a larger number of people indirectly. we present five methodological proposals at the intersection of hci and nlp and situate them in the context of ml-based nlp models. our goal is to foster interdisciplinary collaboration and progress in both fields by emphasizing what the fields can learn from each other.
227,van-der-lee-van-den-bosch-2017-exploring,"   Exploring Lexical and Syntactic Features for Language Variety Identification"",
",we present a method to discriminate between texts written in either the netherlandic or the flemish variant of the dutch language. the method draws on a feature bundle representing text statistics syntactic features and word $n$-grams. text statistics include average word length and sentence length while syntactic features include ratios of function words and part-of-speech $n$-grams. the effectiveness of the classifier was measured by classifying dutch subtitles developed for either dutch or flemish television. several machine learning algorithms were compared as well as feature combination methods in order to find the optimal generalization performance. a machine-learning meta classifier based on adaboost attained the best f-score of 0.92.
228,jang-etal-2019-pyopendial,"   {P}y{O}pen{D}ial: A Python-based Domain-Independent Toolkit for Developing Spoken Dialogue Systems with Probabilistic Rules"",
",we present pyopendial a python-based domain-independent open-source toolkit for spoken dialogue systems. recent advances in core components of dialogue systems such as speech recognition language understanding dialogue management and language generation harness deep learning to achieve state-of-the-art performance. the original opendial implemented in java provides a plugin architecture to integrate external modules but lacks python bindings making it difficult to interface with popular deep learning frameworks such as tensorflow or pytorch. to this end we re-implemented opendial in python and extended the toolkit with a number of novel functionalities for neural dialogue state tracking and action planning. we describe the overall architecture and its extensions and illustrate their use on an example where the system response model is implemented with a recurrent neural network.
229,hengchen-tahmasebi-2021-supersim,"   {S}uper{S}im: a test set for word similarity and relatedness in {S}wedish"",
",language models are notoriously difficult to evaluate. we release supersim a large-scale similarity and relatedness test set for swedish built with expert human judgements. the test set is composed of 1360 word-pairs independently judged for both relatedness and similarity by five annotators. we evaluate three different models (word2vec fasttext and glove) trained on two separate swedish datasets namely the swedish gigaword corpus and a swedish wikipedia dump to provide a baseline for future comparison. we will release the fully annotated test set code models and data.
230,zhang-etal-2021-findings,"   Findings of the Second Workshop on Automatic Simultaneous Translation"",
",this paper presents the results of the shared task of the 2nd workshop on automatic simultaneous translation (autosimtrans). the task includes two tracks one for text-to-text translation and one for speech-to-text requiring participants to build systems to translate from either the source text or speech into the target text. different from traditional machine translation the autosimtrans shared task evaluates not only translation quality but also latency. we propose a metric {``}monotonic optimal sequence{''} (mos) considering both quality and latency to rank the submissions. we also discuss some important open issues in simultaneous translation.
231,granero-moya-oikonomou-filandras-2021-taking,"   Taking Things Personally: Third Person to First Person Rephrasing"",
",the recent advancement of digital assistant technologies has opened new possibilities in the experiences they can provide. one of them is the ability to converse with a persona e.g. celebrities famous imaginary characters etc. this experience requires that the replies are answered from the point of view of the persona i.e. the first person. since the facts about characters are typically found expressed in the third person there is a need to rephrase them to the first person in order for the assistant not to break character and the experience to remain immersive. however the automatic solution to such a problem is largely unexplored by the community. in this work we present a new task for nlp: third person to first person rephrasing. we define the task and analyze its major challenges. we create and publish a novel dataset with 3493 human-annotated pairs of celebrity facts in the third person with their rephrased sentence in the first person. moreover we propose a transformer-based pipeline that correctly rephrases 92.0{\%} of sentences compared to 77.0{\%} rephrased by a rule-based baseline system.
232,zhang-etal-2019-recosa,"   {R}e{C}o{S}a: Detecting the Relevant Contexts with Self-Attention for Multi-turn Dialogue Generation"",
",in multi-turn dialogue generation response is usually related with only a few contexts. therefore an ideal model should be able to detect these relevant contexts and produce a suitable response accordingly. however the widely used hierarchical recurrent encoder-decoder models just treat all the contexts indiscriminately which may hurt the following response generation process. some researchers try to use the cosine similarity or the traditional attention mechanism to find the relevant contexts but they suffer from either insufficient relevance assumption or position bias problem. in this paper we propose a new model named recosa to tackle this problem. firstly a word level lstm encoder is conducted to obtain the initial representation of each context. then the self-attention mechanism is utilized to update both the context and masked response representation. finally the attention weights between each context and response representations are computed and used in the further decoding process. experimental results on both chinese customer services dataset and english ubuntu dialogue dataset show that recosa significantly outperforms baseline models in terms of both metric-based and human evaluations. further analysis on attention shows that the detected relevant contexts by recosa are highly coherent with human{'}s understanding validating the correctness and interpretability of recosa.
233,carlebach-etal-2020-news,"   News Aggregation with Diverse Viewpoint Identification Using Neural Embeddings and Semantic Understanding Models"",
",today{'}s news volume makes it impractical for readers to get a diverse and comprehensive view of published articles written from opposing viewpoints. we introduce a transformer-based news aggregation system composed of topic modeling semantic clustering claim extraction and textual entailment that identifies viewpoints presented in articles within a semantic cluster and classifies them into positive neutral and negative entailments. our novel embedded topic model using bert-based embeddings outperforms baseline topic modeling algorithms by an 11{\%} relative improvement. we compare recent semantic similarity models in the context of news aggregation evaluate transformer-based models for claim extraction on news data and demonstrate the use of textual entailment models for diverse viewpoint identification.
234,wang-etal-2017-tdparse,"   {TDP}arse: Multi-target-specific sentiment recognition on {T}witter"",
",existing target-specific sentiment recognition methods consider only a single target per tweet and have been shown to miss nearly half of the actual targets mentioned. we present a corpus of uk election tweets with an average of 3.09 entities per tweet and more than one type of sentiment in half of the tweets. this requires a method for multi-target specific sentiment recognition which we develop by using the context around a target as well as syntactic dependencies involving the target. we present results of our method on both a benchmark corpus of single targets and the multi-target election corpus showing state-of-the art performance in both corpora and outperforming previous approaches to multi-target sentiment task as well as deep learning models for single-target sentiment.
235,lafourcade-2006-conceptual,"   Conceptual Vector Learning - Comparing Bootstrapping from a Thesaurus or Induction by Emergence"",
",in the framework of the word sense disambiguation (wsd) and lexical transfer in machine translation (mt) the representation of word meanings is one critical issue. the conceptual vector model aims at representing thematic activations for chunks of text lexical entries up to whole documents. roughly speaking vectors are supposed to encode ideas associated to words or expressions. in this paper we first expose the conceptual vectors model and the notions of semantic distance and contextualization between terms. then we present in details the text analysis process coupled with conceptual vectors which is used in text classification thematic analysis and vector learning. the question we focus on is whether a thesaurus is really needed and desirable for bootstrapping the learning. we conducted two experiments with and without a thesaurus and are exposing here some comparative results. our contribution is that dimension distribution is done more regularly by an emergent procedure. in other words the resources are more efficiently exploited with an emergent procedure than with a thesaurus terms (concepts) as listed in a thesaurus somehow relate to their importance in the language but nor to their frequency in usage neither to their power of discrimination or representativeness.
236,zhang-etal-2020-summarizing,"   Summarizing {C}hinese Medical Answer with Graph Convolution Networks and Question-focused Dual Attention"",
",online search engines are a popular source of medical information for users where users can enter questions and obtain relevant answers. it is desirable to generate answer summaries for online search engines particularly summaries that can reveal direct answers to questions. moreover answer summaries are expected to reveal the most relevant information in response to questions; hence the summaries should be generated with a focus on the question which is a challenging topic-focused summarization task. in this paper we propose an approach that utilizes graph convolution networks and question-focused dual attention for chinese medical answer summarization. we first organize the original long answer text into a medical concept graph with graph convolution networks to better understand the internal structure of the text and the correlation between medical concepts. then we introduce a question-focused dual attention mechanism to generate summaries relevant to questions. experimental results demonstrate that the proposed model can generate more coherent and informative summaries compared with baseline models.
237,otto-2018-team,"   Team {GESIS} Cologne: An all in all sentence-based approach for {FEVER}"",
",in this system description of our pipeline to participate at the fever shared task we describe our sentence-based approach. throughout all steps of our pipeline we regarded single sentences as our processing unit. in our ir-component we searched in the set of all possible wikipedia introduction sentences without limiting sentences to a fixed number of relevant documents. in the entailment module we judged every sentence separately and combined the result of the classifier for the top 5 sentences with the help of an ensemble classifier to make a judgment whether the truth of a statement can be derived from the given claim.
238,mariani-etal-2014-facing,"   Facing the Identification Problem in Language-Related Scientific Data Analysis."",
",this paper describes the problems that must be addressed when studying large amounts of data over time which require entity normalization applied not to the usual genres of news or political speech but to the genre of academic discourse about language resources technologies and sciences. it reports on the normalization processes that had to be applied to produce data usable for computing statistics in three past studies on the lre map the isca archive and the ldc bibliography. it shows the need for human expertise during normalization and the necessity to adapt the work to the study objectives. it investigates possible improvements for reducing the workload necessary to produce comparable results. through this paper we show the necessity to define and agree on international persistent and unique identifiers.
239,niklaus-etal-2018-survey,"   A Survey on Open Information Extraction"",
",we provide a detailed overview of the various approaches that were proposed to date to solve the task of open information extraction. we present the major challenges that such systems face show the evolution of the suggested approaches over time and depict the specific issues they address. in addition we provide a critique of the commonly applied evaluation procedures for assessing the performance of open ie systems and highlight some directions for future work.
240,dipersio-cieri-2016-trends,"   Trends in {HLT} Research: A Survey of {LDC}{'}s Data Scholarship Program"",
",since its inception in 2010 the linguistic data consortium{'}s data scholarship program has awarded no cost grants in data to 64 recipients from 26 countries. a survey of the twelve cycles to date ― two awards each in the fall and spring semesters from fall 2010 through spring 2016 ― yields an interesting view into graduate program research trends in human language technology and related fields and the particular data sets deemed important to support that research. the survey also reveals regions in which such activity appears to be on a rise including in arabic-speaking regions and portions of the americas and asia.
241,abercrombie-batista-navarro-2019-semantic,"   Semantic Change in the Language of {UK} Parliamentary Debates"",
",we investigate changes in the meanings of words used in the uk parliament across two different epochs. we use word embeddings to explore changes in the distribution of words of interest and uncover words that appear to have undergone semantic transformation in the intervening period and explore different ways of obtaining target words for this purpose. we find that semantic changes are generally in line with those found in other corpora and little evidence that parliamentary language is more static than general english. it also seems that words with senses that have been recorded in the dictionary as having fallen into disuse do not undergo semantic changes in this domain.
242,perera-etal-2008-clios,"   {CLI}o{S}: Cross-lingual Induction of Speech Recognition Grammars"",
",we present an approach for the cross-lingual induction of speech recognition grammars that separates the task of translation from the task of grammar generation. the source speech recognition grammar is used to generate phrases which are translated by a common translation service. the target recognition grammar is induced by using the production rules of the source language manually translated sentences and a statistical word alignment tool. we induce grammars for the target languages spanish and japanese. the coverage of the resulting grammars is evaluated on two corpora and compared quantitatively and qualitatively to a grammar induced with unsupervised monolingual grammar induction.
243,uva-etal-2018-injecting,"   Injecting Relational Structural Representation in Neural Networks for Question Similarity"",
",effectively using full syntactic parsing information in neural networks (nns) for solving relational tasks e.g. question similarity is still an open problem. in this paper we propose to inject structural representations in nns by (i) learning a model with tree kernels (tks) on relatively few pairs of questions (few thousands) as gold standard (gs) training data is typically scarce (ii) predicting labels on a very large corpus of question pairs and (iii) pre-training nns on such large corpus. the results on quora and semeval question similarity datasets show that nns using our approach can learn more accurate models especially after fine tuning on gs.
244,collovini-etal-2016-sequence,"   A Sequence Model Approach to Relation Extraction in {P}ortuguese"",
",the task of relation extraction from texts is one of the main challenges in the area of information extraction considering the required linguistic knowledge and the sophistication of the language processing techniques employed. this task aims at identifying and classifying semantic relations that occur between entities recognized in a given text. in this paper we evaluated a conditional random fields classifier for the extraction of any relation descriptor occurring between named entities (organisation person and place categories) as well as pre-defined relation types between these entities in portuguese texts.
245,mulki-etal-2019-l,"   {L}-{HSAB}: A {L}evantine {T}witter Dataset for Hate Speech and Abusive Language"",
",hate speech and abusive language have become a common phenomenon on arabic social media. automatic hate speech and abusive detection systems can facilitate the prohibition of toxic textual contents. the complexity informality and ambiguity of the arabic dialects hindered the provision of the needed resources for arabic abusive/hate speech detection research. in this paper we introduce the first publicly-available levantine hate speech and abusive (l-hsab) twitter dataset with the objective to be a benchmark dataset for automatic detection of online levantine toxic contents. we further provide a detailed review of the data collection steps and how we design the annotation guidelines such that a reliable dataset annotation is guaranteed. this has been later emphasized through the comprehensive evaluation of the annotations as the annotation agreement metrics of cohen{'}s kappa (k) and krippendorff{'}s alpha (α) indicated the consistency of the annotations.
246,el-ballouli-etal-2017-cat,"   {CAT}: Credibility Analysis of {A}rabic Content on {T}witter"",
",data generated on twitter has become a rich source for various data mining tasks. those data analysis tasks that are dependent on the tweet semantics such as sentiment analysis emotion mining and rumor detection among others suffer considerably if the tweet is not credible not real or spam. in this paper we perform an extensive analysis on credibility of arabic content on twitter. we also build a classification model (cat) to automatically predict the credibility of a given arabic tweet. of particular originality is the inclusion of features extracted directly or indirectly from the author{'}s profile and timeline. to train and test cat we annotated for credibility a data set of 9000 arabic tweets that are topic independent. cat achieved consistent improvements in predicting the credibility of the tweets when compared to several baselines and when compared to the state-of-the-art approach with an improvement of 21{\%} in weighted average f-measure. we also conducted experiments to highlight the importance of the user-based features as opposed to the content-based features. we conclude our work with a feature reduction experiment that highlights the best indicative features of credibility.
247,alrowili-shanker-2021-arabictransformer-efficient,"   {A}rabic{T}ransformer: Efficient Large {A}rabic Language Model with Funnel Transformer and {ELECTRA} Objective"",
",pre-training transformer-based models such as bert and electra on a collection of arabic corpora demonstrated by both arabert and araelectra shows an impressive result on downstream tasks. however pre-training transformer-based language models is computationally expensive especially for large-scale models. recently funnel transformer has addressed the sequential redundancy inside transformer architecture by compressing the sequence of hidden states leading to a significant reduction in the pre-training cost. this paper empirically studies the performance and efficiency of building an arabic language model with funnel transformer and electra objective. we find that our model achieves state-of-the-art results on several arabic downstream tasks despite using less computational resources compared to other bert-based models.
248,sakaue-etal-2018-provable,"   Provable Fast Greedy Compressive Summarization with Any Monotone Submodular Function"",
",submodular maximization with the greedy algorithm has been studied as an effective approach to extractive summarization. this approach is known to have three advantages: its applicability to many useful submodular objective functions the efficiency of the greedy algorithm and the provable performance guarantee. however when it comes to compressive summarization we are currently missing a counterpart of the extractive method based on submodularity. in this paper we propose a fast greedy method for compressive summarization. our method is applicable to any monotone submodular objective function including many functions well-suited for document summarization. we provide an approximation guarantee of our greedy algorithm. experiments show that our method is about 100 to 400 times faster than an existing method based on integer-linear-programming (ilp) formulations and that our method empirically achieves more than 95{\%}-approximation.
249,rozovskaya-2021-spelling,"   Spelling Correction for {R}ussian: A Comparative Study of Datasets and Methods"",
",we develop a minimally-supervised model for spelling correction and evaluate its performance on three datasets annotated for spelling errors in russian. the first corpus is a dataset of russian social media data that was recently used in a shared task on russian spelling correction. the other two corpora contain texts produced by learners of russian as a foreign language. evaluating on three diverse datasets allows for a cross-corpus comparison. we compare the performance of the minimally-supervised model to two baseline models that do not use context for candidate re-ranking as well as to a character-level statistical machine translation system with context-based re-ranking. we show that the minimally-supervised model outperforms all of the other models. we also present an analysis of the spelling errors and discuss the difficulty of the task compared to the spelling correction problem in english.
250,abdelali-etal-2021-qadi,"   {QADI}: {A}rabic Dialect Identification in the Wild"",
",proper dialect identification is important for a variety of arabic nlp applications. in this paper we present a method for rapidly constructing a tweet dataset containing a wide range of country-level arabic dialects {---}covering 18 different countries in the middle east and north africa region. our method relies on applying multiple filters to identify users who belong to different countries based on their account descriptions and to eliminate tweets that either write mainly in modern standard arabic or mostly use vulgar language. the resultant dataset contains 540k tweets from 2525 users who are evenly distributed across 18 arab countries. using intrinsic evaluation we show that the labels of a set of randomly selected tweets are 91.5{\%} accurate. for extrinsic evaluation we are able to build effective country level dialect identification on tweets with a macro-averaged f1-score of 60.6{\%} across 18 classes.
251,mohanty-etal-2020-annotated,"   Annotated Corpus for Sentiment Analysis in {O}dia Language"",
",given the lack of an annotated corpus of non-traditional odia literature which serves as the standard when it comes sentiment analysis we have created an annotated corpus of odia sentences and made it publicly available to promote research in the field. secondly in order to test the usability of currently available odia sentiment lexicon we experimented with various classifiers by training and testing on the sentiment annotated corpus while using identified affective words from the same as features. annotation and classification are done at sentence level as the usage of sentiment lexicon is best suited to sentiment analysis at this level. the created corpus contains 2045 odia sentences from news domain annotated with sentiment labels using a well-defined annotation scheme. an inter-annotator agreement score of 0.79 is reported for the corpus.
252,abeywardana-thayasivam-2020-privacy,"   A Privacy Preserving Data Publishing Middleware for Unstructured, Textual Social Media Data"",
",privacy is going to be an integral part of data science and analytics in the coming years. the next hype of data experimentation is going to be heavily dependent on privacy preserving techniques mainly as it{'}s going to be a legal responsibility rather than a mere social responsibility. privacy preservation becomes more challenging specially in the context of unstructured data. social networks have become predominantly popular over the past couple of decades and they are creating a huge data lake at a high velocity. social media profiles contain a wealth of personal and sensitive information creating enormous opportunities for third parties to analyze them with different algorithms draw conclusions and use in disinformation campaigns and micro targeting based dark advertising. this study provides a mitigation mechanism for disinformation campaigns that are done based on the insights extracted from personal/sensitive data analysis. specifically this research is aimed at building a privacy preserving data publishing middleware for unstructured social media data without compromising the true analytical value of those data. a novel way is proposed to apply traditional structured privacy preserving techniques on unstructured data. creating a comprehensive twitter corpus annotated with privacy attributes is another objective of this research especially because the research community is lacking one.
253,leng-etal-2020-controllable,"   Controllable Neural Natural Language Generation: comparison of state-of-the-art control strategies"",
",most nlg systems target text fluency and grammatical correctness disregarding control over text structure and length. however control over the output plays an important part in industrial nlg applications. in this paper we study different strategies of control in triple-totext generation systems particularly from the aspects of text structure and text length. regarding text structure we present an approach that relies on aligning the input entities with the facts in the target side. it makes sure that the order and the distribution of entities in both the input and the text are the same. as for control over text length we show two different approaches. one is to supply length constraint as input while the other is to force the end-ofsentence tag to be included at each step when using top-k decoding strategy. finally we propose four metrics to assess the degree to which these methods will affect a nlg system{'}s ability to control text structure and length. our analyses demonstrate that all the methods enhance the system{'}s ability with a slight decrease in text fluency. in addition constraining length at the input level performs much better than control at decoding level.
254,madaan-sadat-2020-multilingual,"   Multilingual Neural Machine Translation involving {I}ndian Languages"",
",neural machine translations (nmt) models are capable of translating a single bilingual pair and require a new model for each new language pair. multilingual neural machine translation models are capable of translating multiple language pairs even pairs which it hasn{'}t seen before in training. availability of parallel sentences is a known problem in machine translation. multilingual nmt model leverages information from all the languages to improve itself and performs better. we propose a data augmentation technique that further improves this model profoundly. the technique helps achieve a jump of more than 15 points in bleu score from the multilingual nmt model. a bleu score of 36.2 was achieved for sindhi{--}english translation which is higher than any score on the leaderboard of the loresmt sharedtask at mt summit 2019 which provided the data for the experiments.
255,zhao-etal-2020-designing,"   Designing Precise and Robust Dialogue Response Evaluators"",
",automatic dialogue response evaluator has been proposed as an alternative to automated metrics and human evaluation. however existing automatic evaluators achieve only moderate correlation with human judgement and they are not robust. in this work we propose to build a reference-free evaluator and exploit the power of semi-supervised training and pretrained (masked) language models. experimental results demonstrate that the proposed evaluator achieves a strong correlation ({\textgreater} 0.6) with human judgement and generalizes robustly to diverse responses and corpora. we open-source the code and data in https://github.com/zhaoting/dialog-processing.
256,pagliardini-etal-2018-unsupervised,"   Unsupervised Learning of Sentence Embeddings Using Compositional n-Gram Features"",
",the recent tremendous success of unsupervised word embeddings in a multitude of applications raises the obvious question if similar methods could be derived to improve embeddings (i.e. semantic representations) of word sequences as well. we present a simple but efficient unsupervised objective to train distributed representations of sentences. our method outperforms the state-of-the-art unsupervised models on most benchmark tasks highlighting the robustness of the produced general-purpose sentence embeddings.
257,ruiter-etal-2010-human,"   Human Language Technology and Communicative Disabilities: Requirements and Possibilities for the Future"",
",for some years now the nederlandse taalunie (dutch language union) has been active in promoting the development of human language technology (hlt) applications for users of dutch with communication disabilities. the reason is that hlt products and services may enable these users to improve their verbal autonomy and communication skills. we sought to identify a minimum common set of hlt resources that is required to develop tools for a wide range of communication disabilities. in order to reach this goal we investigated the specific hlt needs of communicatively disabled people and related these needs to the underlying hlt software components. by analysing the availability and quality of these essential hlt resources we were able to identify which of the crucial elements need further research and development to become usable for developing applications for communicatively disabled users of dutch. the results obtained in the current survey can be used to inform policy institutions on how they can stimulate the development of hlt resources for this target group. in the current study results were obtained for dutch but a similar approach can also be used for other languages.
258,huang-etal-2021-retriever-reader,"   When Retriever-Reader Meets Scenario-Based Multiple-Choice Questions"",
",scenario-based question answering (sqa) requires retrieving and reading paragraphs from a large corpus to answer a question which is contextualized by a long scenario description. since a scenario contains both keyphrases for retrieval and much noise retrieval for sqa is extremely difficult. moreover it can hardly be supervised due to the lack of relevance labels of paragraphs for sqa. to meet the challenge in this paper we propose a joint retriever-reader model called jeeves where the retriever is implicitly supervised only using qa labels via a novel word weighting mechanism. jeeves significantly outperforms a variety of strong baselines on multiple-choice questions in three sqa datasets.
259,yin-etal-2021-signed,"   Signed Coreference Resolution"",
",coreference resolution is key to many natural language processing tasks and yet has been relatively unexplored in sign language processing. in signed languages space is primarily used to establish reference. solving coreference resolution for signed languages would not only enable higher-level sign language processing systems but also enhance our understanding of language in different modalities and of situated references which are key problems in studying grounded language. in this paper we: (1) introduce signed coreference resolution (scr) a new challenge for coreference modeling and sign language processing; (2) collect an annotated corpus of german sign language with gold labels for coreference together with an annotation software for the task; (3) explore features of hand gesture iconicity and spatial situated properties and move forward to propose a set of linguistically informed heuristics and unsupervised models for the task; (4) put forward several proposals about ways to address the complexities of this challenge effectively.
260,lu-etal-2020-supervised,"   Supervised Seeded Iterated Learning for Interactive Language Learning"",
",language drift has been one of the major obstacles to train language models through interaction. when word-based conversational agents are trained towards completing a task they tend to invent their language rather than leveraging natural language. in recent literature two general methods partially counter this phenomenon: supervised selfplay (s2p) and seeded iterated learning (sil). while s2p jointly trains interactive and supervised losses to counter the drift sil changes the training dynamics to prevent language drift from occurring. in this paper we first highlight their respective weaknesses i.e. late-stage training collapses and higher negative likelihood when evaluated on human corpus. given these observations we introduce supervised seeded iterated learning (ssil) to combine both methods to minimize their respective weaknesses. we then show the effectiveness of in the language-drift translation game.
261,wang-etal-2021-gender,"   Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in Image Search"",
",internet search affects people{'}s cognition of the world so mitigating biases in search results and learning fair models is imperative for social good. we study a unique gender bias in image search in this work: the search images are often gender-imbalanced for gender-neutral natural language queries. we diagnose two typical image search models the specialized model trained on in-domain datasets and the generalized representation model pre-trained on massive image and text data across the internet. both models suffer from severe gender bias. therefore we introduce two novel debiasing approaches: an in-processing fair sampling method to address the gender imbalance issue for training models and a post-processing feature clipping method base on mutual information to debias multimodal representations of pre-trained models. extensive experiments on ms-coco and flickr30k benchmarks show that our methods significantly reduce the gender bias in image search models.
262,kahardipraja-etal-2020-exploring,"   Exploring Span Representations in Neural Coreference Resolution"",
",in coreference resolution span representations play a key role to predict coreference links accurately. we present a thorough examination of the span representation derived by applying bert on coreference resolution (joshi et al. 2019) using a probing model. our results show that the span representation is able to encode a significant amount of coreference information. in addition we find that the head-finding attention mechanism involved in creating the spans is crucial in encoding coreference knowledge. last our analysis shows that the span representation cannot capture non-local coreference as efficiently as local coreference.
263,buechel-hahn-2017-readers,"   Readers vs. Writers vs. Texts: Coping with Different Perspectives of Text Understanding in Emotion Annotation"",
",we here examine how different perspectives of understanding written discourse like the reader{'}s the writer{'}s or the text{'}s point of view affect the quality of emotion annotations. we conducted a series of annotation experiments on two corpora a popular movie review corpus and a genre- and domain-balanced corpus of standard english. we found statistical evidence that the writer{'}s perspective yields superior annotation quality overall. however the quality one perspective yields compared to the other(s) seems to depend on the domain the utterance originates from. our data further suggest that the popular movie review data set suffers from an atypical bimodal distribution which may decrease model performance when used as a training resource.
264,ladhak-etal-2020-wikilingua,"   {W}iki{L}ingua: A New Benchmark Dataset for Cross-Lingual Abstractive Summarization"",
",we introduce wikilingua a large-scale multilingual dataset for the evaluation of cross-lingual abstractive summarization systems. we extract article and summary pairs in 18 languages from wikihow a high quality collaborative resource of how-to guides on a diverse set of topics written by human authors. we create gold-standard article-summary alignments across languages by aligning the images that are used to describe each how-to step in an article. as a set of baselines for further studies we evaluate the performance of existing cross-lingual abstractive summarization methods on our dataset. we further propose a method for direct cross-lingual summarization (i.e. without requiring translation at inference time) by leveraging synthetic data and neural machine translation as a pre-training step. our method significantly outperforms the baseline approaches while being more cost efficient during inference.
265,mahata-etal-2019-jumt,"   {JUMT} at {WMT}2019 News Translation Task: A Hybrid Approach to Machine Translation for {L}ithuanian to {E}nglish"",
",in the current work we present a description of the system submitted to wmt 2019 news translation shared task. the system was created to translate news text from lithuanian to english. to accomplish the given task our system used a word embedding based neural machine translation model to post edit the outputs generated by a statistical machine translation model. the current paper documents the architecture of our model descriptions of the various modules and the results produced using the same. our system garnered a bleu score of 17.6.
266,zirn-etal-2015-lost,"   Unsupervised Identification of Translationese"",
",translated texts are distinctively different from original ones to the extent that supervised text classification methods can distinguish between them with high accuracy. these differences were proven useful for statistical machine translation. however it has been suggested that the accuracy of translation detection deteriorates when the classifier is evaluated outside the domain it was trained on. we show that this is indeed the case in a variety of evaluation scenarios. we then show that unsupervised classification is highly accurate on this task. we suggest a method for determining the correct labels of the clustering outcomes and then use the labels for voting improving the accuracy even further. moreover we suggest a simple method for clustering in the challenging case of mixed-domain datasets in spite of the dominance of domain-related features over translation-related ones. the result is an effective fully-unsupervised method for distinguishing between original and translated texts that can be applied to new domains with reasonable accuracy.
267,ngo-etal-2019-transformer,"   How Transformer Revitalizes Character-based Neural Machine Translation: An Investigation on {J}apanese-{V}ietnamese Translation Systems"",
",while translating between east asian languages many works have discovered clear advantages of using characters as the translation unit. unfortunately traditional recurrent neural machine translation systems hinder the practical usage of those character-based systems due to their architectural limitations. they are unfavorable in handling extremely long sequences as well as highly restricted in parallelizing the computations. in this paper we demonstrate that the new transformer architecture can perform character-based trans- lation better than the recurrent one. we conduct experiments on a low-resource language pair: japanese-vietnamese. our models considerably outperform the state-of-the-art systems which employ word-based recurrent architectures.
268,van-den-bogaert-etal-2020-cefat4cities,"   {CEFAT}4{C}ities, a Natural Language Layer for the {ISA}2 Core Public Service Vocabulary"",
",the cefat4cities project (2020-2022) will create a {``}smart cities natural language context{''} (a software layer that facilitates the conversion of natural-language administrative procedures into machine-readable data sets) on top of the existing isa2 interoperability layer for public services. integration with the fiware/orion {``}smart city{''} context broker will make existing paper-based public services discoverable through {``}smart city{''} frameworks thus allowing for the development of more sophisticated and more user-friendly public services applications. an automated translation component will be included to provide a solution that can be used by all eu member states. as a result the project will allow eu citizens and businesses to interact with public services on the city national regional and eu level in their own language.
269,vadapalli-etal-2018-science,"   When science journalism meets artificial intelligence : An interactive demonstration"",
",we present an online interactive tool that generates titles of blog titles and thus take the first step toward automating science journalism. science journalism aims to transform jargon-laden scientific articles into a form that the common reader can comprehend while ensuring that the underlying meaning of the article is retained. in this work we present a tool which given the title and abstract of a research paper will generate a blog title by mimicking a human science journalist. the tool makes use of a model trained on a corpus of 87328 pairs of research papers and their corresponding blogs built from two science news aggregators. the architecture of the model is a two-stage mechanism which generates blog titles. evaluation using standard metrics indicate the viability of the proposed system.
270,muzerelle-etal-2014-ancor,"   {ANCOR}{\_}{C}entre, a large free spoken {F}rench coreference corpus: description of the resource and reliability measures"",
",this article presents ancor{\_}centre a french coreference corpus available under the creative commons licence. with a size of around 500000 words the corpus is large enough to serve the needs of data-driven approaches in nlp and represents one of the largest coreference resources currently available. the corpus focuses exclusively on spoken language it aims at representing a certain variety of spoken genders. ancor{\_}centre includes anaphora as well as coreference relations which involve nominal and pronominal mentions. the paper describes into details the annotation scheme and the reliability measures computed on the resource.
271,swanson-etal-2019-building,"   Building a Production Model for Retrieval-Based Chatbots"",
",response suggestion is an important task for building human-computer conversation systems. recent approaches to conversation modeling have introduced new model architectures with impressive results but relatively little attention has been paid to whether these models would be practical in a production setting. in this paper we describe the unique challenges of building a production retrieval-based conversation system which selects outputs from a whitelist of candidate responses. to address these challenges we propose a dual encoder architecture which performs rapid inference and scales well with the size of the whitelist. we also introduce and compare two methods for generating whitelists and we carry out a comprehensive analysis of the model and whitelists. experimental results on a large proprietary help desk chat dataset including both offline metrics and a human evaluation indicate production-quality performance and illustrate key lessons about conversation modeling in practice.
272,kitazawa-etal-2008-multimodal,"   A Multimodal Infant Behavior Annotation for Developmental Analysis of Demonstrative Expressions"",
",we have obtained the valuable findings about the developmental processes of demonstrative expression skills which is concerned with the fundamental commonsense of human knowledge such as to get an object and to catch someones attention. we have already developed a framework to record genuine spontaneous speech of infants. we are constructing a multimodal infant behavior corpus which enables us to elucidate human commonsense knowledge and its acquisition mechanism. based on the observation of the corpus we proposed a multimodal behavior description for observation of demonstrative expressions. we proved that the proposed model has the nearly 90{\%} coverage in an open test of the behavior description task. the analysis using the model produced many valuable findings from multimodal viewpoints; for example the change of line of sight from object to person to person to object means that the infant has obtained a better way to catch someones attention. our intention-based analysis provided us with an infant behavior model that may apply to a likely behavior simulation system.
273,lee-etal-2020-empowering,"   {E}mpowering {A}ctive {L}earning to {J}ointly {O}ptimize {S}ystem and {U}ser {D}emands"",
",existing approaches to active learning maximize the system performance by sampling unlabeled instances for annotation that yield the most efficient training. however when active learning is integrated with an end-user application this can lead to frustration for participating users as they spend time labeling instances that they would not otherwise be interested in reading. in this paper we propose a new active learning approach that jointly optimizes the seemingly counteracting objectives of the active learning system (training efficiently) and the user (receiving useful instances). we study our approach in an educational application which particularly benefits from this technique as the system needs to rapidly learn to predict the appropriateness of an exercise to a particular user while the users should receive only exercises that match their skills. we evaluate multiple learning strategies and user types with data from real users and find that our joint approach better satisfies both objectives when alternative methods lead to many unsuitable exercises for end users.
274,xia-monti-2021-multilingual,"   Multilingual Neural Semantic Parsing for Low-Resourced Languages"",
",multilingual semantic parsing is a cost-effective method that allows a single model to understand different languages. however researchers face a great imbalance of availability of training data with english being resource rich and other languages having much less data. to tackle the data limitation problem we propose using machine translation to bootstrap multilingual training data from the more abundant english data. to compensate for the data quality of machine translated training data we utilize transfer learning from pretrained multilingual encoders to further improve the model. to evaluate our multilingual models on human-written sentences as opposed to machine translated ones we introduce a new multilingual semantic parsing dataset in english italian and japanese based on the facebook task oriented parsing (top) dataset. we show that joint multilingual training with pretrained encoders substantially outperforms our baselines on the top dataset and outperforms the state-of-the-art model on the public nlmaps dataset. we also establish a new baseline for zero-shot learning on the top dataset. we find that a semantic parser trained only on english data achieves a zero-shot performance of 44.9{\%} exact-match accuracy on italian sentences.
275,kocmi-bojar-2019-cuni,"   {CUNI} Submission for Low-Resource Languages in {WMT} News 2019"",
",this paper describes the cuni submission to the wmt 2019 news translation shared task for the low-resource languages: gujarati-english and kazakh-english. we participated in both language pairs in both translation directions. our system combines transfer learning from a different high-resource language pair followed by training on backtranslated monolingual data. thanks to the simultaneous training in both directions we can iterate the backtranslation process. we are using the transformer model in a constrained submission.
276,kleinberg-etal-2018-identifying,"   Identifying the sentiment styles of {Y}ou{T}ube{'}s vloggers"",
",vlogs provide a rich public source of data in a novel setting. this paper examined the continuous sentiment styles employed in 27333 vlogs using a dynamic intra-textual approach to sentiment analysis. using unsupervised clustering we identified seven distinct continuous sentiment trajectories characterized by fluctuations of sentiment throughout a vlog{'}s narrative time. we provide a taxonomy of these seven continuous sentiment styles and found that vlogs whose sentiment builds up towards a positive ending are the most prevalent in our sample. gender was associated with preferences for different continuous sentiment trajectories. this paper discusses the findings with respect to previous work and concludes with an outlook towards possible uses of the corpus method and findings of this paper for related areas of research.
277,peyrard-eckle-kohler-2016-general,"   A General Optimization Framework for Multi-Document Summarization Using Genetic Algorithms and Swarm Intelligence"",
",extracting summaries via integer linear programming and submodularity are popular and successful techniques in extractive multi-document summarization. however many interesting optimization objectives are neither submodular nor factorizable into an integer linear program. we address this issue and present a general optimization framework where any function of input documents and a system summary can be plugged in. our framework includes two kinds of summarizers {--} one based on genetic algorithms the other using a swarm intelligence approach. in our experimental evaluation we investigate the optimization of two information-theoretic summary evaluation metrics and find that our framework yields competitive results compared to several strong summarization baselines. our comparative analysis of the genetic and swarm summarizers reveals interesting complementary properties.
278,arnold-etal-2019-interpretation,"   Interpr{\'e}tation et visualisation contextuelle de {NOTAM}s (messages aux navigants a{\'e}riens) ()"",
",dans cet article nous pr{\'e}sentons une d{\'e}monstration de visualisation de l{'}information extraite automatiquement de la partie textuelle des notams. dans le domaine a{\'e}ronautique les notams sont des messages publi{\'e}s par les agences gouvernementales de contr{\^o}le de la navigation a{\'e}rienne. nous d{\'e}taillons la construction du jeu de donn{\'e}es les exp{\'e}riences d{'}extraction d{'}information par apprentissage profond (approche et r{\'e}sultats) ainsi que le lien avec la visualisation contextuelle sur des cartes d{'}a{\'e}roports.
279,wang-etal-2021-relation,"   Relation-aware Bidirectional Path Reasoning for Commonsense Question Answering"",
",commonsense question answering is an important natural language processing (nlp) task that aims to predict the correct answer to a question through commonsense reasoning. previous studies utilize pre-trained models on large-scale corpora such as bert or perform reasoning on knowledge graphs. however these methods do not explicitly model the \textit{relations} that connect entities which are informational and can be used to enhance reasoning. to address this issue we propose a relation-aware reasoning method. our method uses a relation-aware graph neural network to capture the rich contextual information from both entities and relations. compared with methods that use fixed relation embeddings from pre-trained models our model dynamically updates relations with contextual information from a multi-source subgraph built from multiple external knowledge sources. the enhanced representations of relations are then fed to a bidirectional reasoning module. a bidirectional attention mechanism is applied between the question sequence and the paths that connect entities which provides us with transparent interpretability. experimental results on the commonsenseqa dataset illustrate that our method results in significant improvements over the baselines while also providing clear reasoning paths.
280,enayati-etal-2021-visualization,"   A Visualization Approach for Rapid Labeling of Clinical Notes for Smoking Status Extraction"",
",labeling is typically the most human-intensive step during the development of supervised learning models. in this paper we propose a simple and easy-to-implement visualization approach that reduces cognitive load and increases the speed of text labeling. the approach is fine-tuned for task of extraction of patient smoking status from clinical notes. the proposed approach consists of the ordering of sentences that mention smoking centering them at smoking tokens and annotating to enhance informative parts of the text. our experiments on clinical notes from the mimic-iii clinical database demonstrate that our visualization approach enables human annotators to label sentences up to 3 times faster than with a baseline approach.
281,ding-etal-2003-algorithm,"   An algorithm for word-level alignment of parallel dependency trees"",
",structural divergence presents a challenge to the use of syntax in statistical machine translation. we address this problem with a new algorithm for alignment of loosely matched non-isomorphic dependency trees. the algorithm selectively relaxes the constraints of the two tree structures while keeping computational complexity polynomial in the length of the sentences. experimentation with a large chinese-english corpus shows an improvement in alignment results over the unstructured models of (brown et al. 1993).
282,socha-2020-ks,"   {KS}@{LTH} at {S}em{E}val-2020 Task 12: Fine-tuning Multi- and Monolingual Transformer Models for Offensive Language Detection"",
",this paper describes the ks@lth system for semeval-2020 task 12 offenseval2: multilingual offensive language identification in social media. we compare mono- and multilingual models based on fine-tuning pre-trained transformer models for offensive language identification in arabic greek english and turkish. for danish we explore the possibility of fine-tuning a model pre-trained on a similar language swedish and additionally also cross-lingual training together with english.
283,korvas-etal-2014-free,"   Free {E}nglish and {C}zech telephone speech corpus shared under the {CC}-{BY}-{SA} 3.0 license"",
",we present a dataset of telephone conversations in english and czech developed for training acoustic models for automatic speech recognition (asr) in spoken dialogue systems (sdss). the data comprise 45 hours of speech in english and over 18 hours in czech. large part of the data both audio and transcriptions was collected using crowdsourcing the rest are transcriptions by hired transcribers. we release the data together with scripts for data pre-processing and building acoustic models using the htk and kaldi asr toolkits. we publish also the trained models described in this paper. the data are released under the cc-by-sa{\textasciitilde}3.0 license the scripts are licensed under apache{\textasciitilde}2.0. in the paper we report on the methodology of collecting the data on the size and properties of the data and on the scripts and their use. we verify the usability of the datasets by training and evaluating acoustic models using the presented data and scripts.
284,milbauer-etal-2021-aligning,"   Aligning Multidimensional Worldviews and Discovering Ideological Differences"",
",the internet is home to thousands of communities each with their own unique worldview and associated ideological differences. with new communities constantly emerging and serving as ideological birthplaces battlegrounds and bunkers it is critical to develop a framework for understanding worldviews and ideological distinction. most existing work however takes a predetermined view based on political polarization: the {``}right vs. left{''} dichotomy of u.s. politics. in reality both political polarization {--} and worldviews more broadly {--} transcend one-dimensional difference and deserve a more complete analysis. extending the ability of word embedding models to capture the semantic and cultural characteristics of their training corpora we propose a novel method for discovering the multifaceted ideological and worldview characteristics of communities. using over 1b comments collected from the largest communities on reddit.com representing {\textasciitilde}40{\%} of reddit activity we demonstrate the efficacy of this approach to uncover complex ideological differences across multiple axes of polarization.
285,zhao-etal-2021-meta,"   Meta-Reinforcement Learning for Mastering Multiple Skills and Generalizing across Environments in Text-based Games"",
",text-based games can be used to develop task-oriented text agents for accomplishing tasks with high-level language instructions which has potential applications in domains such as human-robot interaction. given a text instruction reinforcement learning is commonly used to train agents to complete the intended task owing to its convenience of learning policies automatically. however because of the large space of combinatorial text actions learning a policy network that generates an action word by word with reinforcement learning is challenging. recent research works show that imitation learning provides an effective way of training a generation-based policy network. however trained agents with imitation learning are hard to master a wide spectrum of task types or skills and it is also difficult for them to generalize to new environments. in this paper we propose a meta reinforcement learning based method to train text agents through learning-to-explore. in particular the text agent first explores the environment to gather task-specific information and then adapts the execution policy for solving the task with this information. on the publicly available testbed alfworld we conducted a comparison study with imitation learning and show the superiority of our method.
286,lee-etal-2016-make,"   Can We Make Computers Laugh at Talks?"",
",considering the importance of public speech skills a system which makes a prediction on where audiences laugh in a talk can be helpful to a person who prepares for a talk. we investigated a possibility that a state-of-the-art humor recognition system can be used in detecting sentences inducing laughters in talks. in this study we used ted talks and laughters in the talks as data. our results showed that the state-of-the-art system needs to be improved in order to be used in a practical application. in addition our analysis showed that classifying humorous sentences in talks is very challenging due to close distance between humorous and non-humorous sentences.
287,nguyen-etal-2021-rst,"   {RST} Parsing from Scratch"",
",we introduce a novel top-down end-to-end formulation of document level discourse parsing in the rhetorical structure theory (rst) framework. in this formulation we consider discourse parsing as a sequence of splitting decisions at token boundaries and use a seq2seq network to model the splitting decisions. our framework facilitates discourse parsing from scratch without requiring discourse segmentation as a prerequisite; rather it yields segmentation as part of the parsing process. our unified parsing model adopts a beam search to decode the best tree structure by searching through a space of high scoring trees. with extensive experiments on the standard rst discourse treebank we demonstrate that our parser outperforms existing methods by a good margin in both end-to-end parsing and parsing with gold segmentation. more importantly it does so without using any handcrafted features making it faster and easily adaptable to new languages and domains.
288,excell-al-moubayed-2021-towards,"   Towards Equal Gender Representation in the Annotations of Toxic Language Detection"",
",classifiers tend to propagate biases present in the data on which they are trained. hence it is important to understand how the demographic identities of the annotators of comments affect the fairness of the resulting model. in this paper we focus on the differences in the ways men and women annotate comments for toxicity investigating how these differences result in models that amplify the opinions of male annotators. we find that the bert model associates toxic comments containing offensive words with male annotators causing the model to predict 67.7{\%} of toxic comments as having been annotated by men. we show that this disparity between gender predictions can be mitigated by removing offensive words and highly toxic comments from the training data. we then apply the learned associations between gender and language to toxic language classifiers finding that models trained exclusively on female-annotated data perform 1.8{\%} better than those trained solely on male-annotated data and that training models on data after removing all offensive words reduces bias in the model by 55.5{\%} while increasing the sensitivity by 0.4{\%}.
289,nadeem-etal-2021-stereoset,"   {S}tereo{S}et: Measuring stereotypical bias in pretrained language models"",
",a stereotype is an over-generalized belief about a particular group of people e.g. asians are good at math or african americans are athletic. such beliefs (biases) are known to hurt target groups. since pretrained language models are trained on large real-world data they are known to capture stereotypical biases. it is important to quantify to what extent these biases are present in them. although this is a rapidly growing area of research existing literature lacks in two important aspects: 1) they mainly evaluate bias of pretrained language models on a small set of artificial sentences even though these models are trained on natural data 2) current evaluations focus on measuring bias without considering the language modeling ability of a model which could lead to misleading trust on a model even if it is a poor language model. we address both these problems. we present stereoset a large-scale natural english dataset to measure stereotypical biases in four domains: gender profession race and religion. we contrast both stereotypical bias and language modeling ability of popular models like bert gpt-2 roberta and xlnet. we show that these models exhibit strong stereotypical biases. our data and code are available at https://stereoset.mit.edu.
290,jiang-etal-2021-multi,"   Multi-hop Graph Convolutional Network with High-order Chebyshev Approximation for Text Reasoning"",
",graph convolutional network (gcn) has become popular in various natural language processing (nlp) tasks with its superiority in long-term and non-consecutive word interactions. however existing single-hop graph reasoning in gcn may miss some important non-consecutive dependencies. in this study we define the spectral graph convolutional network with the high-order dynamic chebyshev approximation (hdgcn) which augments the multi-hop graph reasoning by fusing messages aggregated from direct and long-term dependencies into one convolutional layer. to alleviate the over-smoothing in high-order chebyshev approximation a multi-vote-based cross-attention (mvcattn) with linear computation complexity is also proposed. the empirical results on four transductive and inductive nlp tasks and the ablation study verify the efficacy of the proposed model.
291,emami-etal-2018-generalized,"   A Generalized Knowledge Hunting Framework for the {W}inograd Schema Challenge"",
",we introduce an automatic system that performs well on two common-sense reasoning tasks the winograd schema challenge (wsc) and the choice of plausible alternatives (copa). problem instances from these tasks require diverse complex forms of inference and knowledge to solve. our method uses a knowledge-hunting module to gather text from the web which serves as evidence for candidate problem resolutions. given an input problem our system generates relevant queries to send to a search engine. it extracts and classifies knowledge from the returned results and weighs it to make a resolution. our approach improves f1 performance on the wsc by 0.16 over the previous best and is competitive with the state-of-the-art on copa demonstrating its general applicability.
292,liu-etal-2018-learning,"   Learning to Actively Learn Neural Machine Translation"",
",traditional active learning (al) methods for machine translation (mt) rely on heuristics. however these heuristics are limited when the characteristics of the mt problem change due to e.g. the language pair or the amount of the initial bitext. in this paper we present a framework to learn sentence selection strategies for neural mt. we train the al query strategy using a high-resource language-pair based on al simulations and then transfer it to the low-resource language-pair of interest. the learned query strategy capitalizes on the shared characteristics between the language pairs to make an effective use of the al budget. our experiments on three language-pairs confirms that our method is more effective than strong heuristic-based methods in various conditions including cold-start and warm-start as well as small and extremely small data conditions.
293,gong-etal-2019-reinforcement,"   Reinforcement Learning Based Text Style Transfer without Parallel Training Corpus"",
",text style transfer rephrases a text from a source style (e.g. informal) to a target style (e.g. formal) while keeping its original meaning. despite the success existing works have achieved using a parallel corpus for the two styles transferring text style has proven significantly more challenging when there is no parallel training corpus. in this paper we address this challenge by using a reinforcement-learning-based generator-evaluator architecture. our generator employs an attention-based encoder-decoder to transfer a sentence from the source style to the target style. our evaluator is an adversarially trained style discriminator with semantic and syntactic constraints that score the generated sentence for style meaning preservation and fluency. experimental results on two different style transfer tasks{--}sentiment transfer and formality transfer{--}show that our model outperforms state-of-the-art approaches.furthermore we perform a manual evaluation that demonstrates the effectiveness of the proposed method using subjective metrics of generated text quality.
294,pannitto-etal-2021-teaching,"   {M}asakha{NER}: Named Entity Recognition for {A}frican Languages"",
",abstract we take a step towards addressing the under- representation of the african continent in nlp research by bringing together different stakeholders to create the first large publicly available high-quality dataset for named entity recognition (ner) in ten african languages. we detail the characteristics of these languages to help researchers and practitioners better understand the challenges they pose for ner tasks. we analyze our datasets and conduct an extensive empirical evaluation of state- of-the-art methods across both supervised and transfer learning settings. finally we release the data code and models to inspire future research on african nlp.1
295,krishnan-ragavan-2021-morphology,"   Morphology-Aware Meta-Embeddings for {T}amil"",
",in this work we explore generating morphologically enhanced word embeddings for tamil a highly agglutinative south indian language with rich morphology that remains low-resource with regards to nlp tasks. we present here the first-ever word analogy dataset for tamil consisting of 4499 hand-curated word tetrads across 10 semantic and 13 morphological relation types. using a rules-based segmenter to capture morphology as well as meta-embedding techniques we train meta-embeddings that outperform existing baselines by 16{\%} on our analogy task and appear to mitigate a previously observed trade-off between semantic and morphological accuracy.
296,ribeiro-etal-2001-cognates,"   Cognates alignment"",
",some authors (simard et al.; melamed; danielsson {\&} mühlenbock) have suggested measures of similarity of words in different languages so as to find extra clues for alignment of parallel texts. cognate words like {`}parliament{'} and {`}parlement{'} in english and french respectively provide extra anchors that help to improve the quality of the alignment. in this paper we will extend an alignment algorithm proposed by ribeiro et al. using typical contiguous and non-contiguous sequences of characters extracted using a statistically sound method (dias et al.). with these typical sequences we are able to find more reliable correspondence points and improve the alignment quality without recurring to heuristics to identify cognates.
297,kumar-bora-2018-part,"   Part-of-Speech Annotation of {E}nglish-{A}ssamese code-mixed texts: Two Approaches"",
",in this paper we discuss the development of a part-of-speech tagger for english-assamese code-mixed texts. we provide a comparison of 2 approaches to annotating code-mixed data {--} a) annotation of the texts from the two languages using monolingual resources from each language and b) annotation of the text through a different resource created specifically for code-mixed data. we present a comparative study of the efforts required in each approach and the final performance of the system. based on this we argue that it might be a better approach to develop new technologies using code-mixed data instead of monolingual {`}clean{'} data especially for those languages where we do not have significant tools and technologies available till now.
298,qarqaz-etal-2021-r00,"   R00 at {NLP}4{IF}-2021 Fighting {COVID}-19 Infodemic with Transformers and More Transformers"",
",this paper describes the winning model in the arabic nlp4if shared task for fighting the covid-19 infodemic. the goal of the shared task is to check disinformation about covid-19 in arabic tweets. our proposed model has been ranked 1st with an f1-score of 0.780 and an accuracy score of 0.762. a variety of transformer-based pre-trained language models have been experimented with through this study. the best-scored model is an ensemble of arabert-base asafya-bert and arbert models. one of the study{'}s key findings is showing the effect the pre-processing can have on every model{'}s score. in addition to describing the winning model the current study shows the error analysis.
299,xu-etal-2018-incorporating,"   Incorporating Latent Meanings of Morphological Compositions to Enhance Word Embeddings"",
",traditional word embedding approaches learn semantic information at word level while ignoring the meaningful internal structures of words like morphemes. furthermore existing morphology-based models directly incorporate morphemes to train word embeddings but still neglect the latent meanings of morphemes. in this paper we explore to employ the latent meanings of morphological compositions of words to train and enhance word embeddings. based on this purpose we propose three latent meaning models (lmms) named lmm-a lmm-s and lmm-m respectively which adopt different strategies to incorporate the latent meanings of morphemes during the training process. experiments on word similarity syntactic analogy and text classification are conducted to validate the feasibility of our models. the results demonstrate that our models outperform the baselines on five word similarity datasets. on wordsim-353 and rg-65 datasets our models nearly achieve 5{\%} and 7{\%} gains over the classic cbow model respectively. for the syntactic analogy and text classification tasks our models also surpass all the baselines including a morphology-based model.
300,peng-etal-2018-sequence,"   Sequence-to-sequence Models for Cache Transition Systems"",
",in this paper we present a sequence-to-sequence based approach for mapping natural language sentences to amr semantic graphs. we transform the sequence to graph mapping problem to a word sequence to transition action sequence problem using a special transition system called a cache transition system. to address the sparsity issue of neural amr parsing we feed feature embeddings from the transition state to provide relevant local information for each decoder state. we present a monotonic hard attention model for the transition framework to handle the strictly left-to-right alignment between each transition state and the current buffer input focus. we evaluate our neural transition model on the amr parsing task and our parser outperforms other sequence-to-sequence approaches and achieves competitive results in comparison with the best-performing models.
301,bergmanis-goldwater-2018-context,"   Context Sensitive Neural Lemmatization with {L}ematus"",
",the main motivation for developing contextsensitive lemmatizers is to improve performance on unseen and ambiguous words. yet previous systems have not carefully evaluated whether the use of context actually helps in these cases. we introduce lematus a lemmatizer based on a standard encoder-decoder architecture which incorporates character-level sentence context. we evaluate its lemmatization accuracy across 20 languages in both a full data setting and a lower-resource setting with 10k training examples in each language. in both settings we show that including context significantly improves results against a context-free version of the model. context helps more for ambiguous words than for unseen words though the latter has a greater effect on overall performance differences between languages. we also compare to three previous context-sensitive lemmatization systems which all use pre-extracted edit trees as well as hand-selected features and/or additional sources of information such as tagged training data. without using any of these our context-sensitive model outperforms the best competitor system (lemming) in the fulldata setting and performs on par in the lowerresource setting.
302,wen-etal-2017-network,"   A Network-based End-to-End Trainable Task-oriented Dialogue System"",
",teaching machines to accomplish tasks by conversing naturally with humans is challenging. currently developing task-oriented dialogue systems requires creating multiple components and typically this involves either a large amount of handcrafting or acquiring costly labelled datasets to solve a statistical learning problem for each component. in this work we introduce a neural network-based text-in text-out end-to-end trainable goal-oriented dialogue system along with a new way of collecting dialogue data based on a novel pipe-lined wizard-of-oz framework. this approach allows us to develop dialogue systems easily and without making too many assumptions about the task at hand. the results show that the model can converse with human subjects naturally whilst helping them to accomplish tasks in a restaurant search domain.
303,lee-etal-2017-dialport,"   {D}ial{P}ort, Gone Live: An Update After A Year of Development"",
",dialport collects user data for connected spoken dialog systems. at present six systems are linked to a central portal that directs the user to the applicable system and suggests systems that the user may be interested in. user data has started to flow into the system.
304,daval-frerot-weis-2020-wmd,"   {WMD} at {S}em{E}val-2020 Tasks 7 and 11: Assessing Humor and Propaganda Using Unsupervised Data Augmentation"",
",in this work we combine the state-of-the-art bert architecture with the semi-supervised learning technique uda in order to exploit unlabeled raw data to assess humor and detect propaganda in the tasks 7 and 11 of the semeval-2020 competition. the use of uda shows promising results with a systematic improvement of the performances over the four different subtasks and even outperforms supervised learning with the additional labels of the funlines dataset.
305,sawhney-etal-2021-dmix,"   {DM}ix: Distance Constrained Interpolative Mixup"",
",interpolation-based regularisation methods have proven to be effective for various tasks and modalities. mixup is a data augmentation method that generates virtual training samples from convex combinations of individual inputs and labels. we extend mixup and propose dmix distance-constrained interpolative mixup for sentence classification leveraging the hyperbolic space. dmix achieves state-of-the-art results on sentence classification over existing data augmentation methods across datasets in four languages.
306,windhouwer-2012-relcat,"   {REL}cat: a Relation Registry for {ISO}cat data categories"",
",the isocat data category registry contains basically a flat and easily extensible list of data category specifications. to foster reuse and standardization only very shallow relationships among data categories are stored in the registry. however to assist crosswalks possibly based on personal views between various (application) domains and to overcome possible proliferation of data categories more types of ontological relationships need to be specified. relcat is a first prototype of a relation registry which allows storing arbitrary relationships. these relationships can reflect the personal view of one linguist or a larger community. the basis of the registry is a relation type taxonomy that can easily be extended. this allows on one hand to load existing sets of relations specified in for example an owl (2) ontology or skos taxonomy. and on the other hand allows algorithms that query the registry to traverse the stored semantic network to remain ignorant of the original source vocabulary. this paper describes first experiences with relcat and explains some initial design decisions.
307,zhao-etal-2018-language,"   A Language Model based Evaluator for Sentence Compression"",
",we herein present a language-model-based evaluator for deletion-based sentence compression and view this task as a series of deletion-and-evaluation operations using the evaluator. more specifically the evaluator is a syntactic neural language model that is first built by learning the syntactic and structural collocation among words. subsequently a series of trial-and-error deletion operations are conducted on the source sentences via a reinforcement learning framework to obtain the best target compression. an empirical study shows that the proposed model can effectively generate more readable compression comparable or superior to several strong baselines. furthermore we introduce a 200-sentence test set for a large-scale dataset setting a new baseline for the future research.
308,bizzoni-etal-2014-making,"   The Making of {A}ncient {G}reek {W}ord{N}et"",
",this paper describes the process of creation and review of a new lexico-semantic resource for the classical studies: ancientgreekwordnet. the candidate sets of synonyms (synsets) are extracted from greek-english dictionaries on the assumption that greek words translated by the same english word or phrase have a high probability of being synonyms or at least semantically closely related. the process of validation and the web interface developed to edit and query the resource are described in detail. the lexical coverage of ancient greek wordnet is illustrated and the accuracy is evaluated. finally scenarios for exploiting the resource are discussed.
309,wang-etal-2021-mode,"   Mode Effects{'} Challenge to Authorship Attribution"",
",the success of authorship attribution relies on the presence of linguistic features specific to individual authors. there is however limited research assessing to what extent authorial style remains constant when individuals switch from one writing modality to another. we measure the effect of writing mode on writing style in the context of authorship attribution research using a corpus of documents composed online (in a web browser) and documents composed offline using a traditional word processor. the results confirm the existence of a {``}mode effect{''} on authorial style. online writing differs systematically from offline writing in terms of sentence length word use readability and certain part-of-speech ratios. these findings have implications for research design and feature engineering in authorship attribution studies.
310,xie-zeng-2021-mixture,"   A Mixture-of-Experts Model for Antonym-Synonym Discrimination"",
",discrimination between antonyms and synonyms is an important and challenging nlp task. antonyms and synonyms often share the same or similar contexts and thus are hard to make a distinction. this paper proposes two underlying hypotheses and employs the mixture-of-experts framework as a solution. it works on the basis of a divide-and-conquer strategy where a number of localized experts focus on their own domains (or subspaces) to learn their specialties and a gating mechanism determines the space partitioning and the expert mixture. experimental results have shown that our method achieves the state-of-the-art performance on the task.
311,chen-etal-2020-synchronous,"   Synchronous Double-channel Recurrent Network for Aspect-Opinion Pair Extraction"",
",opinion entity extraction is a fundamental task in fine-grained opinion mining. related studies generally extract aspects and/or opinion expressions without recognizing the relations between them. however the relations are crucial for downstream tasks including sentiment classification opinion summarization etc. in this paper we explore aspect-opinion pair extraction (aope) task which aims at extracting aspects and opinion expressions in pairs. to deal with this task we propose synchronous double-channel recurrent network (sdrn) mainly consisting of an opinion entity extraction unit a relation detection unit and a synchronization unit. the opinion entity extraction unit and the relation detection unit are developed as two channels to extract opinion entities and relations simultaneously. furthermore within the synchronization unit we design entity synchronization mechanism (esm) and relation synchronization mechanism (rsm) to enhance the mutual benefit on the above two channels. to verify the performance of sdrn we manually build three datasets based on semeval 2014 and 2015 benchmarks. extensive experiments demonstrate that sdrn achieves state-of-the-art performances.
312,jana-etal-2019-compositionality,"   On the Compositionality Prediction of Noun Phrases using Poincar{\'e} Embeddings"",
",the compositionality degree of multiword expressions indicates to what extent the meaning of a phrase can be derived from the meaning of its constituents and their grammatical relations. prediction of (non)-compositionality is a task that has been frequently addressed with distributional semantic models. we introduce a novel technique to blend hierarchical information with distributional information for predicting compositionality. in particular we use hypernymy information of the multiword and its constituents encoded in the form of the recently introduced poincar{\'e} embeddings in addition to the distributional information to detect compositionality for noun phrases. using a weighted average of the distributional similarity and a poincar{\'e} similarity function we obtain consistent and substantial statistically significant improvement across three gold standard datasets over state-of-the-art models based on distributional information only. unlike traditional approaches that solely use an unsupervised setting we have also framed the problem as a supervised task obtaining comparable improvements. further we publicly release our poincar{\'e} embeddings which are trained on the output of handcrafted lexical-syntactic patterns on a large corpus.
313,schafer-etal-2008-extracting,"   Extracting and Querying Relations in Scientific Papers on Language Technology"",
",we describe methods for extracting interesting factual relations from scientific texts in computational linguistics and language technology taken from the acl anthology. we use a hybrid nlp architecture with shallow preprocessing for increased robustness and domain-specific ontology-based named entity recognition followed by a deep hpsg parser running the english resource grammar (erg). the extracted relations in the mrs (minimal recursion semantics) format are simplified and generalized using wordnet. the resulting quriples are stored in a database from where they can be retrieved (again using abstraction methods) by relation-based search. the query interface is embedded in a web browser-based application we call the scientists workbench. it supports researchers in editing and online-searching scientific papers.
314,hakala-pyysalo-2019-biomedical,"   Biomedical Named Entity Recognition with Multilingual {BERT}"",
",we present the approach of the turku nlp group to the pharmaconer task on spanish biomedical named entity recognition. we apply a crf-based baseline approach and multilingual bert to the task achieving an f-score of 88{\%} on the development data and 87{\%} on the test set with bert. our approach reflects a straightforward application of a state-of-the-art multilingual model that is not specifically tailored to either the language nor the application domain. the source code is available at: https://github.com/chaanim/pharmaconer
315,xiao-etal-2021-open,"   Open-Domain Question Answering with Pre-Constructed Question Spaces"",
",open-domain question answering aims at locating the answers to user-generated questions in massive collections of documents. retriever-readers and knowledge graph approaches are two big families of solutions to this task. a retriever-reader first applies information retrieval techniques to locate a few passages that are likely to be relevant and then feeds the retrieved text to a neural network reader to extract the answer. alternatively knowledge graphs can be constructed and queried to answer users{'} questions. we propose an algorithm with a novel reader-retriever design that differs from both families. our reader-retriever first uses an offline reader to read the corpus and generate collections of all answerable questions associated with their answers and then uses an online retriever to respond to user queries by searching the pre-constructed question spaces for answers that are most likely to be asked in the given way. we further combine one retriever-reader and two reader-retrievers into a hybrid model called r6 for the best performance. experiments with two large-scale public datasets show that r6 achieves state-of-the-art accuracy.
316,weller-di-marco-etal-2017-addressing,"   Addressing Problems across Linguistic Levels in {SMT}: Combining Approaches to Model Morphology, Syntax and Lexical Choice"",
",many errors in phrase-based smt can be attributed to problems on three linguistic levels: morphological complexity in the target language structural differences and lexical choice. we explore combinations of linguistically motivated approaches to address these problems in english-to-german smt and show that they are complementary to one another but also that the popular verbal pre-ordering can cause problems on the morphological and lexical level. a discriminative classifier can overcome these problems in particular when enriching standard lexical features with features geared towards verbal inflection.
317,akbik-etal-2019-flair,"   {FLAIR}: An Easy-to-Use Framework for State-of-the-Art {NLP}"",
",we present flair an nlp framework designed to facilitate training and distribution of state-of-the-art sequence labeling text classification and language models. the core idea of the framework is to present a simple unified interface for conceptually very different types of word and document embeddings. this effectively hides all embedding-specific engineering complexity and allows researchers to {``}mix and match{''} various embeddings with little effort. the framework also implements standard model training and hyperparameter selection routines as well as a data fetching module that can download publicly available nlp datasets and convert them into data structures for quick set up of experiments. finally flair also ships with a {``}model zoo{''} of pre-trained models to allow researchers to use state-of-the-art nlp models in their applications. this paper gives an overview of the framework and its functionality. the framework is available on github at https://github.com/zalandoresearch/flair .
318,wu-etal-2020-dtca,"   {DTCA}: Decision Tree-based Co-Attention Networks for Explainable Claim Verification"",
",recently many methods discover effective evidence from reliable sources by appropriate neural networks for explainable claim verification which has been widely recognized. however in these methods the discovery process of evidence is nontransparent and unexplained. simultaneously the discovered evidence is aimed at the interpretability of the whole sequence of claims but insufficient to focus on the false parts of claims. in this paper we propose a decision tree-based co-attention model (dtca) to discover evidence for explainable claim verification. specifically we first construct decision tree-based evidence model (dte) to select comments with high credibility as evidence in a transparent and interpretable way. then we design co-attention self-attention networks (casa) to make the selected evidence interact with claims which is for 1) training dte to determine the optimal decision thresholds and obtain more powerful evidence; and 2) utilizing the evidence to find the false parts in the claim. experiments on two public datasets rumoureval and pheme demonstrate that dtca not only provides explanations for the results of claim verification but also achieves the state-of-the-art performance boosting the f1-score by more than 3.11{\%} 2.41{\%} respectively.
319,colbath-2010-terminology,"   Terminology Management for Web Monitoring"",
",current state-of-the-art in speech recognition machine translation and natural language processing (nlp) technologies has allowed the development of powerful media monitoring systems that provide today's analysts with automatic tools for ingesting and searching through different types of data such as broadcast video web pages documents and scanned images. however the core human-language technologies (hlt) in these media monitoring systems are static learners which mean that they learn from a pool of labeled data and apply the induced knowledge to operational data in the field. to enable successful and widespread deployment and adoption of hlt these technologies need to be able to adapt effectively to new operational domains on demand. to provide the us government analyst with dynamic tools that adapt to these changing domains these hlt systems must support customizable lexicons. however the lexicon customization capability in hlt systems presents another unique challenge especially in the context of multiple users of typical media monitoring system installations in the field. lexicon customization requests from multiple users can be quite extensive and may conflict in orthographic representation (spelling transliteration or stylistic consistency) or in overall meaning. to protect against spurious and inconsistent updates to the system the media monitoring systems need to support a central terminology management capability to collect manage and execute customization requests across multiple users of the system. in this talk we will describe the integration of a user-driven lexicon/dictionary customization and terminology management capability in the context of the raytheon bbn web monitoring system (wms) to allow intelligence analysts to update the machine translation (mt) system in the wms with domain- and mission-specific source-to-english phrase translation rules. the language learning broker (llb) tool from the technology development group (tdg) is a distributed system that supports dictionary/terminology management personalized dictionaries and a workflow between linguists and linguist management. llb is integrated with the wms to provide a terminology management capability for users to submit review validate and manage customizations of the mt system through the wms user interface (ui). we will also describe an ongoing experiment to measure the effectiveness of this user-driven customization capability in terms of increased translation utility through a controlled experiment conducted with the help of intelligence analysts.
320,li-etal-2020-towards,"   Towards Medical Machine Reading Comprehension with Structural Knowledge and Plain Text"",
",machine reading comprehension (mrc) has achieved significant progress on the open domain in recent years mainly due to large-scale pre-trained language models. however it performs much worse in specific domains such as the medical field due to the lack of extensive training data and professional structural knowledge neglect. as an effort we first collect a large scale medical multi-choice question dataset (more than 21k instances) for the national licensed pharmacist examination in china. it is a challenging medical examination with a passing rate of less than 14.2{\%} in 2018. then we propose a novel reading comprehension model kmqa which can fully exploit the structural medical knowledge (i.e. medical knowledge graph) and the reference medical plain text (i.e. text snippets retrieved from reference books). the experimental results indicate that the kmqa outperforms existing competitive models with a large margin and passes the exam with 61.8{\%} accuracy rate on the test set.
321,xu-etal-2021-generalization-text,"   Generalization in Text-based Games via Hierarchical Reinforcement Learning"",
",deep reinforcement learning provides a promising approach for text-based games in studying natural language communication between humans and artificial agents. however the generalization still remains a big challenge as the agents depend critically on the complexity and variety of training tasks. in this paper we address this problem by introducing a hierarchical framework built upon the knowledge graph-based rl agent. in the high level a meta-policy is executed to decompose the whole game into a set of subtasks specified by textual goals and select one of them based on the kg. then a sub-policy in the low level is executed to conduct goal-conditioned reinforcement learning. we carry out experiments on games with various difficulty levels and show that the proposed method enjoys favorable generalizability.
322,lapalme-2019-realizing,"   Realizing {U}niversal {D}ependencies Structures"",
",we first describe a surface realizer foruniversal dependencies (ud) structures. the system uses a symbolic approach to transform the dependency tree into a tree of constituents that is transformed into an english sentence by an existing realizer. this approach was then adapted for the two shared tasks of sr{'}19. the system is quite fast and showed competitive results for english sentences using automatic and manual evaluation measures.
323,elliot-etal-2014-lexterm,"   {L}ex{T}erm Manager: Design for an Integrated Lexicography and Terminology System"",
",we present a design for a multi-modal database system for lexical information that can be accessed in either lexicographical or terminological views. the use of a single merged data model makes it easy to transfer common information between termbases and dictionaries thus facilitating information sharing and re-use. our combined model is based on the lmf and tmf metamodels for lexicographical and terminological databases and is compatible with both thus allowing for the import of information from existing dictionaries and termbases which may be transferred to the complementary view and re-exported. we also present a new linguistic configuration model analogous to a tbx xcs file which can be used to specify multiple language-specific schemata for validating and understanding lexical information in a single database. linguistic configurations are mutable and can be refined and evolved over time as understanding of documentary needs improves. the system is designed with a client-server architecture using the http protocol allowing for the independent implementation of multiple clients for specific use cases and easy deployment over the web.
324,gardent-lorenzo-2010-identifying,"   Identifying Sources of Weakness in Syntactic Lexicon Extraction"",
",previous work has shown that large scale subcategorisation lexicons could be extracted from parsed corpora with reasonably high precision. in this paper we apply a standard extraction procedure to a 100 millions words parsed corpus of french and obtain rather poor results. we investigate different factors likely to improve performance such as in particular the specific extraction procedure and the parser used; the size of the input corpus; and the type of frames learned. we try out different ways of interleaving the output of several parsers with the lexicon extraction process and show that none of them improves the results. conversely we show that increasing the size of the input corpus and modifying the extraction procedure to better differentiate prepositional arguments from prepositional modifiers improves performance. in conclusion we suggest that a more sophisticated approach to parser combination and better probabilistic models of the various types of prepositional objects in french are likely ways to get better results.
325,oluwatobi-mueller-2020-dlgnet,"   {DLGN}et: A Transformer-based Model for Dialogue Response Generation"",
",neural dialogue models despite their successes still suffer from lack of relevance diversity and in many cases coherence in their generated responses. on the other hand transformer-based models such as gpt-2 have demonstrated an excellent ability to capture long-range structures in language modeling tasks. in this paper we present dlgnet a transformer-based model for dialogue modeling. we specifically examine the use of dlgnet for multi-turn dialogue response generation. in our experiments we evaluate dlgnet on the open-domain movie triples dataset and the closed-domain ubuntu dialogue dataset. dlgnet models although trained with only the maximum likelihood objective achieve significant improvements over state-of-the-art multi-turn dialogue models. they also produce best performance to date on the two datasets based on several metrics including bleu rouge and distinct n-gram. our analysis shows that the performance improvement is mostly due to the combination of (1) the long-range transformer architecture with (2) the injection of random informative paddings. other contributing factors include the joint modeling of dialogue context and response and the 100{\%} tokenization coverage from the byte pair encoding (bpe).
326,behnke-heafield-2020-losing,"   Losing Heads in the Lottery: Pruning Transformer Attention in Neural Machine Translation"",
",the attention mechanism is the crucial component of the transformer architecture. recent research shows that most attention heads are not confident in their decisions and can be pruned. however removing them before training a model results in lower quality. in this paper we apply the lottery ticket hypothesis to prune heads in the early stages of training. our experiments on machine translation show that it is possible to remove up to three-quarters of attention heads from transformer-big during early training with an average -0.1 change in bleu for turkish→english. the pruned model is 1.5 times as fast at inference albeit at the cost of longer training. our method is complementary to other approaches such as teacher-student with english→german student model gaining an additional 10{\%} speed-up with 75{\%} encoder attention removed and 0.2 bleu loss.
327,zhong-zettlemoyer-2019-e3,"   {E}3: Entailment-driven Extracting and Editing for Conversational Machine Reading"",
",conversational machine reading systems help users answer high-level questions (e.g. determine if they qualify for particular government benefits) when they do not know the exact rules by which the determination is made (e.g. whether they need certain income levels or veteran status). the key challenge is that these rules are only provided in the form of a procedural text (e.g. guidelines from government website) which the system must read to figure out what to ask the user. we present a new conversational machine reading model that jointly extracts a set of decision rules from the procedural text while reasoning about which are entailed by the conversational history and which still need to be edited to create questions for the user. on the recently introduced sharc conversational machine reading dataset our entailment-driven extract and edit network (e3) achieves a new state-of-the-art outperforming existing systems as well as a new bert-based baseline. in addition by explicitly highlighting which information still needs to be gathered e3 provides a more explainable alternative to prior work. we release source code for our models and experiments at https://github.com/vzhong/e3.
328,wroblewska-przepiorkowski-2014-projection,"   Projection-based Annotation of a {P}olish Dependency Treebank"",
",this paper presents an approach of automatic annotation of sentences with dependency structures. the approach builds on the idea of cross-lingual dependency projection. the presented method of acquiring dependency trees involves a weighting factor in the processes of projecting source dependency relations to target sentences and inducing well-formed target dependency trees from sets of projected dependency relations. using a parallel corpus source trees are transferred onto equivalent target sentences via an extended set of alignment links. projected arcs are initially weighted according to the certainty of word alignment links. then arc weights are recalculated using a method based on the em selection algorithm. maximum spanning trees selected from em-scored digraphs and labelled with appropriate grammatical functions constitute a target dependency treebank. extrinsic evaluation shows that parsers trained on such a treebank may perform comparably to parsers trained on a manually developed treebank.
329,van-der-wees-etal-2016-simple,"   A Simple but Effective Approach to Improve {A}rabizi-to-{E}nglish Statistical Machine Translation"",
",a major challenge for statistical machine translation (smt) of arabic-to-english user-generated text is the prevalence of text written in arabizi or romanized arabic. when facing such texts a translation system trained on conventional arabic-english data will suffer from extremely low model coverage. in addition arabizi is not regulated by any official standardization and therefore highly ambiguous which prevents rule-based approaches from achieving good translation results. in this paper we improve arabizi-to-english machine translation by presenting a simple but effective arabizi-to-arabic transliteration pipeline that does not require knowledge by experts or native arabic speakers. we incorporate this pipeline into a phrase-based smt system and show that translation quality after automatically transliterating arabizi to arabic yields results that are comparable to those achieved after human transliteration.
330,marlet-2008-un,"   Un sens logique pour les graphes s{\'e}mantiques"",
",nous discutons du sens des graphes s{\'e}mantiques notamment de ceux utilis{\'e}s en th{\'e}orie sens-texte. nous leur donnons un sens pr{\'e}cis {\'e}ventuellement sous-sp{\'e}cifi{\'e} gr{\^a}ce {\`a} une traduction simple vers une formule de minimal recursion semantics qui couvre les cas de pr{\'e}dications multiples sur plusieurs entit{\'e}s de pr{\'e}dication d{'}ordre sup{\'e}rieur et de modalit{\'e}s.
331,kawanami-etal-2006-long,"   Long-term Analysis of Prosodic Features of Spoken Guidance System User Speech"",
",as a practical information guidance system we have been developing a speech-oriented system named ``takemaru-kun''. the system has been operated on a public space since nov. 2002. the system answers to user's question about the hall facilities sightseeing transportation weather information around the city etc. all triggered inputs to the system have been recorded since the operation started. and all system inputs during 22 months are manually transcribed and labelled for speakers gender and age category. in this paper we conduct a long-term prosody analysis of user speech to find a clue to obtain users attitude from a users speech. in this preliminary analysis it is observed that f0 decreases regardless of age and gender category when the stability of the dialogue system is not established.
332,ye-etal-2020-benchmarking,"   Benchmarking Multimodal Regex Synthesis with Complex Structures"",
",existing datasets for regular expression (regex) generation from natural language are limited in complexity; compared to regex tasks that users post on stackoverflow the regexes in these datasets are simple and the language used to describe them is not diverse. we introduce structuredregex a new regex synthesis dataset differing from prior ones in three aspects. first to obtain structurally complex and realistic regexes we generate the regexes using a probabilistic grammar with pre-defined macros observed from real-world stackoverflow posts. second to obtain linguistically diverse natural language descriptions we show crowdworkers abstract depictions of the underlying regex and ask them to describe the pattern they see rather than having them paraphrase synthetic language. third we augment each regex example with a collection of strings that are and are not matched by the ground truth regex similar to how real users give examples. our quantitative and qualitative analysis demonstrates the advantages of structuredregex over prior datasets. further experimental results using various multimodal synthesis techniques highlight the challenge presented by our dataset including non-local constraints and multi-modal inputs.
333,tomlinson-etal-2014-mygoal,"   {\#}mygoal: Finding Motivations on {T}witter"",
",our everyday language reflects our psychological and cognitive state and effects the states of other individuals. in this contribution we look at the intersection between motivational state and language. we create a set of hashtags which are annotated for the degree to which they are used by individuals to mark-up language that is indicative of a collection of factors that interact with an individual{'}s motivational state. we look for tags that reflect a goal mention reward or a perception of control. finally we present results for a language-model based classifier which is able to predict the presence of one of these factors in a tweet with between 69{\textbackslash}{\%} and 80{\textbackslash}{\%} accuracy on a balanced testing set. our approach suggests that hashtags can be used to understand not just the language of topics but the deeper psychological and social meaning of a tweet.
334,lim-poibeau-2017-system,"   A System for Multilingual Dependency Parsing based on Bidirectional {LSTM} Feature Representations"",
",in this paper we present our multilingual dependency parser developed for the conll 2017 ud shared task dealing with {``}multilingual parsing from raw text to universal dependencies{''}. our parser extends the monolingual bist-parser as a multi-source multilingual trainable parser. thanks to multilingual word embeddings and one hot encodings for languages our system can use both monolingual and multi-source training. we trained 69 monolingual language models and 13 multilingual models for the shared task. our multilingual approach making use of different resources yield better results than the monolingual approach for 11 languages. our system ranked 5 th and achieved 70.93 overall las score over the 81 test corpora (macro-averaged las f1 score).
335,asaadi-rudolph-2017-gradual,"   Gradual Learning of Matrix-Space Models of Language for Sentiment Analysis"",
",learning word representations to capture the semantics and compositionality of language has received much research interest in natural language processing. beyond the popular vector space models matrix representations for words have been proposed since then matrix multiplication can serve as natural composition operation. in this work we investigate the problem of learning matrix representations of words. we present a learning approach for compositional matrix-space models for the task of sentiment analysis. we show that our approach which learns the matrices gradually in two steps outperforms other approaches and a gradient-descent baseline in terms of quality and computational cost.
336,kruijff-korbayova-etal-2006-sammie,"   The {SAMMIE} Corpus of Multimodal Dialogues with an {MP}3 Player"",
",we describe a corpus of multimodal dialogues with an mp3player collected in wizard-of-oz experiments and annotated with a richfeature set at several layers. we are using the nite xml toolkit (nxt) to represent and further process the data. we designed an nxtdata model converted experiment log file data and manualtranscriptions into nxt and are building tools for additionalannotation using nxt libraries. the annotated corpus will be used to (i) investigate various aspects of multimodal presentation andinteraction strategies both within and across annotation layers; (ii) design an initial policy for reinforcement learning of multimodalclarification requests.
337,mao-etal-2018-word,"   Word Embedding and {W}ord{N}et Based Metaphor Identification and Interpretation"",
",metaphoric expressions are widespread in natural language posing a significant challenge for various natural language processing tasks such as machine translation. current word embedding based metaphor identification models cannot identify the exact metaphorical words within a sentence. in this paper we propose an unsupervised learning method that identifies and interprets metaphors at word-level without any preprocessing outperforming strong baselines in the metaphor identification task. our model extends to interpret the identified metaphors paraphrasing them into their literal counterparts so that they can be better translated by machines. we evaluated this with two popular translation systems for english to chinese showing that our model improved the systems significantly.
338,laskar-etal-2020-contextualized,"   Contextualized Embeddings based Transformer Encoder for Sentence Similarity Modeling in Answer Selection Task"",
",word embeddings that consider context have attracted great attention for various natural language processing tasks in recent years. in this paper we utilize contextualized word embeddings with the transformer encoder for sentence similarity modeling in the answer selection task. we present two different approaches (feature-based and fine-tuning-based) for answer selection. in the feature-based approach we utilize two types of contextualized embeddings namely the embeddings from language models (elmo) and the bidirectional encoder representations from transformers (bert) and integrate each of them with the transformer encoder. we find that integrating these contextual embeddings with the transformer encoder is effective to improve the performance of sentence similarity modeling. in the second approach we fine-tune two pre-trained transformer encoder models for the answer selection task. based on our experiments on six datasets we find that the fine-tuning approach outperforms the feature-based approach on all of them. among our fine-tuning-based models the robustly optimized bert pretraining approach (roberta) model results in new state-of-the-art performance across five datasets.
339,jurczyk-choi-2017-cross,"   Cross-genre Document Retrieval: Matching between Conversational and Formal Writings"",
",this paper challenges a cross-genre document retrieval task where the queries are in formal writing and the target documents are in conversational writing. in this task a query is a sentence extracted from either a summary or a plot of an episode in a tv show and the target document consists of transcripts from the corresponding episode. to establish a strong baseline we employ the current state-of-the-art search engine to perform document retrieval on the dataset collected for this work. we then introduce a structure reranking approach to improve the initial ranking by utilizing syntactic and semantic structures generated by nlp tools. our evaluation shows an improvement of more than 4{\%} when the structure reranking is applied which is very promising.
340,hu-etal-2021-bidirectional-hierarchical,"   Bidirectional Hierarchical Attention Networks based on Document-level Context for Emotion Cause Extraction"",
",emotion cause extraction (ece) aims to extract the causes behind the certain emotion in text. some works related to the ece task have been published and attracted lots of attention in recent years. however these methods neglect two major issues: 1) pay few attentions to the effect of document-level context information on ece and 2) lack of sufficient exploration for how to effectively use the annotated emotion clause. for the first issue we propose a bidirectional hierarchical attention network (bha) corresponding to the specified candidate cause clause to capture the document-level context in a structured and dynamic manner. for the second issue we design an emotional filtering module (ef) for each layer of the graph attention network which calculates a gate score based on the emotion clause to filter the irrelevant information. combining the bha and ef the ef-bha can dynamically aggregate the contextual information from two directions and filters irrelevant information. the experimental results demonstrate that ef-bha achieves the competitive performances on two public datasets in different languages (chinese and english). moreover we quantify the effect of context on emotion cause extraction and provide the visualization of the interactions between candidate cause clauses and contexts.
341,naik-etal-2017-extracting,"   Extracting Personal Medical Events for User Timeline Construction using Minimal Supervision"",
",in this paper we describe a system for automatic construction of user disease progression timelines from their posts in online support groups using minimal supervision. in recent years several online support groups have been established which has led to a huge increase in the amount of patient-authored text available. creating systems which can automatically extract important medical events and create disease progression timelines for users from such text can help in patient health monitoring as well as studying links between medical events and users{'} participation in support groups. prior work in this domain has used manually constructed keyword sets to detect medical events. in this work our aim is to perform medical event detection using minimal supervision in order to develop a more general timeline construction system. our system achieves an accuracy of 55.17{\%} which is 92{\%} of the performance achieved by a supervised baseline system.
342,haque-etal-2010-supertags,"   Supertags as Source Language Context in Hierarchical Phrase-Based {SMT}"",
",statistical machine translation (smt) models have recently begun to include source context modeling under the assumption that the proper lexical choice of the translation for an ambiguous word can be determined from the context in which it appears. various types of lexical and syntactic features have been explored as effective source context to improve phrase selection in smt. in the present work we introduce lexico-syntactic descriptions in the form of supertags as source-side context features in the state-of-the-art hierarchical phrase-based smt (hpb) model. these features enable us to exploit source similarity in addition to target similarity as modelled by the language model. in our experiments two kinds of supertags are employed: those from lexicalized tree-adjoining grammar (ltag) and combinatory categorial grammar (ccg). we use a memory-based classification framework that enables the efficient estimation of these features. despite the differences between the two supertagging approaches they give similar improvements. we evaluate the performance of our approach on an english-to-dutch translation task and report statistically significant improvements of 4.48{\%} and 6.3{\%} bleu scores in translation quality when adding ccg and ltag supertags respectively as context-informed features.
343,zhang-vo-2016-neural,"   Neural Networks for Sentiment Analysis"",
",sentiment analysis has been a major research topic in natural language processing (nlp). traditionally the problem has been attacked using discrete models and manually-defined sparse features. over the past few years neural network models have received increased research efforts in most sub areas of sentiment analysis giving highly promising results. a main reason is the capability of neural models to automatically learn dense features that capture subtle semantic information over words sentences and documents which are difficult to model using traditional discrete features based on words and ngram patterns. this tutorial gives an introduction to neural network models for sentiment analysis discussing the mathematics of word embeddings sequence models and tree structured models and their use in sentiment analysis on the word sentence and document levels and fine-grained sentiment analysis. the tutorial covers a range of neural network models (e.g. cnn rnn recnn lstm) and their extensions which are employed in four main subtasks of sentiment analysis:sentiment-oriented embeddings;sentence-level sentiment;document-level sentiment;fine-grained sentiment.the content of the tutorial is divided into 3 sections of 1 hour each. we assume that the audience is familiar with linear algebra and basic neural network structures introduce the mathematical details of the most typical models. first we will introduce the sentiment analysis task basic concepts related to neural network models for sentiment analysis and show detail approaches to integrate sentiment information into embeddings. sentence-level models will be described in the second section. finally we will discuss neural network models use for document-level and fine-grained sentiment.
344,chen-etal-2020-improving,"   Improving Commonsense Question Answering by Graph-based Iterative Retrieval over Multiple Knowledge Sources"",
",in order to facilitate natural language understanding the key is to engage commonsense or background knowledge. however how to engage commonsense effectively in question answering systems is still under exploration in both research academia and industry. in this paper we propose a novel question-answering method by integrating multiple knowledge sources i.e. conceptnet wikipedia and the cambridge dictionary to boost the performance. more concretely we first introduce a novel graph-based iterative knowledge retrieval module which iteratively retrieves concepts and entities related to the given question and its choices from multiple knowledge sources. afterward we use a pre-trained language model to encode the question retrieved knowledge and choices and propose an answer choice-aware attention mechanism to fuse all hidden representations of the previous modules. finally the linear classifier for specific tasks is used to predict the answer. experimental results on the commonsenseqa dataset show that our method significantly outperforms other competitive methods and achieves the new state-of-the-art. in addition further ablation studies demonstrate the effectiveness of our graph-based iterative knowledge retrieval module and the answer choice-aware attention module in retrieving and synthesizing background knowledge from multiple knowledge sources.
345,khan-etal-2020-coding,"   Coding Textual Inputs Boosts the Accuracy of Neural Networks"",
",natural language processing (nlp) tasks are usually performed word by word on textual inputs. we can use arbitrary symbols to represent the linguistic meaning of a word and use these symbols as inputs. as {``}alternatives{''} to a text representation we introduce soundex metaphone nysiis logogram to nlp and develop fixed-output-length coding and its extension using huffman coding. each of those codings combines different character/digital sequences and constructs a new vocabulary based on codewords. we find that the integration of those codewords with text provides more reliable inputs to neural-network-based nlp systems through redundancy than text-alone inputs. experiments demonstrate that our approach outperforms the state-of-the-art models on the application of machine translation language modeling and part-of-speech tagging. the source code is available at https://github.com/abdulrafae/coding{\_}nmt.
346,effrosynidis-etal-2018-duth,"   {DUTH} at {S}em{E}val-2018 Task 2: Emoji Prediction in Tweets"",
",this paper describes the approach that was developed for semeval 2018 task 2 (multilingual emoji prediction) by the duth team. first we employed a combination of pre-processing techniques to reduce the noise of tweets and produce a number of features. then we built several n-grams to represent the combination of word and emojis. finally we trained our system with a tuned linearsvc classifier. our approach in the leaderboard ranked 18th amongst 48 teams.
347,wu-etal-2021-training,"   Training Adaptive Computation for Open-Domain Question Answering with Computational Constraints"",
",adaptive computation (ac) has been shown to be effective in improving the efficiency of open-domain question answering (odqa) systems. however the current ac approaches require tuning of all model parameters and training state-of-the-art odqa models requires significant computational resources that may not be available for most researchers. we propose adaptive passage encoder an ac method that can be applied to an existing odqa model and can be trained efficiently on a single gpu. it keeps the parameters of the base odqa model fixed but it overrides the default layer-by-layer computation of the encoder with an ac policy that is trained to optimise the computational efficiency of the model. our experimental results show that our method improves upon a state-of-the-art model on two datasets and is also more accurate than previous ac methods due to the stronger base odqa model. all source code and datasets are available at https://github.com/uclnlp/ape.
348,kerrigan-etal-2020-differentially,"   Differentially Private Language Models Benefit from Public Pre-training"",
",language modeling is a keystone task in natural language processing. when training a language model on sensitive information differential privacy (dp) allows us to quantify the degree to which our private data is protected. however training algorithms which enforce differential privacy often lead to degradation in model quality. we study the feasibility of learning a language model which is simultaneously high-quality and privacy preserving by tuning a public base model on a private corpus. we find that dp fine-tuning boosts the performance of language models in the private domain making the training of such models possible.
349,loukachevitch-dobrov-2004-development,"   Development of Bilingual Domain-Specific Ontology for Automatic Conceptual Indexing"",
",in the paper we describe development means of evaluation and applications of russian-english sociopolitical thesaurus specially developed as a linguistic resource for automatic text processing applications. the sociopolitical domain is not a domain of social research but a broad domain of social relations including economic political military cultural sports and other subdomains. the knowledge of this domain is necessary for automatic text processing of such important documents as official documents legislative acts newspaper articles.
350,wang-etal-2020-learning-efficient,"   Learning Efficient Dialogue Policy from Demonstrations through Shaping"",
",training a task-oriented dialogue agent with reinforcement learning is prohibitively expensive since it requires a large volume of interactions with users. human demonstrations can be used to accelerate learning progress. however how to effectively leverage demonstrations to learn dialogue policy remains less explored. in this paper we present s{\^{}}2agent that efficiently learns dialogue policy from demonstrations through policy shaping and reward shaping. we use an imitation model to distill knowledge from demonstrations based on which policy shaping estimates feedback on how the agent should act in policy space. reward shaping is then incorporated to bonus state-actions similar to demonstrations explicitly in value space encouraging better exploration. the effectiveness of the proposed s{\^{}}2agentt is demonstrated in three dialogue domains and a challenging domain adaptation task with both user simulator evaluation and human evaluation.
351,arnulphy-etal-2012-event,"   Event Nominals: Annotation Guidelines and a Manually Annotated Corpus in {F}rench"",
",within the general purpose of information extraction detection of event descriptions is an important clue. a word refering to an event is more powerful than a single word because it implies a location a time protagonists (persons organizations{\textbackslash}dots). however if verbal designations of events are well studied and easier to detect than nominal ones nominal designations do not claim as much definition effort and resources. in this work we focus on nominals desribing events. as our application domain is information extraction we follow a named entity approach to describe and annotate events. in this paper we present a typology and annotation guidelines for event nominals annotation. we applied them to french newswire articles and produced an annotated corpus. we present observations about the designations used in our manually annotated corpus and the behavior of their triggers. we provide statistics concerning word ambiguity and context of use of event nominals as well as machine learning experiments showing the difficulty of using lexicons for extracting events.
352,pannitto-etal-2021-teaching,"   Conversation Graph: Data Augmentation, Training, and Evaluation for Non-Deterministic Dialogue Management"",
",task-oriented dialogue systems typically rely on large amounts of high-quality training data or require complex handcrafted rules. however existing datasets are often limited in size con- sidering the complexity of the dialogues. additionally conventional training signal in- ference is not suitable for non-deterministic agent behavior namely considering multiple actions as valid in identical dialogue states. we propose the conversation graph (convgraph) a graph-based representation of dialogues that can be exploited for data augmentation multi- reference training and evaluation of non- deterministic agents. convgraph generates novel dialogue paths to augment data volume and diversity. intrinsic and extrinsic evaluation across three datasets shows that data augmentation and/or multi-reference training with convgraph can improve dialogue success rates by up to 6.4{\%}.
353,wehrli-etal-2016-line,"   On-line Multilingual Linguistic Services"",
",in this demo we present our free on-line multilingual linguistic services which allow to analyze sentences or to extract collocations from a corpus directly on-line or by uploading a corpus. they are available for 8 european languages (english french german greek italian portuguese romanian spanish) and can also be accessed as web services by programs. while several open systems are available for pos-tagging and dependency parsing or terminology extraction their integration into an application requires some computational competence. furthermore none of the parsers/taggers handles mwes very satisfactorily in particular when the two terms of the collocation are distant from each other or in reverse order. our tools on the other hand are specifically designed for users with no particular computational literacy. they do not require from the user any download installation or adaptation if used on-line and their integration in an application using one the scripts described below is quite easy. furthermore by default the parser handles collocations and other mwes as well as anaphora resolution (limited to 3rd person personal pronouns). when used in the tagger mode it can be set to display grammatical functions and collocations.
354,wang-etal-2020-connecting,"   Connecting the Dots: A Knowledgeable Path Generator for Commonsense Question Answering"",
",commonsense question answering (qa) requires background knowledge which is not explicitly stated in a given context. prior works use commonsense knowledge graphs (kgs) to obtain this knowledge for reasoning. however relying entirely on these kgs may not suffice considering their limited coverage and the contextual dependence of their knowledge. in this paper we augment a general commonsense qa framework with a knowledgeable path generator. by extrapolating over existing paths in a kg with a state-of-the-art language model our generator learns to connect a pair of entities in text with a dynamic and potentially novel multi-hop relational path. such paths can provide structured evidence for solving commonsense questions without fine-tuning the path generator. experiments on two datasets show the superiority of our method over previous works which fully rely on knowledge from kgs (with up to 6{\%} improvement in accuracy) across various amounts of training data. further evaluation suggests that the generated paths are typically interpretable novel and relevant to the task.
355,offenga-etal-2006-metadata,"   Metadata Profile in the {ISO} Data Category Registry"",
",metadata descriptions of language resources become an increasing necessity since the shear amount of language resources is increasing rapidly and especially since we are now creating infrastuctures to access these resources via the web through integrated domains of language resource archives. yet the metadata frameworks offered for the domain of language resources (imdi and olac) although mature are not as widely accepted as necessary. the lack of confidence in the stability and persistence of the concepts and formats introduced by these metadata sets seems to be one argument for people to not invest the time needed for metadata creation. the introduction of these concepts into an iso standardization process may convince contributors to make use of the terminology. the availability of the iso data category registry that includes a metadata profile will also offer the opportunity for researchers to construct their own metadata set tailored to the needs of the project at hand but nevertheless supporting interoperability.
356,agarwal-etal-2018-char2char,"   Char2char Generation with Reranking for the {E}2{E} {NLG} Challenge"",
",this paper describes our submission to the e2e nlg challenge. recently neural seq2seq approaches have become mainstream in nlg often resorting to pre- (respectively post-) processing \textit{delexicalization} (relexicalization) steps at the word-level to handle rare words. by contrast we train a simple character level seq2seq model which requires no pre/post-processing (delexicalization tokenization or even lowercasing) with surprisingly good results. for further improvement we explore two re-ranking approaches for scoring candidates. we also introduce a synthetic dataset creation procedure which opens up a new way of creating artificial datasets for natural language generation.
357,ekstedt-skantze-2021-projection,"   Projection of Turn Completion in Incremental Spoken Dialogue Systems"",
",the ability to take turns in a fluent way (i.e. without long response delays or frequent interruptions) is a fundamental aspect of any spoken dialog system. however practical speech recognition services typically induce a long response delay as it takes time before the processing of the user{'}s utterance is complete. there is a considerable amount of research indicating that humans achieve fast response times by projecting what the interlocutor will say and estimating upcoming turn completions. in this work we implement this mechanism in an incremental spoken dialog system by using a language model that generates possible futures to project upcoming completion points. in theory this could make the system more responsive while still having access to semantic information not yet processed by the speech recognizer. we conduct a small study which indicates that this is a viable approach for practical dialog systems and that this is a promising direction for future research.
358,kasai-etal-2021-finetuning,"   Finetuning Pretrained Transformers into {RNN}s"",
",transformers have outperformed recurrent neural networks (rnns) in natural language generation. but this comes with a signifi- cant computational cost as the attention mechanism{'}s complexity scales quadratically with sequence length. efficient transformer variants have received increasing interest in recent works. among them a linear-complexity recurrent variant has proven well suited for autoregressive generation. it approximates the softmax attention with randomized or heuristic feature maps but can be difficult to train and may yield suboptimal accuracy. this work aims to convert a pretrained transformer into its efficient recurrent counterpart improving efficiency while maintaining accuracy. specifically we propose a swap-then-finetune procedure: in an off-the-shelf pretrained transformer we replace the softmax attention with its linear-complexity recurrent alternative and then finetune. with a learned feature map our approach provides an improved tradeoff between efficiency and accuracy over the standard transformer and other recurrent variants. we also show that the finetuning process has lower training cost relative to training these recurrent variants from scratch. as many models for natural language tasks are increasingly dependent on large-scale pretrained transformers this work presents a viable approach to improving inference efficiency without repeating the expensive pretraining process.
359,jimenez-zafra-etal-2016-problematic,"   Problematic Cases in the Annotation of Negation in {S}panish"",
",this paper presents the main sources of disagreement found during the annotation of the spanish sfu review corpus with negation (sfu reviewsp -neg). negation detection is a challenge in most of the task related to nlp so the availability of corpora annotated with this phenomenon is essential in order to advance in tasks related to this area. a thorough analysis of the problems found during the annotation could help in the study of this phenomenon.
360,wu-etal-2021-novel,"   Novel Slot Detection: A Benchmark for Discovering Unknown Slot Types in the Task-Oriented Dialogue System"",
",existing slot filling models can only recognize pre-defined in-domain slot types from a limited slot set. in the practical application a reliable dialogue system should know what it does not know. in this paper we introduce a new task novel slot detection (nsd) in the task-oriented dialogue system. nsd aims to discover unknown or out-of-domain slot types to strengthen the capability of a dialogue system based on in-domain training data. besides we construct two public nsd datasets propose several strong nsd baselines and establish a benchmark for future work. finally we conduct exhaustive experiments and qualitative analysis to comprehend key challenges and provide new guidance for future directions.
361,strapparava-etal-2010-predicting,"   Predicting Persuasiveness in Political Discourses"",
",in political speeches the audience tends to react or resonate to signals of persuasive communication including an expected theme a name or an expression. automatically predicting the impact of such discourses is a challenging task. in fact nowadays with the huge amount of textual material that flows on the web (news discourses blogs etc.) it can be useful to have a measure for testing the persuasiveness of what we retrieve or possibly of what we want to publish on web. in this paper we exploit a corpus of political discourses collected from various web sources tagged with audience reactions such as applause as indicators of persuasive expressions. in particular we use this data set in a machine learning framework to explore the possibility of classifying the transcript of political discourses according to their persuasive power predicting the sentences that possibly trigger applause. we also explore differences between democratic and republican speeches experiment the resulting classifiers in grading some of the discourses in the obama-mccain presidential campaign available on the web.
362,raoul-2009-sagace,"   {SAGACE}-v3.3 ; Analyseur de corpus pour langues non flexionnelles"",
",nous pr{\'e}sentons la derni{\`e}re version du logiciel sagace analyseur de corpus pour langues faiblement flexionnelles (par exemple japonais ou chinois). ce logiciel est distribu{\'e} avec un lexique o{\`u} les cat{\'e}gories sont exprim{\'e}es {\`a} l{'}aide de syst{\`e}mes de traits.
363,almarwani-diab-2017-arabic,"   {A}rabic Textual Entailment with Word Embeddings"",
",determining the textual entailment between texts is important in many nlp tasks such as summarization question answering and information extraction and retrieval. various methods have been suggested based on external knowledge sources; however such resources are not always available in all languages and their acquisition is typically laborious and very costly. distributional word representations such as word embeddings learned over large corpora have been shown to capture syntactic and semantic word relationships. such models have contributed to improving the performance of several nlp tasks. in this paper we address the problem of textual entailment in arabic. we employ both traditional features and distributional representations. crucially we do not depend on any external resources in the process. our suggested approach yields state of the art performance on a standard data set arbte achieving an accuracy of 76.2 {\%} compared to state of the art of 69.3 {\%}.
364,chen-etal-2017-cost,"   Cost Weighting for Neural Machine Translation Domain Adaptation"",
",in this paper we propose a new domain adaptation technique for neural machine translation called cost weighting which is appropriate for adaptation scenarios in which a small in-domain data set and a large general-domain data set are available. cost weighting incorporates a domain classifier into the neural machine translation training algorithm using features derived from the encoder representation in order to distinguish in-domain from out-of-domain data. classifier probabilities are used to weight sentences according to their domain similarity when updating the parameters of the neural translation model. we compare cost weighting to two traditional domain adaptation techniques developed for statistical machine translation: data selection and sub-corpus weighting. experiments on two large-data tasks show that both the traditional techniques and our novel proposal lead to significant gains with cost weighting outperforming the traditional methods.
365,otterbacher-radev-2004-revisionbank,"   {R}evision{B}ank: A Resource for Revision-based Multi-document Summarization and Evaluation"",
",multi-document summaries produced via sentence extraction often suffer from a number of cohesion problems including dangling anaphora sudden shifts in topic and incorrect or awkward chronological ordering. therefore the development of an automated revision process to correct such problems is a research area of current interest. we present the revisionbank a corpus of 240 extractive multi-document summaries that have been manually revised to promote cohesion. the summaries were revised by six linguistic students using a constrained set of revision operations that we previously developed. in the current paper we describe the process of developing a taxonomy of cohesion problems and corrective revision operators that address such problems as well as an annotation schema for our corpus. finally we discuss how our taxonomy and corpus can be used for the study of revision-based multi-document summarization as well as for summary evaluation.
366,liu-etal-2020-multilingual,"   Multilingual Graphemic Hybrid {ASR} with Massive Data Augmentation"",
",towards developing high-performing asr for low-resource languages approaches to address the lack of resources are to make use of data from multiple languages and to augment the training data by creating acoustic variations. in this work we present a single grapheme-based asr model learned on 7 geographically proximal languages using standard hybrid blstm-hmm acoustic models with lattice-free mmi objective. we build the single asr grapheme set via taking the union over each language-specific grapheme set and we find such multilingual graphemic hybrid asr model can perform language-independent recognition on all 7 languages and substantially outperform each monolingual asr model. secondly we evaluate the efficacy of multiple data augmentation alternatives within language as well as their complementarity with multilingual modeling. overall we show that the proposed multilingual graphemic hybrid asr with various data augmentation can not only recognize any within training set languages but also provide large asr performance improvements.
367,chen-etal-2020-towards,"   Towards Interpretable Clinical Diagnosis with {B}ayesian Network Ensembles Stacked on Entity-Aware {CNN}s"",
",the automatic text-based diagnosis remains a challenging task for clinical use because it requires appropriate balance between accuracy and interpretability. in this paper we attempt to propose a solution by introducing a novel framework that stacks bayesian network ensembles on top of entity-aware convolutional neural networks (cnn) towards building an accurate yet interpretable diagnosis system. the proposed framework takes advantage of the high accuracy and generality of deep neural networks as well as the interpretability of bayesian networks which is critical for ai-empowered healthcare. the evaluation conducted on the real electronic medical record (emr) documents from hospitals and annotated by professional doctors proves that the proposed framework outperforms the previous automatic diagnosis methods in accuracy performance and the diagnosis explanation of the framework is reasonable.
368,yang-etal-2021-syntactically,"   Syntactically-Informed Unsupervised Paraphrasing with Non-Parallel Data"",
",previous works on syntactically controlled paraphrase generation heavily rely on large-scale parallel paraphrase data that is not easily available for many languages and domains. in this paper we take this research direction to the extreme and investigate whether it is possible to learn syntactically controlled paraphrase generation with nonparallel data. we propose a syntactically-informed unsupervised paraphrasing model based on conditional variational auto-encoder (vae) which can generate texts in a specified syntactic structure. particularly we design a two-stage learning method to effectively train the model using non-parallel data. the conditional vae is trained to reconstruct the input sentence according to the given input and its syntactic structure. furthermore to improve the syntactic controllability and semantic consistency of the pre-trained conditional vae we fine-tune it using syntax controlling and cycle reconstruction learning objectives and employ gumbel-softmax to combine these new learning objectives. experiment results demonstrate that the proposed model trained only on non-parallel data is capable of generating diverse paraphrases with specified syntactic structure. additionally we validate the effectiveness of our method for generating syntactically adversarial examples on the sentiment analysis task.
369,mitrofan-etal-2019-monero,"   {M}o{NER}o: a Biomedical Gold Standard Corpus for the {R}omanian Language"",
",in an era when large amounts of data are generated daily in various fields the biomedical field among others linguistic resources can be exploited for various tasks of natural language processing. moreover increasing number of biomedical documents are available in languages other than english. to be able to extract information from natural language free text resources methods and tools are needed for a variety of languages. this paper presents the creation of the monero corpus a gold standard biomedical corpus for romanian annotated with both part of speech tags and named entities. monero comprises 154825 morphologically annotated tokens and 23188 entity annotations belonging to four entity semantic groups corresponding to umls semantic groups.
370,ezeani-etal-2018-igbo,"   {I}gbo Diacritic Restoration using Embedding Models"",
",igbo is a low-resource language spoken by approximately 30 million people worldwide. it is the native language of the igbo people of south-eastern nigeria. in igbo language diacritics - orthographic and tonal - play a huge role in the distinguishing the meaning and pronunciation of words. omitting diacritics in texts often leads to lexical ambiguity. diacritic restoration is a pre-processing task that replaces missing diacritics on words from which they have been removed. in this work we applied embedding models to the diacritic restoration task and compared their performances to those of n-gram models. although word embedding models have been successfully applied to various nlp tasks it has not been used to our knowledge for diacritic restoration. two classes of word embeddings models were used: those projected from the english embedding space; and those trained with igbo bible corpus ({\mbox{$\approx$}} 1m). our best result 82.49{\%} is an improvement on the baseline n-gram models.
371,ciobanu-etal-2018-german,"   {G}erman Dialect Identification Using Classifier Ensembles"",
",in this paper we present the gdi classification entry to the second german dialect identification (gdi) shared task organized within the scope of the vardial evaluation campaign 2018. we present a system based on svm classifier ensembles trained on characters and words. the system was trained on a collection of speech transcripts of five swiss-german dialects provided by the organizers. the transcripts included in the dataset contained speakers from basel bern lucerne and zurich. our entry in the challenge reached 62.03{\%} f1 score and was ranked third out of eight teams.
372,pon-barry-etal-2014-eliciting,"   Eliciting and Annotating Uncertainty in Spoken Language"",
",a major challenge in the field of automatic recognition of emotion and affect in speech is the subjective nature of affect labels. the most common approach to acquiring affect labels is to ask a panel of listeners to rate a corpus of spoken utterances along one or more dimensions of interest. for applications ranging from educational technology to voice search to dictation a speaker{'}s level of certainty is a primary dimension of interest. in such applications we would like to know the speaker{'}s actual level of certainty but past research has only revealed listeners{'} perception of the speaker{'}s level of certainty. in this paper we present a method for eliciting spoken utterances using stimuli that we design such that they have a quantitative crowdsourced legibility score. while we cannot control a speaker{'}s actual internal level of certainty the use of these stimuli provides a better estimate of internal certainty compared to existing speech corpora. the harvard uncertainty speech corpus containing speech data certainty annotations and prosodic features is made available to the research community.
373,gonen-goldberg-2016-semi,"   Semi Supervised Preposition-Sense Disambiguation using Multilingual Data"",
",prepositions are very common and very ambiguous and understanding their sense is critical for understanding the meaning of the sentence. supervised corpora for the preposition-sense disambiguation task are small suggesting a semi-supervised approach to the task. we show that signals from unannotated multilingual data can be used to improve supervised preposition-sense disambiguation. our approach pre-trains an lstm encoder for predicting the translation of a preposition and then incorporates the pre-trained encoder as a component in a supervised classification system and fine-tunes it for the task. the multilingual signals consistently improve results on two preposition-sense datasets.
374,prevot-etal-2016-cup,"   A {CUP} of {C}o{F}ee: A large Collection of feedback Utterances Provided with communicative function annotations"",
",there have been several attempts to annotate communicative functions to utterances of verbal feedback in english previously. here we suggest an annotation scheme for verbal and non-verbal feedback utterances in french including the categories base attitude previous and visual. the data comprises conversations maptasks and negotiations from which we extracted ca. 13000 candidate feedback utterances and gestures. 12 students were recruited for the annotation campaign of ca. 9500 instances. each instance was annotated by between 2 and 7 raters. the evaluation of the annotation agreement resulted in an average best-pair kappa of 0.6. while the base category with the values acknowledgement evaluation answer elicit achieve good agreement this is not the case for the other main categories. the data sets which also include automatic extractions of lexical positional and acoustic features are freely available and will further be used for machine learning classification experiments to analyse the form-function relationship of feedback.
375,zhou-etal-2021-amr,"   {AMR} Parsing with Action-Pointer Transformer"",
",abstract meaning representation parsing is a sentence-to-graph prediction task where target nodes are not explicitly aligned to sentence tokens. however since graph nodes are semantically based on one or more sentence tokens implicit alignments can be derived. transition-based parsers operate over the sentence from left to right capturing this inductive bias via alignments at the cost of limited expressiveness. in this work we propose a transition-based system that combines hard-attention over sentences with a target-side action pointer mechanism to decouple source tokens from node representations and address alignments. we model the transitions as well as the pointer mechanism through straightforward modifications within a single transformer architecture. parser state and graph structure information are efficiently encoded using attention heads. we show that our action-pointer approach leads to increased expressiveness and attains large gains (+1.6 points) against the best transition-based amr parser in very similar conditions. while using no graph re-categorization our single model yields the second best smatch score on amr 2.0 (81.8) which is further improved to 83.4 with silver data and ensemble decoding.
376,savenkov-agichtein-2017-evinets,"   {E}vi{N}ets: Neural Networks for Combining Evidence Signals for Factoid Question Answering"",
",a critical task for question answering is the final answer selection stage which has to combine multiple signals available about each answer candidate. this paper proposes evinets: a novel neural network architecture for factoid question answering. evinets scores candidate answer entities by combining the available supporting evidence e.g. structured knowledge bases and unstructured text documents. evinets represents each piece of evidence with a dense embeddings vector scores their relevance to the question and aggregates the support for each candidate to predict their final scores. each of the components is generic and allows plugging in a variety of models for semantic similarity scoring and information aggregation. we demonstrate the effectiveness of evinets in experiments on the existing trec qa and wikimovies benchmarks and on the new yahoo! answers dataset introduced in this paper. evinets can be extended to other information types and could facilitate future work on combining evidence signals for joint reasoning in question answering.
377,ning-etal-2019-team,"   Team Peter-Parker at {S}em{E}val-2019 Task 4: {BERT}-Based Method in Hyperpartisan News Detection"",
",this paper describes the team peter-parker{'}s participation in hyperpartisan news detection task (semeval-2019 task 4) which requires to classify whether a given news article is bias or not. we decided to use java to do the article parsing tool and the bert-based model to do the bias prediction. furthermore we will show experiment results with analysis.
378,abdellatif-elgammal-2020-ulmfit,"   {ULMF}i{T} replication"",
",authors: mohamed abdellatif and ahmed elgammal gitlab url: https://gitlab.com/abdollatif/lrec{\_}app commit hash: 3f20b2ddb96d8c865e5f56f5566edf371214785f tag name: splits2 dataset file md5: 5aee3dac5e48d1ac3d279083212734c9 dataset url: https://drive.google.com/file/d/1cv5huqhgfvizupfi40dzreems2gmm498/view?usp=sharing
379,naderi-hirst-2017-classifying,"   Classifying Frames at the Sentence Level in News Articles"",
",previous approaches to generic frame classification analyze frames at the document level. here we propose a supervised based approach based on deep neural networks and distributional representations for classifying frames at the sentence level in news articles. we conduct our experiments on the publicly available media frames corpus compiled from the u.s. newspapers. using (b)lstms and gru networks to represent the meaning of frames we demonstrate that our approach yields at least 14-point improvement over several baseline methods.
380,popescu-grefenstette-2008-conceptual,"   A Conceptual Approach to Web Image Retrieval"",
",people use the internet to find a wide variety of images. existing image search engines do not understand the pictures they return. the introduction of semantic layers in information retrieval frameworks may enhance the quality of the results compared to existing systems. one important challenge in the field is to develop architectures that fit the requirements of real-life applications like the internet search engines. in this paper we describe olive an image retrieval application that exploits a large scale conceptual hierarchy (extracted from wordnet) to automatically reformulate user queries search for associated images and present results in an interactive and structured fashion. when searching a concept in the hierarchy olive reformulates the query using its deepest subtypes in wordnet. on the answers page the system displays a selection of related classes and proposes a content based retrieval functionality among the pictures sharing the same linguistic label. in order to validate our approach we run to series of tests to assess the performances of the application and report the results here. first two precision evaluations over a panel of concepts from different domains are realized and second a user test is designed so as to assess the interaction with the system.
381,hettiarachchi-ranasinghe-2021-transwic,"   {T}rans{W}i{C} at {S}em{E}val-2021 Task 2: Transformer-based Multilingual and Cross-lingual Word-in-Context Disambiguation"",
",identifying whether a word carries the same meaning or different meaning in two contexts is an important research area in natural language processing which plays a significant role in many applications such as question answering document summarisation information retrieval and information extraction. most of the previous work in this area rely on language-specific resources making it difficult to generalise across languages. considering this limitation our approach to semeval-2021 task 2 is based only on pretrained transformer models and does not use any language-specific processing and resources. despite that our best model achieves 0.90 accuracy for english-english subtask which is very compatible compared to the best result of the subtask; 0.93 accuracy. our approach also achieves satisfactory results in other monolingual and cross-lingual language pairs as well.
382,alahverdzhieva-lascarides-2011-integration,"   Int{\'e}gration de la parole et du geste d{\'e}ictique dans une grammaire multimodale (Integration of Speech and Deictic Gesture in a Multimodal Grammar)"",
",dans cet article nous pr{\'e}sentons une analyse {\`a} base de contraintes de la relation forme-sens des gestes d{\'e}ictiques et de leur signal de parole synchrone. en nous basant sur une {\'e}tude empirique de corpus multimodaux nous d{\'e}finissons quels {\'e}nonc{\'e}s multimodaux sont bien form{\'e}s et lesquels ne pourraient jamais produire le sens voulu dans la situation communicative. plus pr{\'e}cis{\'e}ment nous formulons une grammaire multimodale dont les r{\`e}gles de construction utilisent la prosodie la syntaxe et la s{\'e}mantique de la parole la forme et le sens du signal d{\'e}ictique ainsi que la performance temporelle de la parole et la deixis afin de contraindre la production d{'}un arbre de syntaxe combinant parole et gesture d{\'e}ictique ainsi que la repr{\'e}sentation unifi{\'e}e du sens pour l{'}action multimodale correspondant {\`a} cet arbre. la contribution de notre projet est double : nous ajoutons aux ressources existantes pour le tal un corpus annot{\'e} de parole et de gestes et nous cr{\'e}ons un cadre th{\'e}orique pour la grammaire au sein duquel la composition s{\'e}mantique d{'}un {\'e}nonc{\'e} d{\'e}coule de la synchronie entre geste et parole.
383,doan-bao-etal-2020-sunbear,"   {S}un{B}ear at {WNUT}-2020 Task 2: Improving {BERT}-Based Noisy Text Classification with Knowledge of the Data domain"",
",this paper proposes an improved custom model for wnut task 2: identification of informative covid-19 english tweet. we improve experiment with the effectiveness of fine-tuning methodologies for state-of-the-art language model roberta. we make a preliminary instantiation of this formal model for the text classification approaches. with appropriate training techniques our model is able to achieve 0.9218 f1-score on public validation set and the ensemble version settles at top 9 f1-score (0.9005) and top 2 recall (0.9301) on private test set.
384,chen-etal-2020-multi,"   Multi-choice Relational Reasoning for Machine Reading Comprehension"",
",this paper presents our study of cloze-style reading comprehension by imitating human reading comprehension which normally involves tactical comparing and reasoning over candidates while choosing the best answer. we propose a multi-choice relational reasoning (mcr$^2$) model with an aim to enable relational reasoning on candidates based on fusion representations of document query and candidates. for the fusion representations we develop an efficient encoding architecture by integrating the schemes of bidirectional attention flow self-attention and document-gated query reading. then comparing and inferring over candidates are executed by a novel relational reasoning network. we conduct extensive experiments on four datasets derived from two public corpora children{'}s book test and who did what to verify the validity and advantages of our model. the results show that it outperforms all baseline models significantly on the four benchmark datasets. the effectiveness of its key components is also validated by an ablation study.
385,nguyen-etal-2020-wnut,"   {WNUT}-2020 Task 2: Identification of Informative {COVID}-19 {E}nglish Tweets"",
",in this paper we provide an overview of the wnut-2020 shared task on the identification of informative covid-19 english tweets. we describe how we construct a corpus of 10k tweets and organize the development and evaluation phases for this task. in addition we also present a brief summary of results obtained from the final system evaluation submissions of 55 teams finding that (i) many systems obtain very high performance up to 0.91 f1 score (ii) the majority of the submissions achieve substantially higher results than the baseline fasttext (joulin et al. 2017) and (iii) fine-tuning pre-trained language models on relevant language data followed by supervised training performs well in this task.
386,schafer-2006-ontonerdie,"   {O}nto{NER}d{IE} {--} Mapping and Linking Ontologies to Named Entity Recognition and Information Extraction Resources"",
",semantic web and nlp we describe an implemented offline procedure that maps owl/rdf-encoded ontologies with large dynamically maintained instance data to named entity recognition (ner) and information extraction (ie) engine resources preserving hierarchical concept information and links back to the ontology concepts and instances. the main motivations are (i) improving ner/ie precision and recall in closed domains (ii) exploiting linguistic knowledge (context inflection anaphora) for identifying ontology instances in texts more robustly (iii) giving full access to ontology instances and concepts in natural language processing results e.g. for subsequent ontology queries navigation or inference (iv) avoiding duplication of work in development and maintenance of similar resources in independent places namely lingware and ontologies. we show an application in hybrid deep-shallow natural language processing that is e.g. used for question analysis in closed domains. further applications could be automatic hyperlinking or other innovative semantic-web related applications.
387,barrett-etal-2018-unsupervised,"   Unsupervised Induction of Linguistic Categories with Records of Reading, Speaking, and Writing"",
",when learning pos taggers and syntactic chunkers for low-resource languages different resources may be available and often all we have is a small tag dictionary motivating type-constrained unsupervised induction. even small dictionaries can improve the performance of unsupervised induction algorithms. this paper shows that performance can be further improved by including data that is readily available or can be easily obtained for most languages i.e. eye-tracking speech or keystroke logs (or any combination thereof). we project information from all these data sources into shared spaces in which the union of words is represented. for english unsupervised pos induction the additional information which is not required at test time leads to an average error reduction on ontonotes domains of 1.5{\%} over systems augmented with state-of-the-art word embeddings. on penn treebank the best model achieves 5.4{\%} error reduction over a word embeddings baseline. we also achieve significant improvements for syntactic chunk induction. our analysis shows that improvements are even bigger when the available tag dictionaries are smaller.
388,hernandez-abrego-etal-2020-self,"   Self-Supervised Learning for Pairwise Data Refinement"",
",pairwise data automatically constructed from weakly supervised signals has been widely used for training deep learning models. pairwise datasets such as parallel texts can have uneven quality levels overall but usually contain data subsets that are more useful as learning examples. we present two methods to refine data that are aimed to obtain that kind of subsets in a self-supervised way. our methods are based on iteratively training dual-encoder models to compute similarity scores. we evaluate our methods on de-noising parallel texts and training neural machine translation models. we find that: (i) the self-supervised refinement achieves most machine translation gains in the first iteration but following iterations further improve its intrinsic evaluation. (ii) machine translations can improve the de-noising performance when combined with selection steps. (iii) our methods are able to reach the performance of a supervised method. being entirely self-supervised our methods are well-suited to handle pairwise data without the need of prior knowledge or human annotations.
389,mrini-etal-2020-rethinking,"   Rethinking Self-Attention: Towards Interpretability in Neural Parsing"",
",attention mechanisms have improved the performance of nlp tasks while allowing models to remain explainable. self-attention is currently widely used however interpretability is difficult due to the numerous attention distributions. recent work has shown that model representations can benefit from label-specific information while facilitating interpretation of predictions. we introduce the label attention layer: a new form of self-attention where attention heads represent labels. we test our novel layer by running constituency and dependency parsing experiments and show our new model obtains new state-of-the-art results for both tasks on both the penn treebank (ptb) and chinese treebank. additionally our model requires fewer self-attention layers compared to existing work. finally we find that the label attention heads learn relations between syntactic categories and show pathways to analyze errors.
390,stefanec-etal-2016-croatian,"   {C}roatian Error-Annotated Corpus of Non-Professional Written Language"",
",in the paper authors present the croatian corpus of non-professional written language. consisting of two subcorpora i.e. the clinical subcorpus consisting of written texts produced by speakers with various types of language disorders and the healthy speakers subcorpus as well as by the levels of its annotation it offers an opportunity for different lines of research. the authors present the corpus structure describe the sampling methodology explain the levels of annotation and give some very basic statistics. on the basis of data from the corpus existing language technologies for croatian are adapted in order to be implemented in a platform facilitating text production to speakers with language disorders. in this respect several analyses of the corpus data and a basic evaluation of the developed technologies are presented.
391,yamamura-etal-2016-kyutech,"   The {K}yutech corpus and topic segmentation using a combined method"",
",summarization of multi-party conversation is one of the important tasks in natural language processing. in this paper we explain a japanese corpus and a topic segmentation task. to the best of our knowledge the corpus is the first japanese corpus annotated for summarization tasks and freely available to anyone. we call it {``}the kyutech corpus.{''} the task of the corpus is a decision-making task with four participants and it contains utterances with time information topic segmentation and reference summaries. as a case study for the corpus we describe a method combined with lcseg and topictiling for a topic segmentation task. we discuss the effectiveness and the problems of the combined method through the experiment with the kyutech corpus.
392,sankaran-etal-2008-common,"   A Common Parts-of-Speech Tagset Framework for {I}ndian Languages"",
",we present a universal parts-of-speech (pos) tagset framework covering most of the indian languages (ils) following the hierarchical and decomposable tagset schema. in spite of significant number of speakers there is no workable pos tagset and tagger for most ils which serve as fundamental building blocks for nlp research. existing il pos tagsets are often designed for a specific language; the few that have been designed for multiple languages cover only shallow linguistic features ignoring linguistic richness and the idiosyncrasies. the new framework that is proposed here addresses these deficiencies in an efficient and principled manner. we follow a hierarchical schema similar to that of eagles and this enables the framework to be flexible enough to capture rich features of a language/ language family even while capturing the shared linguistic structures in a methodical way. the proposed common framework further facilitates the sharing and reusability of scarce resources in these languages and ensures cross-linguistic compatibility.
393,guderlei-assenmacher-2020-evaluating,"   Evaluating Unsupervised Representation Learning for Detecting Stances of Fake News"",
",our goal is to evaluate the usefulness of unsupervised representation learning techniques for detecting stances of fake news. therefore we examine several pre-trained language models with respect to their performance on two fake news related data sets both consisting of instances with a headline an associated news article and the stance of the article towards the respective headline. specifically the aim is to understand how much hyperparameter tuning is necessary when fine-tuning the pre-trained architectures how well transfer learning works in this specific case of stance detection and how sensitive the models are to changes in hyperparameters like batch size learning rate (schedule) sequence length as well as the freezing technique. the results indicate that the computationally more expensive autoregression approach of xlnet (yanget al. 2019) is outperformed by bert-based models notably by roberta (liu et al. 2019).while the learning rate seems to be the most important hyperparameter experiments with different freezing techniques indicate that all evaluated architectures had already learned powerful language representations that pose a good starting point for fine-tuning them.
394,colson-2020-hmsid,"   {HMS}id and {HMS}id2 at {PARSEME} Shared Task 2020: Computational Corpus Linguistics and unseen-in-training {MWE}s"",
",this paper is a system description of hmsid officially sent to the parseme shared task 2020 for one language (french) in the open track. it also describes hmsid2 sent to the organ-izers of the workshop after the deadline and using the same methodology but in the closed track. both systems do not rely on machine learning but on computational corpus linguistics. their score for unseen mwes is very promising especially in the case of hmsid2 which would have received the best score for unseen mwes in the french closed track.
395,roy-etal-2018-ensemble,"   An Ensemble Approach for Aggression Identification in {E}nglish and {H}indi Text"",
",this paper describes our system submitted in the shared task at coling 2018 trac-1: aggression identification. the objective of this task was to predict online aggression spread through online textual post or comment. the dataset was released in two languages english and hindi. we submitted a single system for hindi and a single system for english. both the systems are based on an ensemble architecture where the individual models are based on convoluted neural network and support vector machine. evaluation shows promising results for both the languages.the total submission for english was 30 and hindi was 15. our system on english facebook and social media obtained f1 score of 0.5151 and 0.5099 respectively where hindi facebook and social media obtained f1 score of 0.5599 and 0.3790 respectively.
396,das-etal-2018-constructing,"   Constructing a Lexicon of {E}nglish Discourse Connectives"",
",we present a new lexicon of english discourse connectives called dimlex-eng built by merging information from two annotated corpora and an additional list of relation signals from the literature. the format follows the german connective lexicon dimlex which provides a cross-linguistically applicable xml schema. dimlex-eng contains 149 english connectives and gives information on syntactic categories discourse semantics and non-connective uses (if any). we report on the development steps and discuss design decisions encountered in the lexicon expansion phase. the resource is freely available for use in studies of discourse structure and computational applications.
397,teixeira-2014-perceived,"   Perceived vs. measured performance in the post-editing of suggestions from machine translation and translation memories"",
",this paper investigates the behaviour of ten professional translators when performing translation tasks with and without translation suggestions and with and without translation metadata. the measured performances are then compared with the translators{'} perceptions of their performances. the variables that are taken into consideration are time edits and errors. keystroke logging and screen recording are used to measure time and edits an error score system is used to identify errors and post-performance interviews are used to assess participants{'} perceptions. the study looks at the correlations between the translators{'} perceptions and their actual performances and tries to understand the reasons behind any discrepancies. translators are found to prefer an environment with translation suggestions and translation metadata to an environment without metadata. this preference however does not always correlate with an improved performance. task familiarity seems to be the most prominent factor responsible for the positive perceptions rather than any intrinsic characteristics in the tasks. a certain prejudice against mt is also present in some of the comments.
398,basaldella-etal-2017-exploiting,"   Exploiting and Evaluating a Supervised, Multilanguage Keyphrase Extraction pipeline for under-resourced languages"",
",this paper evaluates different techniques for building a supervised multilanguage keyphrase extraction pipeline for languages which lack a gold standard. starting from an unsupervised english keyphrase extraction pipeline we implement pipelines for arabic italian portuguese and romanian and we build test collections for languages which lack one. then we add a machine learning module trained on a well-known english language corpus and we evaluate the performance not only over english but on the other languages as well. finally we repeat the same evaluation after training the pipeline over an arabic language corpus to check whether using a language-specific corpus brings a further improvement in performance. on the five languages we analyzed results show an improvement in performance when using a machine learning algorithm even if such algorithm is not trained and tested on the same language.
399,chen-etal-2020-mixtext,"   {M}ix{T}ext: Linguistically-Informed Interpolation of Hidden Space for Semi-Supervised Text Classification"",
",this paper presents mixtext a semi-supervised learning method for text classification which uses our newly designed data augmentation method called tmix. tmix creates a large amount of augmented training samples by interpolating text in hidden space. moreover we leverage recent advances in data augmentation to guess low-entropy labels for unlabeled data hence making them as easy to use as labeled data. by mixing labeled unlabeled and augmented data mixtext significantly outperformed current pre-trained and fined-tuned models and other state-of-the-art semi-supervised learning methods on several text classification benchmarks. the improvement is especially prominent when supervision is extremely limited. we have publicly released our code at https://github.com/gt-salt/mixtext.
400,wang-etal-2017-ynudlg-semeval,"   {YNUDLG} at {S}em{E}val-2017 Task 4: A {GRU}-{SVM} Model for Sentiment Classification and Quantification in {T}witter"",
",sentiment analysis is one of the central issues in natural language processing and has become more and more important in many fields. typical sentiment analysis classifies the sentiment of sentences into several discrete classes (e.g.positive or negative). in this paper we describe our deep learning system(combining gru and svm) to solve both two- three- and five-tweet polarity classifications. we first trained a gated recurrent neural network using pre-trained word embeddings then we extracted features from gru layer and input these features into support vector machine to fulfill both the classification and quantification subtasks. the proposed approach achieved 37th 19th and 14rd places in subtasks a b and c respectively.
401,kachkovskaia-etal-2016-coruss,"   {C}o{R}u{SS} - a New Prosodically Annotated Corpus of {R}ussian Spontaneous Speech"",
",this paper describes speech data recording processing and annotation of a new speech corpus coruss (corpus of russian spontaneous speech) which is based on connected communicative speech recorded from 60 native russian male and female speakers of different age groups (from 16 to 77). some russian speech corpora available at the moment contain plain orthographic texts and provide some kind of limited annotation but there are no corpora providing detailed prosodic annotation of spontaneous conversational speech. this corpus contains 30 hours of high quality recorded spontaneous russian speech half of it has been transcribed and prosodically labeled. the recordings consist of dialogues between two speakers monologues (speakers{'} self-presentations) and reading of a short phonetically balanced text. since the corpus is labeled for a wide range of linguistic - phonetic and prosodic - information it provides basis for empirical studies of various spontaneous speech phenomena as well as for comparison with those we observe in prepared read speech. since the corpus is designed as a open-access resource of speech data it will also make possible to advance corpus-based analysis of spontaneous speech data across languages and speech technology development as well.
402,heinzerling-etal-2017-revisiting,"   Revisiting Selectional Preferences for Coreference Resolution"",
",selectional preferences have long been claimed to be essential for coreference resolution. however they are modeled only implicitly by current coreference resolvers. we propose a dependency-based embedding model of selectional preferences which allows fine-grained compatibility judgments with high coverage. incorporating our model improves performance matching state-of-the-art results of a more complex system. however it comes with a cost that makes it debatable how worthwhile are such improvements.
403,lo-wu-2010-evaluating,"   Evaluating Machine Translation Utility via Semantic Role Labels"",
",we present the methodology that underlies mew metrics for semantic machine translation evaluation we are developing. unlike widely-used lexical and n-gram based mt evaluation metrics the aim of semantic mt evaluation is to measure the utility of translations. we discuss the design of empirical studies to evaluate the utility of machine translation output by assessing the accuracy for key semantic roles. these roles are from the english 5w templates (who what when where why) used in recent gale distillation evaluations. recent work by wu and fung (2009) introduced semantic role labeling into statistical machine translation to enhance the quality of mt output. however this approach has so far only been evaluated using lexical and n-gram based smt evaluation metrics like bleu which are not aimed at evaluating the utility of mt output. direct data analysis are still needed to understand how semantic models can be leveraged to evaluate the utility of mt output. in this paper we discuss a new methodology for evaluating the utility of the machine translation output by assessing the accuracy with which human readers are able to complete the english 5w templates.
404,mladova-etal-2008-sentence,"   From Sentence to Discourse: Building an Annotation Scheme for Discourse Based on {P}rague Dependency Treebank"",
",the present paper reports on a preparatory research for building a language corpus annotation scenario capturing the discourse relations in czech. we primarily focus on the description of the syntactically motivated relations in discourse basing our findings on the theoretical background of the prague dependency treebank 2.0 and the penn discourse treebank 2. our aim is to revisit the present-day syntactico-semantic (tectogrammatical) annotation in the prague dependency treebank extend it for the purposes of a sentence-boundary-crossing representation and eventually to design a new discourse level of annotation. in this paper we propose a feasible process of such a transfer comparing the possibilities the praguian dependency-based approach offers with the penn discourse annotation based primarily on the analysis and classification of discourse connectives.
405,chauhan-etal-2019-reflex,"   {RE}flex: Flexible Framework for Relation Extraction in Multiple Domains"",
",systematic comparison of methods for relation extraction (re) is difficult because many experiments in the field are not described precisely enough to be completely reproducible and many papers fail to report ablation studies that would highlight the relative contributions of their various combined techniques. in this work we build a unifying framework for re applying this on three highly used datasets (from the general biomedical and clinical domains) with the ability to be extendable to new datasets. by performing a systematic exploration of modeling pre-processing and training methodologies we find that choices of preprocessing are a large contributor performance and that omission of such information can further hinder fair comparison. other insights from our exploration allow us to provide recommendations for future research in this area.
406,costa-jussa-fonollosa-2010-using,"   Using Linear Interpolation and Weighted Reordering Hypotheses in the {M}oses System"",
",this paper proposes to introduce a novel reordering model in the open-source moses toolkit. the main idea is to provide weighted reordering hypotheses to the smt decoder. these hypotheses are built using a first-step ngram-based smt translation from a source language into a third representation that is called reordered source language. each hypothesis has its own weight provided by the ngram-based decoder. this proposed reordering technique offers a better and more efficient translation when compared to both the distance-based and the lexicalized reordering. in addition to this reordering approach this paper describes a domain adaptation technique which is based on a linear combination of an specific in-domain and an extra out-domain translation models. results for both approaches are reported in the arabic-to-english 2008 iwslt task. when implementing the weighted reordering hypotheses and the domain adaptation technique in the final translation system translation results reach improvements up to 2.5 bleu compared to a standard state-of-the-art moses baseline system.
407,shin-etal-2020-biomegatron,"   {B}io{M}egatron: Larger Biomedical Domain Language Model"",
",there has been an influx of biomedical domain-specific language models showing language models pre-trained on biomedical text perform better on biomedical domain benchmarks than those trained on general domain text corpora such as wikipedia and books. yet most works do not study the factors affecting each domain language application deeply. additionally the study of model size on domain-specific models has been mostly missing. we empirically study and evaluate several factors that can affect performance on domain language applications such as the sub-word vocabulary set model size pre-training corpus and domain transfer. we show consistent improvements on benchmarks with our larger biomegatron model trained on a larger domain corpus contributing to our understanding of domain language model applications. we demonstrate noticeable improvements over the previous state-of-the-art (sota) on standard biomedical nlp benchmarks of question answering named entity recognition and relation extraction. code and checkpoints to reproduce our experiments are available at [github.com/nvidia/nemo].
408,edman-etal-2021-unsupervised,"   Unsupervised Translation of {G}erman{--}{L}ower {S}orbian: Exploring Training and Novel Transfer Methods on a Low-Resource Language"",
",this paper describes the methods behind the systems submitted by the university of groningen for the wmt 2021 unsupervised machine translation task for german{--}lower sorbian (de{--}dsb): a high-resource language to a low-resource one. our system uses a transformer encoder-decoder architecture in which we make three changes to the standard training procedure. first our training focuses on two languages at a time contrasting with a wealth of research on multilingual systems. second we introduce a novel method for initializing the vocabulary of an unseen language achieving improvements of 3.2 bleu for de-{\textgreater}dsb and 4.0 bleu for dsb-{\textgreater}de.lastly we experiment with the order in which offline and online back-translation are used to train an unsupervised system finding that using online back-translation first works better for de-{\textgreater}dsb by 2.76 bleu. our submissions ranked first (tied with another team) for dsb-{\textgreater}de and third for de-{\textgreater}dsb.
409,ji-etal-2020-diversified,"   Diversified Multiple Instance Learning for Document-Level Multi-Aspect Sentiment Classification"",
",neural document-level multi-aspect sentiment classification (dmsc) usually requires a lot of manual aspect-level sentiment annotations which is time-consuming and laborious. as document-level sentiment labeled data are widely available from online service it is valuable to perform dmsc with such free document-level annotations. to this end we propose a novel diversified multiple instance learning network (d-miln) which is able to achieve aspect-level sentiment classification with only document-level weak supervision. specifically we connect aspect-level and document-level sentiment by formulating this problem as multiple instance learning providing a way to learn aspect-level classifier from the back propagation of document-level supervision. two diversified regularizations are further introduced in order to avoid the overfitting on document-level signals during training. diversified textual regularization encourages the classifier to select aspect-relevant snippets and diversified sentimental regularization prevents the aspect-level sentiments from being overly consistent with document-level sentiment. experimental results on tripadvisor and beeradvocate datasets show that d-miln remarkably outperforms recent weakly-supervised baselines and is also comparable to the supervised method.
410,behnke-heafield-2021-pruning,"   Pruning Neural Machine Translation for Speed Using Group Lasso"",
",unlike most work on pruning neural networks we make inference faster. group lasso regularisation enables pruning entire rows columns or blocks of parameters that result in a smaller dense network. because the network is still dense efficient matrix multiply routines are still used and only minimal software changes are required to support variable layer sizes. moreover pruning is applied during training so there is no separate pruning step. experiments on top of english-{\textgreater}german models which already have state-of-the-art speed and size show that two-thirds of feedforward connections can be removed with 0.2 bleu loss. with 6 decoder layers the pruned model is 34{\%} faster; with 2 tied decoder layers the pruned model is 14{\%} faster. pruning entire heads and feedforward connections in a 12{--}1 encoder-decoder architecture gains an additional 51{\%} speed-up. these push the pareto frontier with respect to the trade-off between time and quality compared to strong baselines. in the wmt 2021 efficiency task our pruned and quantised models are 1.9{--}2.7x faster at the cost 0.9{--}1.7 bleu in comparison to the unoptimised baselines. across language pairs we see similar sparsity patterns: an ascending or u-shaped distribution in encoder feedforward and attention layers and an ascending distribution in the decoder.
411,gwinnup-etal-2019-afrl,"   The {AFRL} {WMT}19 Systems: Old Favorites and New Tricks"",
",this paper describes the air force research laboratory (afrl) machine translation systems and the improvements that were developed during the wmt19 evaluation campaign. this year we refine our approach to training popular neural machine translation toolkits experiment with a new domain adaptation technique and again measure improvements in performance on the russian{--}english language pair.
412,barbaresi-lejeune-2020-que,"   Que rec{\`e}lent les donn{\'e}es textuelles issues du web ? (What do text data from the Web have to hide ?)"",
",la collecte et l{'}usage opportunistes de donn{\'e}es textuelles tir{\'e}es du web sont sujets {\`a} une s{\'e}rie de probl{\`e}mes {\'e}thiques m{\'e}thodologiques et {\'e}pist{\'e}mologiques qui m{\'e}ritent l{'}attention de la communaut{\'e} scientifique. nous pr{\'e}sentons des {\'e}tudes empiriques de leur impact en linguistique et tal centr{\'e}es sur la forme (m{\'e}thodes d{'}extraction des donn{\'e}es) ainsi que sur le fond (contenu des corpus).
413,hu-etal-2021-syntax,"   Syntax Matters! Syntax-Controlled in Text Style Transfer"",
",existing text style transfer (tst) methods rely on style classifiers to disentangle the text{'}s content and style attributes for text style transfer. while the style classifier plays a critical role in existing tst methods there is no known investigation on its effect on the tst methods. in this paper we conduct an empirical study on the limitations of the style classifiers used in existing tst methods. we demonstrated that the existing style classifiers cannot learn sentence syntax effectively and ultimately worsen existing tst models{'} performance. to address this issue we propose a novel syntax-aware controllable generation (sacg) model which includes a syntax-aware style classifier that ensures learned style latent representations effectively capture the sentence structure for tst. through extensive experiments on two popular text style transfer tasks we show that our proposed method significantly outperforms twelve state-of-the-art methods. our case studies have also demonstrated sacg{'}s ability to generate fluent target-style sentences that preserved the original content.
414,michael-etal-2018-crowdsourcing,"   Crowdsourcing Question-Answer Meaning Representations"",
",we introduce question-answer meaning representations (qamrs) which represent the predicate-argument structure of a sentence as a set of question-answer pairs. we develop a crowdsourcing scheme to show that qamrs can be labeled with very little training and gather a dataset with over 5000 sentences and 100000 questions. a qualitative analysis demonstrates that the crowd-generated question-answer pairs cover the vast majority of predicate-argument relationships in existing datasets (including propbank nombank and qa-srl) along with many previously under-resourced ones including implicit arguments and relations. we also report baseline models for question generation and answering and summarize a recent approach for using qamr labels to improve an open ie system. these results suggest the freely available qamr data and annotation scheme should support significant future work.
415,trujillo-plowman-1991-automation,"   Automation of Bilingual Lexicon Compilation"",
",this paper shows that there are a number of common concepts which are used to define a class of nouns in standard monolingual english and spanish dictionaries. an experiment is described to show how a small sot of such con- cepts was derived semi-automatically by automatically analysing the definitions in each language and then matching equivalent definitions manually. also some of the benefits of constructing such sets are described together with the problems encountered while carrying out the experiment.
416,azab-etal-2018-speaker,"   Speaker Naming in Movies"",
",we propose a new model for speaker naming in movies that leverages visual textual and acoustic modalities in an unified optimization framework. to evaluate the performance of our model we introduce a new dataset consisting of six episodes of the big bang theory tv show and eighteen full movies covering different genres. our experiments show that our multimodal model significantly outperforms several competitive baselines on the average weighted f-score metric. to demonstrate the effectiveness of our framework we design an end-to-end memory network model that leverages our speaker naming model and achieves state-of-the-art results on the subtitles task of the movieqa 2017 challenge.
417,eder-etal-2021-acquiring,"   Acquiring a Formality-Informed Lexical Resource for Style Analysis"",
",to track different levels of formality in written discourse we introduce a novel type of lexicon for the german language with entries ordered by their degree of (in)formality. we start with a set of words extracted from traditional lexicographic resources extend it by sentence-based similarity computations and let crowdworkers assess the enlarged set of lexical items on a continuous informal-formal scale as a gold standard for evaluation. we submit this lexicon to an intrinsic evaluation related to the best regression models and their effect on predicting formality scores and complement our investigation by an extrinsic evaluation of formality on a german-language email corpus.
418,zhu-etal-2020-multimodal,"   Multimodal Joint Attribute Prediction and Value Extraction for {E}-commerce Product"",
",product attribute values are essential in many e-commerce scenarios such as customer service robots product recommendations and product retrieval. while in the real world the attribute values of a product are usually incomplete and vary over time which greatly hinders the practical applications. in this paper we propose a multimodal method to jointly predict product attributes and extract values from textual product descriptions with the help of the product images. we argue that product attributes and values are highly correlated e.g. it will be easier to extract the values on condition that the product attributes are given. thus we jointly model the attribute prediction and value extraction tasks from multiple aspects towards the interactions between attributes and values. moreover product images have distinct effects on our tasks for different product attributes and values. thus we selectively draw useful visual information from product images to enhance our model. we annotate a multimodal product attribute value dataset that contains 87194 instances and the experimental results on this dataset demonstrate that explicitly modeling the relationship between attributes and values facilitates our method to establish the correspondence between them and selectively utilizing visual product information is necessary for the task. our code and dataset are available at https://github.com/jd-aig/jave.
419,yao-etal-2020-ujnlp,"   {UJNLP} at {S}em{E}val-2020 Task 12: Detecting Offensive Language Using Bidirectional Transformers"",
",in this paper we built several pre-trained models to participate semeval-2020 task 12: multilingual offensive language identification in social media. in the common task of offensive language identification in social media pre-trained models such as bidirectional encoder representation from transformer (bert) have achieved good results. we preprocess the dataset by the language habits of users in social network. considering the data imbalance in offenseval we screened the newly provided machine annotation samples to construct a new dataset. we use the dataset to fine-tune the robustly optimized bert pretraining approach (roberta). for the english subtask b we adopted the method of adding auxiliary sentences (as) to transform the single-sentence classification task into a relationship recognition task between sentences. our team ujnlp wins the ranking 16th of 85 in english subtask a (offensive language identification).
420,singh-etal-2021-exploring,"   Exploring Methodologies for Collecting High-Quality Implicit Reasoning in Arguments"",
",annotation of implicit reasoning (i.e. warrant) in arguments is a critical resource to train models in gaining deeper understanding and correct interpretation of arguments. however warrants are usually annotated in unstructured form having no restriction on their lexical structure which sometimes makes it difficult to interpret how warrants relate to any of the information given in claim and premise. moreover assessing and determining better warrants from the large variety of reasoning patterns of unstructured warrants becomes a formidable task. therefore in order to annotate warrants in a more interpretative and restrictive way we propose two methodologies to annotate warrants in a semi-structured form. to the best of our knowledge we are the first to show how such semi-structured warrants can be annotated on a large scale via crowdsourcing. we demonstrate through extensive quality evaluation that our methodologies enable collecting better quality warrants in comparison to unstructured annotations. to further facilitate research towards the task of explicating warrants in arguments we release our materials publicly (i.e. crowdsourcing guidelines and collected warrants).
421,sen-groves-2021-semantic,"   Semantic Parsing of Disfluent Speech"",
",speech disfluencies are prevalent in spontaneous speech. the rising popularity of voice assistants presents a growing need to handle naturally occurring disfluencies. semantic parsing is a key component for understanding user utterances in voice assistants yet most semantic parsing research to date focuses on written text. in this paper we investigate semantic parsing of disfluent speech with the atis dataset. we find that a state-of-the-art semantic parser does not seamlessly handle disfluencies. we experiment with adding real and synthetic disfluencies at training time and find that adding synthetic disfluencies not only improves model performance by up to 39{\%} but can also outperform adding real disfluencies in the atis dataset.
422,abdul-mageed-etal-2021-nadi,"   {NADI} 2021: The Second Nuanced {A}rabic Dialect Identification Shared Task"",
",we present the findings and results of thesecond nuanced arabic dialect identificationshared task (nadi 2021). this shared taskincludes four subtasks: country-level modernstandard arabic (msa) identification (subtask1.1) country-level dialect identification (subtask1.2) province-level msa identification (subtask2.1) and province-level sub-dialect identifica-tion (subtask 2.2). the shared task dataset cov-ers a total of 100 provinces from 21 arab coun-tries collected from the twitter domain. a totalof 53 teams from 23 countries registered to par-ticipate in the tasks thus reflecting the interestof the community in this area. we received 16submissions for subtask 1.1 from five teams 27submissions for subtask 1.2 from eight teams12 submissions for subtask 2.1 from four teamsand 13 submissions for subtask 2.2 from fourteams.
423,malaviya-etal-2019-simple,"   A Simple Joint Model for Improved Contextual Neural Lemmatization"",
",english verbs have multiple forms. for instance talk may also appear as talks talked or talking depending on the context. the nlp task of lemmatization seeks to map these diverse forms back to a canonical one known as the lemma. we present a simple joint neural model for lemmatization and morphological tagging that achieves state-of-the-art results on 20 languages from the universal dependencies corpora. our paper describes the model in addition to training and decoding procedures. error analysis indicates that joint morphological tagging and lemmatization is especially helpful in low-resource lemmatization and languages that display a larger degree of morphological complexity.
424,wenzek-etal-2020-ccnet,"   {CCN}et: Extracting High Quality Monolingual Datasets from Web Crawl Data"",
",pre-training text representations have led to significant improvements in many areas of natural language processing. the quality of these models benefits greatly from the size of the pretraining corpora as long as its quality is preserved. in this paper we describe an automatic pipeline to extract massive high-quality monolingual datasets from common crawl for a variety of languages. our pipeline follows the data processing introduced in fasttext (mikolov et al. 2017; grave et al. 2018) that deduplicates documents and identifies their language. we augment this pipeline with a filtering step to select documents that are close to high quality corpora like wikipedia.
425,bastings-etal-2019-interpretable,"   Interpretable Neural Predictions with Differentiable Binary Variables"",
",the success of neural networks comes hand in hand with a desire for more interpretability. we focus on text classifiers and make them more interpretable by having them provide a justification{--}a rationale{--}for their predictions. we approach this problem by jointly training two neural network models: a latent model that selects a rationale (i.e. a short and informative part of the input text) and a classifier that learns from the words in the rationale alone. previous work proposed to assign binary latent masks to input positions and to promote short selections via sparsity-inducing penalties such as l0 regularisation. we propose a latent model that mixes discrete and continuous behaviour allowing at the same time for binary selections and gradient-based training without reinforce. in our formulation we can tractably compute the expected value of penalties such as l0 which allows us to directly optimise the model towards a pre-specified text selection rate. we show that our approach is competitive with previous work on rationale extraction and explore further uses in attention mechanisms.
426,ager-etal-2018-modelling,"   Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces"",
",in this paper we consider semantic spaces consisting of objects from some particular domain (e.g. imdb movie reviews). various authors have observed that such semantic spaces often model salient features (e.g. how scary a movie is) as directions. these feature directions allow us to rank objects according to how much they have the corresponding feature and can thus play an important role in interpretable classifiers recommendation systems or entity-oriented search engines among others. methods for learning semantic spaces however are mostly aimed at modelling similarity. in this paper we argue that there is an inherent trade-off between capturing similarity and faithfully modelling features as directions. following this observation we propose a simple method to fine-tune existing semantic spaces with the aim of improving the quality of their feature directions. crucially our method is fully unsupervised requiring only a bag-of-words representation of the objects as input.
427,knauth-2019-language,"   Language-Agnostic {T}witter-Bot Detection"",
",in this paper we address the problem of detecting twitter bots. we analyze a dataset of 8385 twitter accounts and their tweets consisting of both humans and different kinds of bots. we use this data to train machine learning classifiers that distinguish between real and bot accounts. we identify features that are easy to extract while still providing good results. we analyze different feature groups based on account specific tweet specific and behavioral specific features and measure their performance compared to other state of the art bot detection methods. for easy future portability of our work we focus on language-agnostic features. with adaboost the best performing classifier we achieve an accuracy of 0.988 and an auc of 0.995. as the creation of good training data in machine learning is often difficult - especially in the domain of twitter bot detection - we additionally analyze to what extent smaller amounts of training data lead to useful results by reviewing cross-validated learning curves. our results indicate that using few but expressive features already has a good practical benefit for bot detection especially if only a small amount of training data is available.
428,guhr-etal-2020-training,"   Training a Broad-Coverage {G}erman Sentiment Classification Model for Dialog Systems"",
",this paper describes the training of a general-purpose german sentiment classification model. sentiment classification is an important aspect of general text analytics. furthermore it plays a vital role in dialogue systems and voice interfaces that depend on the ability of the system to pick up and understand emotional signals from user utterances. the presented study outlines how we have collected a new german sentiment corpus and then combined this corpus with existing resources to train a broad-coverage german sentiment model. the resulting data set contains 5.4 million labelled samples. we have used the data to train both a simple convolutional and a transformer-based classification model and compared the results achieved on various training configurations. the model and the data set will be published along with this paper.
429,thorn-jakobsen-etal-2021-spurious,"   Spurious Correlations in Cross-Topic Argument Mining"",
",recent work in cross-topic argument mining attempts to learn models that generalise across topics rather than merely relying on within-topic spurious correlations. we examine the effectiveness of this approach by analysing the output of single-task and multi-task models for cross-topic argument mining through a combination of linear approximations of their decision boundaries manual feature grouping challenge examples and ablations across the input vocabulary. surprisingly we show that cross-topic models still rely mostly on spurious correlations and only generalise within closely related topics e.g. a model trained only on closed-class words and a few common open-class words outperforms a state-of-the-art cross-topic model on distant target topics.
430,zeng-etal-2021-wechat,"   {W}e{C}hat Neural Machine Translation Systems for {WMT}21"",
",this paper introduces wechat ai{'}s participation in wmt 2021 shared news translation task on english-{\textgreater}chinese english-{\textgreater}japanese japanese-{\textgreater}english and english-{\textgreater}german. our systems are based on the transformer (vaswani et al. 2017) with several novel and effective variants. in our experiments we employ data filtering large-scale synthetic data generation (i.e. back-translation knowledge distillation forward-translation iterative in-domain knowledge transfer) advanced finetuning approaches and boosted self-bleu based model ensemble. our constrained systems achieve 36.9 46.9 27.8 and 31.3 case-sensitive bleu scores on english-{\textgreater}chinese english-{\textgreater}japanese japanese-{\textgreater}english and english-{\textgreater}german respectively. the bleu scores of english-{\textgreater}chinese english-{\textgreater}japanese and japanese-{\textgreater}english are the highest among all submissions and that of english-{\textgreater}german is the highest among all constrained submissions.
431,haralabopoulos-etal-2020-objective,"   Objective Assessment of Subjective Tasks in Crowdsourcing Applications"",
",labelling or annotation is the process by which we assign labels to an item with regards to a task. in some artificial intelligence problems such as computer vision tasks the goal is to obtain objective labels. however in problems such as text and sentiment analysis subjective labelling is often required. more so when the sentiment analysis deals with actual emotions instead of polarity (positive/negative) . scientists employ human experts to create these labels but it is costly and time consuming. crowdsourcing enables researchers to utilise non-expert knowledge for scientific tasks. from image analysis to semantic annotation interested researchers can gather a large sample of answers via crowdsourcing platforms in a timely manner. however non-expert contributions often need to be thoroughly assessed particularly so when a task is subjective. researchers have traditionally used {`}gold standard{'} {`}thresholding{'} and {`}majority voting{'} as methods to filter non-expert contributions. we argue that these methods are unsuitable for subjective tasks such as lexicon acquisition and sentiment analysis. we discuss subjectivity in human centered tasks and present a filtering method that defines quality contributors based on a set of objectively infused terms in a lexicon acquisition task. we evaluate our method against an established lexicon the diversity of emotions - i.e. subjectivity- and the exclusion of contributions. our proposed objective evaluation method can be used to assess contributors in subjective tasks that will provide domain agnostic quality results with at least 7{\%} improvement over traditional methods.
432,tack-etal-2018-nt2lex,"   {NT}2{L}ex: A {CEFR}-Graded Lexical Resource for {D}utch as a Foreign Language Linked to Open {D}utch {W}ord{N}et"",
",in this paper we introduce nt2lex a novel lexical resource for dutch as a foreign language (nt2) which includes frequency distributions of 17743 words and expressions attested in expert-written textbook texts and readers graded along the scale of the common european framework of reference (cefr). in essence the lexicon informs us about what kind of vocabulary should be understood when reading dutch as a non-native reader at a particular proficiency level. the main novelty of the resource with respect to the previously developed cefr-graded lexicons concerns the introduction of corpus-based evidence for l2 word sense complexity through the linkage to open dutch wordnet (postma et al. 2016). the resource thus contains on top of the lemmatised and part-of-speech tagged lexical entries a total of 11999 unique word senses and 8934 distinct synsets.
433,yoshinaka-etal-2020-sapphire,"   {SAPPHIRE}: Simple Aligner for Phrasal Paraphrase with Hierarchical Representation"",
",we present sapphire a simple aligner for phrasal paraphrase with hierarchical representation. monolingual phrase alignment is a fundamental problem in natural language understanding and also a crucial technique in various applications such as natural language inference and semantic textual similarity assessment. previous methods for monolingual phrase alignment are language-resource intensive; they require large-scale synonym/paraphrase lexica and high-quality parsers. different from them sapphire depends only on a monolingual corpus to train word embeddings. therefore it is easily transferable to specific domains and different languages. specifically sapphire first obtains word alignments using pre-trained word embeddings and then expands them to phrase alignments by bilingual phrase extraction methods. to estimate the likelihood of phrase alignments sapphire uses phrase embeddings that are hierarchically composed of word embeddings. finally sapphire searches for a set of consistent phrase alignments on a lattice of phrase alignment candidates. it achieves search-efficiency by constraining the lattice so that all the paths go through a phrase alignment pair with the highest alignment score. experimental results using the standard dataset for phrase alignment evaluation show that sapphire outperforms the previous method and establishes the state-of-the-art performance.
434,eck-etal-2005-low-cost,"   Low Cost Portability for Statistical Machine Translation based on N-gram Coverage"",
",statistical machine translation relies heavily on the available training data. however in some cases it is necessary to limit the amount of training data that can be created for or actually used by the systems. to solve that problem we introduce a weighting scheme that tries to select more informative sentences first. this selection is based on the previously unseen n-grams the sentences contain and it allows us to sort the sentences according to their estimated importance. after sorting we can construct smaller training corpora and we are able to demonstrate that systems trained on much less training data show a very competitive performance compared to baseline systems using all available training data.
435,dai-etal-2018-fine,"   Fine-grained Structure-based News Genre Categorization"",
",journalists usually organize and present the contents of a news article following a well-defined structure. in this work we propose a new task to categorize news articles based on their content presentation structures which is beneficial for various nlp applications. we first define a small set of news elements considering their functions (e.g. \textit{introducing the main story or event catching the reader{'}s attention} and \textit{providing details}) in a news story and their writing style (\textit{narrative} or \textit{expository}) and then formally define four commonly used news article structures based on their selections and organizations of news elements. we create an annotated dataset for structure-based news genre identification and finally we build a predictive model to assess the feasibility of this classification task using structure indicative features.
436,dutta-chowdhury-etal-2018-multimodal,"   Multimodal Neural Machine Translation for Low-resource Language Pairs using Synthetic Data"",
",in this paper we investigate the effectiveness of training a multimodal neural machine translation (mnmt) system with image features for a low-resource language pair hindi and english using synthetic data. a three-way parallel corpus which contains bilingual texts and corresponding images is required to train a mnmt system with image features. however such a corpus is not available for low resource language pairs. to address this we developed both a synthetic training dataset and a manually curated development/test dataset for hindi based on an existing english-image parallel corpus. we used these datasets to build our image description translation system by adopting state-of-the-art mnmt models. our results show that it is possible to train a mnmt system for low-resource language pairs through the use of synthetic data and that such a system can benefit from image features.
437,matsushita-etal-2021-improving,"   Improving Neural Language Processing with Named Entities"",
",pretraining-based neural network models have demonstrated state-of-the-art (sota) performances on natural language processing (nlp) tasks. the most frequently used sentence representation for neural-based nlp methods is a sequence of subwords that is different from the sentence representation of non-neural methods that are created using basic nlp technologies such as part-of-speech (pos) tagging named entity (ne) recognition and parsing. most neural-based nlp models receive only vectors encoded from a sequence of subwords obtained from an input text. however basic nlp information such as pos tags nes parsing results etc cannot be obtained explicitly from only the large unlabeled text used in pretraining-based models. this paper explores use of nes on two japanese tasks; document classification and headline generation using transformer-based models to reveal the effectiveness of basic nlp information. the experimental results with eight basic nes and approximately 200 extended nes show that nes improve accuracy although a large pretraining-based model trained using 70 gb text data was used.
438,liu-etal-2021-progressively,"   Progressively Guide to Attend: An Iterative Alignment Framework for Temporal Sentence Grounding"",
",a key solution to temporal sentence grounding (tsg) exists in how to learn effective alignment between vision and language features extracted from an untrimmed video and a sentence description. existing methods mainly leverage vanilla soft attention to perform the alignment in a single-step process. however such single-step attention is insufficient in practice since complicated relations between inter- and intra-modality are usually obtained through multi-step reasoning. in this paper we propose an iterative alignment network (ia-net) for tsg task which iteratively interacts inter- and intra-modal features within multiple steps for more accurate grounding. specifically during the iterative reasoning process we pad multi-modal features with learnable parameters to alleviate the nowhere-to-attend problem of non-matched frame-word pairs and enhance the basic co-attention mechanism in a parallel manner. to further calibrate the misaligned attention caused by each reasoning step we also devise a calibration module following each attention module to refine the alignment knowledge. with such iterative alignment scheme our ia-net can robustly capture the fine-grained relations between vision and language domains step-by-step for progressively reasoning the temporal boundaries. extensive experiments conducted on three challenging benchmarks demonstrate that our proposed model performs better than the state-of-the-arts.
439,guo-etal-2019-star,"   Star-Transformer"",
",although transformer has achieved great successes on many nlp tasks its heavy structure with fully-connected attention connections leads to dependencies on large training data. in this paper we present star-transformer a lightweight alternative by careful sparsification. to reduce model complexity we replace the fully-connected structure with a star-shaped topology in which every two non-adjacent nodes are connected through a shared relay node. thus complexity is reduced from quadratic to linear while preserving the capacity to capture both local composition and long-range dependency. the experiments on four tasks (22 datasets) show that star-transformer achieved significant improvements against the standard transformer for the modestly sized datasets.
440,peters-etal-2004-future,"   The Future of Evaluation for Cross-Language Information Retrieval Systems"",
",the objective of the cross-language evaluation forum (clef) is to promote research in the multilingual information access domain. in this short paper we list the achievements of clef during its first four years of activity and describe how the range of tasks has been considerably expanded during this period. the aim of the paper is to demonstrate the importance of evaluation initiatives with respect to system research and development and to show how essential it is for such initiatives to keep abreast of and even anticipate the emerging needs of both system developers and application communities if they are to have a future.
441,costa-jussa-etal-2017-byte,"   Byte-based Neural Machine Translation"",
",this paper presents experiments comparing character-based and byte-based neural machine translation systems. the main motivation of the byte-based neural machine translation system is to build multi-lingual neural machine translation systems that can share the same vocabulary. we compare the performance of both systems in several language pairs and we see that the performance in test is similar for most language pairs while the training time is slightly reduced in the case of byte-based neural machine translation.
442,wu-etal-2021-conditional,"   Conditional Adversarial Networks for Multi-Domain Text Classification"",
",in this paper we propose conditional adversarial networks (cans) a framework that explores the relationship between the shared features and the label predictions to impose stronger discriminability to the learned features for multi-domain text classification (mdtc). the proposed can introduces a conditional domain discriminator to model the domain variance in both the shared feature representations and the class-aware information simultaneously and adopts entropy conditioning to guarantee the transferability of the shared features. we provide theoretical analysis for the can framework showing that can{'}s objective is equivalent to minimizing the total divergence among multiple joint distributions of shared features and label predictions. therefore can is a theoretically sound adversarial network that discriminates over multiple distributions. evaluation results on two mdtc benchmarks show that can outperforms prior methods. further experiments demonstrate that can has a good ability to generalize learned knowledge to unseen domains.
443,dong-etal-2021-injecting,"   Injecting Entity Types into Entity-Guided Text Generation"",
",recent successes in deep generative modeling have led to significant advances in natural language generation (nlg). incorporating entities into neural generation models has demonstrated great improvements by assisting to infer the summary topic and to generate coherent content. to enhance the role of entity in nlg in this paper we aim to model the entity type in the decoding phase to generate contextual words accurately. we develop a novel nlg model to produce a target sequence based on a given list of entities. our model has a multi-step decoder that injects the entity types into the process of entity mention generation. experiments on two public news datasets demonstrate type injection performs better than existing type embedding concatenation baselines.
444,zhai-2018-construction,"   Construction d{'}un corpus multilingue annot{\'e} en relations de traduction (Construction of a multilingual corpus annotated with translation relations )"",
",les relations de traduction qui distinguent la traduction litt{\'e}rale d{'}autres proc{\'e}d{\'e}s constituent un sujet d{'}{\'e}tude important pour les traducteurs humains (chuquet {\&} paillard 1989). or les traitements automatiques fond{\'e}s sur des relations entre langues tels que la traduction automatique ou la m{\'e}thode de g{\'e}n{\'e}ration de paraphrases par {\'e}quivalence de traduction ne les ont pas exploit{\'e}es explicitement jusqu{'}{\`a} pr{\'e}sent. dans ce travail nous pr{\'e}sentons une cat{\'e}gorisation des relations de traduction et nous les annotons dans un corpus parall{\`e}le multilingue (anglais fran{\c{c}}ais chinois) de pr{\'e}sentations orales les ted talks. notre objectif {\`a} plus long terme sera d{'}en faire la d{\'e}tection de mani{\`e}re automatique afin de pouvoir les int{\'e}grer comme caract{\'e}ristiques importantes pour la recherche de segments monolingues en relation d{'}{\'e}quivalence (paraphrases) ou d{'}implication. le corpus annot{\'e} r{\'e}sultant de notre travail sera mis {\`a} disposition de la communaut{\'e}.
445,babko-malaya-2008-annotation,"   Annotation of Nuggets and Relevance in {GALE} Distillation Evaluation"",
",this paper presents an approach to annotation that bae systems has employed in the darpa gale phase 2 distillation evaluation. the purpose of the gale distillation evaluation is to quantify the amount of relevant and non-redundant information a distillation engine is able to produce in response to a specific formatted query; and to compare that amount of information to the amount of information gathered by a bilingual human using commonly available state-of-the-art tools. as part of the evaluation following nist evaluation methodology of complex question answering (voorhees 2003) human annotators were asked to establish the relevancy of responses as well as the presence of atomic facts or information units called nuggets of information. this paper discusses various challenges to the annotation of nuggets called nuggetization which include interaction between the granularity of nuggets and relevancy of these nuggets to the query in question. the approach proposed in the paper views nuggetization as a procedural task and allows annotators to revisit nuggetization based on the requirements imposed by the relevancy guidelines defined with a specific end-user in mind. this approach is shown in the paper to produce consistent annotations with high inter-annotator agreement scores.
446,maziarz-piasecki-2018-towards,"   Towards Mapping Thesauri onto pl{W}ord{N}et"",
",plwordnet the wordnet of polish has become a very comprehensive description of the polish lexical system. this paper presents a plan of its semi-automated integration with thesauri terminological databases and ontologies as a further necessary step in its development. this will improve linking of plwordnet into linked open data and facilitate applications in e.g. wsd keyword extraction or automated metadata generation. we present an overview of resources relevant to polish and a plan for their linking to plwordnet.
447,langlais-2009-etude,"   {\'E}tude quantitative de liens entre l{'}analogie formelle et la morphologie constructionnelle"",
",plusieurs travaux ont r{\'e}cemment {\'e}tudi{\'e} l{'}apport de l{'}apprentissage analogique dans des applications du traitement automatique des langues comme la traduction automatique ou la recherche d{'}information. il est souvent admis que les relations analogiques de forme entre les mots capturent des informations de nature morphologique. le but de cette {\'e}tude est de pr{\'e}senter une analyse des points de rencontre entre l{'}analyse morphologique et les analogies de forme. c{'}est {\`a} notre connaissance la premi{\`e}re {\'e}tude de ce type portant sur des corpus de grande taille et sur plusieurs langues. bien que notre {\'e}tude ne soit pas d{\'e}di{\'e}e {\`a} une t{\^a}che particuli{\`e}re du traitement des langues nous montrons cependant que le principe d{'}analogie permet de segmenter des mots en morph{\`e}mes avec une bonne pr{\'e}cision.
448,kuru-etal-2016-charner,"   {C}har{NER}: Character-Level Named Entity Recognition"",
",we describe and evaluate a character-level tagger for language-independent named entity recognition (ner). instead of words a sentence is represented as a sequence of characters. the model consists of stacked bidirectional lstms which inputs characters and outputs tag probabilities for each character. these probabilities are then converted to consistent word level named entity tags using a viterbi decoder. we are able to achieve close to state-of-the-art ner performance in seven languages with the same basic model using only labeled ner data and no hand-engineered features or other external resources like syntactic taggers or gazetteers.
449,zouhar-2021-sampling,"   Sampling and Filtering of Neural Machine Translation Distillation Data"",
",in most of neural machine translation distillation or stealing scenarios the highest-scoring hypothesis of the target model (teacher) is used to train a new model (student). if reference translations are also available then better hypotheses (with respect to the references) can be oversampled and poor hypotheses either removed or undersampled. this paper explores the sampling method landscape (pruning hypothesis oversampling and undersampling deduplication and their combination) with english to czech and english to german mt models using standard mt evaluation metrics. we show that careful oversampling and combination with the original data leads to better performance when compared to training only on the original or synthesized data or their direct combination.
450,bouamor-etal-2019-madar,"   The {MADAR} Shared Task on {A}rabic Fine-Grained Dialect Identification"",
",in this paper we present the results and findings of the madar shared task on arabic fine-grained dialect identification. this shared task was organized as part of the fourth arabic natural language processing workshop collocated with acl 2019. the shared task includes two subtasks: the madar travel domain dialect identification subtask (subtask 1) and the madar twitter user dialect identification subtask (subtask 2). this shared task is the first to target a large set of dialect labels at the city and country levels. the data for the shared task was created or collected under the multi-arabic dialect applications and resources (madar) project. a total of 21 teams from 15 countries participated in the shared task.
451,bambrick-etal-2020-nstm,"   {NSTM}: Real-Time Query-Driven News Overview Composition at {B}loomberg"",
",millions of news articles from hundreds of thousands of sources around the globe appear in news aggregators every day. consuming such a volume of news presents an almost insurmountable challenge. for example a reader searching on bloomberg{'}s system for news about the u.k. would find 10000 articles on a typical day. apple inc. the world{'}s most journalistically covered company garners around 1800 news articles a day. we realized that a new kind of summarization engine was needed one that would condense large volumes of news into short easy to absorb points. the system would filter out noise and duplicates to identify and summarize key news about companies countries or markets. when given a user query bloomberg{'}s solution key news themes (or nstm) leverages state-of-the-art semantic clustering techniques and novel summarization methods to produce comprehensive yet concise digests to dramatically simplify the news consumption process. nstm is available to hundreds of thousands of readers around the world and serves thousands of requests daily with sub-second latency. at acl 2020 we will present a demo of nstm.
452,ramponi-plank-2020-neural,"   Neural Unsupervised Domain Adaptation in {NLP}{---}{A} Survey"",
",deep neural networks excel at learning from labeled data and achieve state-of-the-art results on a wide array of natural language processing tasks. in contrast learning from unlabeled data especially under domain shift remains a challenge. motivated by the latest advances in this survey we review neural unsupervised domain adaptation techniques which do not require labeled target domain data. this is a more challenging yet a more widely applicable setup. we outline methods from early traditional non-neural methods to pre-trained model transfer. we also revisit the notion of domain and we uncover a bias in the type of natural language processing tasks which received most attention. lastly we outline future directions particularly the broader need for out-of-distribution generalization of future nlp.
453,khashabi-etal-2017-learning,"   Learning What is Essential in Questions"",
",question answering (qa) systems are easily distracted by irrelevant or redundant words in questions especially when faced with long or multi-sentence questions in difficult domains. this paper introduces and studies the notion of essential question terms with the goal of improving such qa solvers. we illustrate the importance of essential question terms by showing that humans{'} ability to answer questions drops significantly when essential terms are eliminated from questions.we then develop a classifier that reliably (90{\%} mean average precision) identifies and ranks essential terms in questions. finally we use the classifier to demonstrate that the notion of question term essentiality allows state-of-the-art qa solver for elementary-level science questions to make better and more informed decisionsimproving performance by up to 5{\%}.we also introduce a new dataset of over 2200 crowd-sourced essential terms annotated science questions.
454,wisler-etal-2019-speech,"   Speech-based Estimation of Bulbar Regression in Amyotrophic Lateral Sclerosis"",
",amyotrophic lateral sclerosis (als) is a progressive neurological disease that leads to degeneration of motor neurons and as a result inhibits the ability of the brain to control muscle movements. monitoring the progression of als is of fundamental importance due to the wide variability in disease outlook that exists across patients. this progression is typically tracked using the als functional rating scale - revised (alsfrs-r) which is the current clinical assessment of a patient{'}s level of functional impairment including speech and other motor tasks. in this paper we investigated automatic estimation of the alsfrs-r bulbar subscore from acoustic and articulatory movement samples. experimental results demonstrated the afsfrs-r bulbar subscore can be predicted from speech samples which has clinical implication for automatic monitoring of the disease progression of als using speech information.
455,wu-etal-2020-corefqa,"   {C}oref{QA}: Coreference Resolution as Query-based Span Prediction"",
",in this paper we present corefqa an accurate and extensible approach for the coreference resolution task. we formulate the problem as a span prediction task like in question answering: a query is generated for each candidate mention using its surrounding context and a span prediction module is employed to extract the text spans of the coreferences within the document using the generated query. this formulation comes with the following key advantages: (1) the span prediction strategy provides the flexibility of retrieving mentions left out at the mention proposal stage; (2) in the question answering framework encoding the mention and its context explicitly in a query makes it possible to have a deep and thorough examination of cues embedded in the context of coreferent mentions; and (3) a plethora of existing question answering datasets can be used for data augmentation to improve the model{'}s generalization capability. experiments demonstrate significant performance boost over previous models with 83.1 (+3.5) f1 score on the conll-2012 benchmark and 87.5 (+2.5) f1 score on the gap benchmark.
456,fu-1999-research,"   The research and development of machine translation in {C}hina"",
",this survey of the situation regarding the r{\&}d of machine translation in china concentrates on how the design and application of mt systems could be on the shadow of the characteristics of chinese language. after a brief historical overview and the investigation on r{\&}d environment some technical features are described including commercialization chinese-english language pair and a multi-level transfer in english-chinese mt systems.
457,gu-etal-2020-filtering,"   Filtering before Iteratively Referring for Knowledge-Grounded Response Selection in Retrieval-Based Chatbots"",
",the challenges of building knowledge-grounded retrieval-based chatbots lie in how to ground a conversation on its background knowledge and how to match response candidates with both context and knowledge simultaneously. this paper proposes a method named filtering before iteratively referring (fire) for this task. in this method a context filter and a knowledge filter are first built which derive knowledge-aware context representations and context-aware knowledge representations respectively by global and bidirectional attention. besides the entries irrelevant to the conversation are discarded by the knowledge filter. after that iteratively referring is performed between context and response representations as well as between knowledge and response representations in order to collect deep matching features for scoring response candidates. experimental results show that fire outperforms previous methods by margins larger than 2.8{\%} and 4.1{\%} on the persona-chat dataset with original and revised personas respectively and margins larger than 3.1{\%} on the cmu{\_}dog dataset in terms of top-1 accuracy. we also show that fire is more interpretable by visualizing the knowledge grounding process.
458,reese-etal-2010-wikicorpus,"   {W}ikicorpus: A Word-Sense Disambiguated Multilingual {W}ikipedia Corpus"",
",this article presents a new freely available trilingual corpus (catalan spanish english) that contains large portions of the wikipedia and has been automatically enriched with linguistic information. to our knowledge this is the largest such corpus that is freely available to the community: in its present version it contains over 750 million words. the corpora have been annotated with lemma and part of speech information using the open source library freeling. also they have been sense annotated with the state of the art word sense disambiguation algorithm ukb. as ukb assigns wordnet senses and wordnet has been aligned across languages via the interlingual index this sort of annotation opens the way to massive explorations in lexical semantics that were not possible before. we present a first attempt at creating a trilingual lexical resource from the sense-tagged wikipedia corpora namely wikinet. moreover we present two by-products of the project that are of use for the nlp community: an open source java-based parser for wikipedia pages developed for the construction of the corpus and the integration of the wsd algorithm ukb in freeling.
459,arase-etal-2020-annotation,"   Annotation of Adverse Drug Reactions in Patients{'} Weblogs"",
",adverse drug reactions are a severe problem that significantly degrade quality of life or even threaten the life of patients. patient-generated texts available on the web have been gaining attention as a promising source of information in this regard. while previous studies annotated such patient-generated content they only reported on limited information such as whether a text described an adverse drug reaction or not. further they only annotated short texts of a few sentences crawled from online forums and social networking services. the dataset we present in this paper is unique for the richness of annotated information including detailed descriptions of drug reactions with full context. we crawled patient{'}s weblog articles shared on an online patient-networking platform and annotated the effects of drugs therein reported. we identified spans describing drug reactions and assigned labels for related drug names standard codes for the symptoms of the reactions and types of effects. as a first dataset we annotated 677 drug reactions with these detailed labels based on 169 weblog articles by japanese lung cancer patients. our annotation dataset is made publicly available at our web site (https://yukiar.github.io/adr-jp/) for further research on the detection of adverse drug reactions and more broadly on patient-generated text processing.
460,uresova-etal-2014-multilingual,"   Multilingual Test Sets for Machine Translation of Search Queries for Cross-Lingual Information Retrieval in the Medical Domain"",
",this paper presents development and test sets for machine translation of search queries in cross-lingual information retrieval in the medical domain. the data consists of the total of 1508 real user queries in english translated to czech german and french. we describe the translation and review process involving medical professionals and present a baseline experiment where our data sets are used for tuning and evaluation of a machine translation system.
461,du-black-2019-top,"   Top-Down Structurally-Constrained Neural Response Generation with Lexicalized Probabilistic Context-Free Grammar"",
",we consider neural language generation under a novel problem setting: generating the words of a sentence according to the order of their first appearance in its lexicalized pcfg parse tree in a depth-first left-to-right manner. unlike previous tree-based language generation methods our approach is both (i) top-down and (ii) explicitly generating syntactic structure at the same time. in addition our method combines neural model with symbolic approach: word choice at each step is constrained by its predicted syntactic function. we applied our model to the task of dialog response generation and found it significantly improves over sequence-to-sequence baseline in terms of diversity and relevance. we also investigated the effect of lexicalization on language generation and found that lexicalization schemes that give priority to content words have certain advantages over those focusing on dependency relations.
462,klie-etal-2021-human,"   Human-In-The-{L}oop{E}ntity Linking for Low Resource Domains"",
",entity linking (el) is concerned with disambiguating entity mentions in a text against knowledge bases (kb). to quickly annotate texts with el even in low-resource domains and noisy text we present a novel human-in-the-loop el approach. we show that it greatly outperforms a strong baseline in simulation. in a user study annotation time is reduced by 35 {\%} compared to annotating without interactive support; users report that they strongly prefer our system over ones without. an open-source and ready-to-use implementation based on the text annotation platform is made available.
463,hall-maudslay-etal-2020-tale,"   A Tale of a Probe and a Parser"",
",measuring what linguistic information is encoded in neural models of language has become popular in nlp. researchers approach this enterprise by training {``}probes{''}{---}supervised models designed to extract linguistic structure from another model{'}s output. one such probe is the structural probe (hewitt and manning 2019) designed to quantify the extent to which syntactic information is encoded in contextualised word representations. the structural probe has a novel design unattested in the parsing literature the precise benefit of which is not immediately obvious. to explore whether syntactic probes would do better to make use of existing techniques we compare the structural probe to a more traditional parser with an identical lightweight parameterisation. the parser outperforms structural probe on uuas in seven of nine analysed languages often by a substantial amount (e.g. by 11.1 points in english). under a second less common metric however there is the opposite trend{---}the structural probe outperforms the parser. this begs the question: which metric should we prefer?
464,xu-bethard-2021-triplet,"   Triplet-Trained Vector Space and Sieve-Based Search Improve Biomedical Concept Normalization"",
",concept normalization the task of linking textual mentions of concepts to concepts in an ontology is critical for mining and analyzing biomedical texts. we propose a vector-space model for concept normalization where mentions and concepts are encoded via transformer networks that are trained via a triplet objective with online hard triplet mining. the transformer networks refine existing pre-trained models and the online triplet mining makes training efficient even with hundreds of thousands of concepts by sampling training triples within each mini-batch. we introduce a variety of strategies for searching with the trained vector-space model including approaches that incorporate domain-specific synonyms at search time with no model retraining. across five datasets our models that are trained only once on their corresponding ontologies are within 3 points of state-of-the-art models that are retrained for each new domain. our models can also be trained for each domain achieving new state-of-the-art on multiple datasets.
465,lange-etal-2020-closing,"   Closing the Gap: Joint De-Identification and Concept Extraction in the Clinical Domain"",
",exploiting natural language processing in the clinical domain requires de-identification i.e. anonymization of personal information in texts. however current research considers de-identification and downstream tasks such as concept extraction only in isolation and does not study the effects of de-identification on other tasks. in this paper we close this gap by reporting concept extraction performance on automatically anonymized data and investigating joint models for de-identification and concept extraction. in particular we propose a stacked model with restricted access to privacy sensitive information and a multitask model. we set the new state of the art on benchmark datasets in english (96.1{\%} f1 for de-identification and 88.9{\%} f1 for concept extraction) and spanish (91.4{\%} f1 for concept extraction).
466,mishra-sachdeva-2020-need,"   Do We Need to Create Big Datasets to Learn a Task?"",
",deep learning research has been largely accelerated by the development of huge datasets such as imagenet. the general trend has been to create big datasets to make a deep neural network learn. a huge amount of resources is being spent in creating these big datasets developing models training them and iterating this process to dominate leaderboards. we argue that the trend of creating bigger datasets needs to be revised by better leveraging the power of pre-trained language models. since the language models have already been pre-trained with huge amount of data and have basic linguistic knowledge there is no need to create big datasets to learn a task. instead we need to create a dataset that is sufficient for the model to learn various task-specific terminologies such as {`}entailment{'} {`}neutral{'} and {`}contradiction{'} for nli. as evidence we show that roberta is able to achieve near-equal performance on 2{\%} data of snli. we also observe competitive zero-shot generalization on several ood datasets. in this paper we propose a baseline algorithm to find the optimal dataset for learning a task.
467,sato-heffernan-2020-homonym,"   Homonym normalisation by word sense clustering: a case in {J}apanese"",
",this work presents a method of word sense clustering that differentiates homonyms and merge homophones taking japanese as an example where orthographical variation causes problem for language processing. it uses contextualised embeddings (bert) to cluster tokens into distinct sense groups and we use these groups to normalise synonymous instances to a single representative form. we see the benefit of this normalisation in language model as well as in transliteration.
468,fu-etal-2020-rethinkcws,"   {R}ethink{CWS}: Is {C}hinese Word Segmentation a Solved Task?"",
",the performance of the chinese word segmentation (cws) systems has gradually reached a plateau with the rapid development of deep neural networks especially the successful use of large pre-trained models. in this paper we take stock of what we have achieved and rethink what{'}s left in the cws task. methodologically we propose a fine-grained evaluation for existing cws systems which not only allows us to diagnose the strengths and weaknesses of existing models (under the in-dataset setting) but enables us to quantify the discrepancy between different criterion and alleviate the negative transfer problem when doing multi-criteria learning. strategically despite not aiming to propose a novel model in this paper our comprehensive experiments on eight models and seven datasets as well as thorough analysis could search for some promising direction for future research. we make all codes publicly available and release an interface that can quickly evaluate and diagnose user{'}s models: https://github.com/neulab/interpreteval
469,qasemizadeh-etal-2019-semeval,"   {S}em{E}val-2019 Task 2: Unsupervised Lexical Frame Induction"",
",this paper presents unsupervised lexical frame induction task 2 of the international workshop on semantic evaluation in 2019. given a set of prespecified syntactic forms in context the task requires that verbs and their arguments be clustered to resemble semantic frame structures. results are useful in identifying polysemous words i.e. those whose frame structures are not easily distinguished as well as discerning semantic relations of the arguments. evaluation of unsupervised frame induction methods fell into two tracks: task a) verb clustering based on framenet 1.7; and b) argument clustering with b.1) based on framenet{'}s core frame elements and b.2) on verbnet 3.2 semantic roles. the shared task attracted nine teams of whom three reported promising results. this paper describes the task and its data reports on methods and resources that these systems used and offers a comparison to human annotation.
470,kaljahi-foster-2018-sentiment,"   Sentiment Expression Boundaries in Sentiment Polarity Classification"",
",we investigate the effect of using sentiment expression boundaries in predicting sentiment polarity in aspect-level sentiment analysis. we manually annotate a freely available english sentiment polarity dataset with these boundaries and carry out a series of experiments which demonstrate that high quality sentiment expressions can boost the performance of polarity classification. our experiments with neural architectures also show that cnn networks outperform lstms on this task and dataset.
471,iomdin-2016-microsyntactic,"   Microsyntactic Phenomena as a Computational Linguistics Issue"",
",microsyntactic linguistic units such as syntactic idioms and non-standard syntactic constructions are poorly represented in linguistic resources mostly because the former are elements occupying an intermediate position between the lexicon and the grammar and the latter are too specific to be routinely tackled by general grammars. consequently many such units produce substantial gaps in systems intended to solve sophisticated computational linguistics tasks such as parsing deep semantic analysis question answering machine translation or text generation. they also present obstacles for applying advanced techniques to these tasks such as machine learning. the paper discusses an approach aimed at bridging such gaps focusing on the development of monolingual and multilingual corpora where microsyntactic units are to be tagged.
472,nicosia-moschitti-2017-learning,"   Learning Contextual Embeddings for Structural Semantic Similarity using Categorical Information"",
",tree kernels (tks) and neural networks are two effective approaches for automatic feature engineering. in this paper we combine them by modeling context word similarity in semantic tks. this way the latter can operate subtree matching by applying neural-based similarity on tree lexical nodes. we study how to learn representations for the words in context such that tks can exploit more focused information. we found that neural embeddings produced by current methods do not provide a suitable contextual similarity. thus we define a new approach based on a siamese network which produces word representations while learning a binary text similarity. we set the latter considering examples in the same category as similar. the experiments on question and sentiment classification show that our semantic tk highly improves previous results.
473,federmann-etal-2012-meta,"   {META}-{SHARE} v2: An Open Network of Repositories for Language Resources including Data and Tools"",
",we describe meta-share which aims at providing an open distributed secure and interoperable infrastructure for the exchange of language resources including both data and tools. the application has been designed and is developed as part of the t4me network of excellence. we explain the underlying motivation for such a distributed repository for metadata storage and give a detailed overview on the meta-share application and its various components. this includes a discussion of the technical architecture of the system as well as a description of the component-based metadata schema format which has been developed in parallel. development of the meta-share infrastructure adopts state-of-the-art technology and follows an open-source approach allowing the general community to participate in the development process. the meta-share software package including full source code has been released to the public in march 2012. we look forward to present an up-to-date version of the meta-share software at the conference.
474,fadaee-etal-2017-learning,"   Learning Topic-Sensitive Word Representations"",
",distributed word representations are widely used for modeling words in nlp tasks. most of the existing models generate one representation per word and do not consider different meanings of a word. we present two approaches to learn multiple topic-sensitive representations per word by using hierarchical dirichlet process. we observe that by modeling topics and integrating topic distributions for each document we obtain representations that are able to distinguish between different meanings of a given word. our models yield statistically significant improvements for the lexical substitution task indicating that commonly used single word representations even when combined with contextual information are insufficient for this task.
475,dragoni-etal-2016-dranziera,"   {DRANZIERA}: An Evaluation Protocol For Multi-Domain Opinion Mining"",
",opinion mining is a topic which attracted a lot of interest in the last years. by observing the literature it is often hard to replicate system evaluation due to the unavailability of the data used for the evaluation or to the lack of details about the protocol used in the campaign. in this paper we propose an evaluation protocol called dranziera composed of a multi-domain dataset and guidelines allowing both to evaluate opinion mining systems in different contexts (closed semi-open and open) and to compare them to each other and to a number of baselines.
476,andresen-zinsmeister-2017-approximating,"   Approximating Style by N-gram-based Annotation"",
",the concept of style is much debated in theoretical as well as empirical terms. from an empirical perspective the key question is how to operationalize style and thus make it accessible for annotation and quantification. in authorship attribution many different approaches have successfully resolved this issue at the cost of linguistic interpretability: the resulting algorithms may be able to distinguish one language variety from the other but do not give us much information on their distinctive linguistic properties. we approach the issue of interpreting stylistic features by extracting linear and syntactic n-grams that are distinctive for a language variety. we present a study that exemplifies this process by a comparison of the german academic languages of linguistics and literary studies. overall our findings show that distinctive n-grams can be related to linguistic categories. the results suggest that the style of german literary studies is characterized by nominal structures and the style of linguistics by verbal ones.
477,pleva-juhar-2014-tuke,"   {TUKE}-{BN}ews-{SK}: {S}lovak Broadcast News Corpus Construction and Evaluation"",
",this article presents an overview of the existing acoustical corpuses suitable for broadcast news automatic transcription task in the slovak language. the tuke-bnews-sk database created in our department was built to support the application development for automatic broadcast news processing and spontaneous speech recognition of the slovak language. the audio corpus is composed of 479 slovak tv broadcast news shows from public slovak television called stv1 or jednotka containing 265 hours of material and 186 hours of clean transcribed speech (4 hours subset extracted for testing purposes). the recordings were manually transcribed using transcriber tool modified for slovak annotators and automatic slovak spell checking. the corpus design acquisition annotation scheme and pronunciation transcription is described together with corpus statistics and tools used. finally the evaluation procedure using automatic speech recognition is presented on the broadcast news and parliamentary speeches test sets.
478,hu-etal-2021-collaborative,"   Collaborative Data Relabeling for Robust and Diverse Voice Apps Recommendation in Intelligent Personal Assistants"",
",intelligent personal assistants (ipas) such as amazon alexa google assistant and apple siri extend their built-in capabilities by supporting voice apps developed by third-party developers. sometimes the smart assistant is not able to successfully respond to user voice commands (aka utterances). there are many reasons including automatic speech recognition (asr) error natural language understanding (nlu) error routing utterances to an irrelevant voice app or simply that the user is asking for a capability that is not supported yet. the failure to handle a voice command leads to customer frustration. in this paper we introduce a fallback skill recommendation system to suggest a voice app to a customer for an unhandled voice command. one of the prominent challenges of developing a skill recommender system for ipas is partial observation. to solve the partial observation problem we propose collaborative data relabeling (cdr) method. in addition cdr also improves the diversity of the recommended skills. we evaluate the proposed method both offline and online. the offline evaluation results show that the proposed system outperforms the baselines. the online a/b testing results show significant gain of customer experience metrics.
479,niehues-2021-continuous,"   Continuous Learning in Neural Machine Translation using Bilingual Dictionaries"",
",while recent advances in deep learning led to significant improvements in machine translation neural machine translation is often still not able to continuously adapt to the environment. for humans as well as for machine translation bilingual dictionaries are a promising knowledge source to continuously integrate new knowledge. however their exploitation poses several challenges: the system needs to be able to perform one-shot learning as well as model the morphology of source and target language. in this work we proposed an evaluation framework to assess the ability of neural machine translation to continuously learn new phrases. we integrate one-shot learning methods for neural machine translation with different word representations and show that it is important to address both in order to successfully make use of bilingual dictionaries. by addressing both challenges we are able to improve the ability to translate new rare words and phrases from 30{\%} to up to 70{\%}. the correct lemma is even generated by more than 90{\%}.
480,lusetti-etal-2018-encoder,"   Encoder-Decoder Methods for Text Normalization"",
",text normalization is the task of mapping non-canonical language typical of speech transcription and computer-mediated communication to a standardized writing. it is an up-stream task necessary to enable the subsequent direct employment of standard natural language processing tools and indispensable for languages such as swiss german with strong regional variation and no written standard. text normalization has been addressed with a variety of methods most successfully with character-level statistical machine translation (csmt). in the meantime machine translation has changed and the new methods known as neural encoder-decoder (ed) models resulted in remarkable improvements. text normalization however has not yet followed. a number of neural methods have been tried but csmt remains the state-of-the-art. in this work we normalize swiss german whatsapp messages using the ed framework. we exploit the flexibility of this framework which allows us to learn from the same training data in different ways. in particular we modify the decoding stage of a plain ed model to include target-side language models operating at different levels of granularity: characters and words. our systematic comparison shows that our approach results in an improvement over the csmt state-of-the-art.
481,shao-etal-2017-character,"   Character-based Joint Segmentation and {POS} Tagging for {C}hinese using Bidirectional {RNN}-{CRF}"",
",we present a character-based model for joint segmentation and pos tagging for chinese. the bidirectional rnn-crf architecture for general sequence tagging is adapted and applied with novel vector representations of chinese characters that capture rich contextual information and lower-than-character level features. the proposed model is extensively evaluated and compared with a state-of-the-art tagger respectively on ctb5 ctb9 and ud chinese. the experimental results indicate that our model is accurate and robust across datasets in different sizes genres and annotation schemes. we obtain state-of-the-art performance on ctb5 achieving 94.38 f1-score for joint segmentation and pos tagging.
482,nn-1978-list-questions,"   The {F}INITE {S}TRING, Volume 15, Number 1"",
",what some semantic theories can{'}t do (th r. hofmann); {nl} in information science (donald e. walker; hans karlgren; martin kay); {cal} in science education; new journal: annuals of the history of computing (bernard a. caller); {n}ew {e}ngland research application center; linguafranca: document search ({llba}); demonstration: interactive search of {llba}; {nfais}/{unesco} indexing education kit; symposium: computer assisted learning (j. j. mathews); 1978 linguistic institute: conferences and symposia; data bases: usability and responsiveness (dr. allen baiter); conferences: internal auditing (d. eugene shaeffer); conferences: briefly noted (k. preston jr.); {nsf} awards in computer science for 1976; {ajcl}: a description; {ajcl}: page format; {ajcl}: opaque card format; {afips} {w}ashington newsletter
483,du-cardie-2020-event,"   Event Extraction by Answering (Almost) Natural Questions"",
",the problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step causing the well-known problem of error propagation. to avoid this issue we introduce a new paradigm for event extraction by formulating it as a question answering (qa) task that extracts the event arguments in an end-to-end manner. empirical results demonstrate that our framework outperforms prior methods substantially; in addition it is capable of extracting event arguments for roles not seen at training time (i.e. in a zero-shot learning setting).
484,wang-etal-2019-self,"   Self-Attention with Structural Position Representations"",
",although self-attention networks (sans) have advanced the state-of-the-art on various nlp tasks one criticism of sans is their ability of encoding positions of input words (shaw et al. 2018). in this work we propose to augment sans with structural position representations to model the latent structure of the input sentence which is complementary to the standard sequential positional representations. specifically we use dependency tree to represent the grammatical structure of a sentence and propose two strategies to encode the positional relationships among words in the dependency tree. experimental results on nist chinese-to-english and wmt14 english-to-german translation tasks show that the proposed approach consistently boosts performance over both the absolute and relative sequential position representations.
485,hartmann-sogaard-2018-limitations,"   Limitations of Cross-Lingual Learning from Image Search"",
",cross-lingual representation learning is an important step in making nlp scale to all the world{'}s languages. previous work on bilingual lexicon induction suggests that it is possible to learn cross-lingual representations of words based on similarities between images associated with these words. however that work focused (almost exclusively) on the translation of nouns only. here we investigate whether the meaning of other parts-of-speech (pos) in particular adjectives and verbs can be learned in the same way. our experiments across five language pairs indicate that previous work does not scale to the problem of learning cross-lingual representations beyond simple nouns.
486,lala-etal-2019-grounded,"   Grounded Word Sense Translation"",
",recent work on visually grounded language learning has focused on broader applications of grounded representations such as visual question answering and multimodal machine translation. in this paper we consider grounded word sense translation i.e. the task of correctly translating an ambiguous source word given the corresponding textual and visual context. our main objective is to investigate the extent to which images help improve word-level (lexical) translation quality. we do so by first studying the dataset for this task to understand the scope and challenges of the task. we then explore different data settings image features and ways of grounding to investigate the gain from using images in each of the combinations. we find that grounding on the image is specially beneficial in weaker unidirectional recurrent translation models. we observe that adding structured image information leads to stronger gains in lexical translation accuracy.
487,groves-etal-2018-treat,"   Treat the system like a human student: Automatic naturalness evaluation of generated text without reference texts"",
",the current most popular method for automatic natural language generation (nlg) evaluation is comparing generated text with human-written reference sentences using a metrics system which has drawbacks around reliability and scalability. we draw inspiration from second language (l2) assessment and extract a set of linguistic features to predict human judgments of sentence naturalness. our experiment using a small dataset showed that the feature-based approach yields promising results with the added potential of providing interpretability into the source of the problems.
488,laaridh-mauclair-2020-sur,"   Sur l{'}utilisation de la reconnaissance automatique de la parole pour l{'}aide au diagnostic diff{\'e}rentiel entre la maladie de {P}arkinson et l{'}{AMS} (On using automatic speech recognition for the differential diagnosis of {P}arkinson{'}s Disease and {MSA} This article presents a study regarding the contribution of automatic speech processing in the differential diagnosis between {P}arkinson{'}s disease and {MSA} (Multi-System Atrophies))"",
",cet article pr{\'e}sente une {\'e}tude concernant l{'}apport du traitement automatique de la parole dans le cadre du diagnostic diff{\'e}rentiel entre la maladie de parkinson et l{'}ams (atrophie multi-syst{\'e}matis{\'e}e). nous proposons des outils de reconnaissance automatique de la parole pour {\'e}valuer le potentiel d{'}indicateurs de la parole dysarthrique caract{\'e}risant ces deux pathologies. dans ce cadre un corpus de parole pathologique (projet anr voice4pd-msa) a {\'e}t{\'e} enregistr{\'e} au sein des centres hospitaliers universitaires (chu) de toulouse et bordeaux. les locuteurs sont des patients atteints de stades pr{\'e}coces de la maladie de parkinson et d{'}ams ainsi que des locuteurs t{\'e}moins. des mesures automatiques caract{\'e}risant la qualit{\'e} de la reconnaissance automatique de la parole ainsi que la prosodie des patients ont montr{\'e} un int{\'e}r{\^e}t pour la caract{\'e}risation des pathologies {\'e}tudi{\'e}es et peuvent {\^e}tre consid{\'e}r{\'e}es comme un outil potentiel pour l{'}aide {\`a} leur diagnostic diff{\'e}rentiel.
489,daxenberger-etal-2017-essence,"   What is the Essence of a Claim? Cross-Domain Claim Identification"",
",argument mining has become a popular research area in nlp. it typically includes the identification of argumentative components e.g. claims as the central component of an argument. we perform a qualitative analysis across six different datasets and show that these appear to conceptualize claims quite differently. to learn about the consequences of such different conceptualizations of claim for practical applications we carried out extensive experiments using state-of-the-art feature-rich and deep learning systems to identify claims in a cross-domain fashion. while the divergent conceptualization of claims in different datasets is indeed harmful to cross-domain classification we show that there are shared properties on the lexical level as well as system configurations that can help to overcome these gaps.
490,huang-etal-2021-document,"   Document-level Entity-based Extraction as Template Generation"",
",document-level entity-based extraction (ee) aiming at extracting entity-centric information such as entity roles and entity relations is key to automatic knowledge acquisition from text corpora for various domains. most document-level ee systems build extractive models which struggle to model long-term dependencies among entities at the document level. to address this issue we propose a generative framework for two document-level ee tasks: role-filler entity extraction (ree) and relation extraction (re). we first formulate them as a template generation problem allowing models to efficiently capture cross-entity dependencies exploit label semantics and avoid the exponential computation complexity of identifying n-ary relations. a novel cross-attention guided copy mechanism topk copy is incorporated into a pre-trained sequence-to-sequence model to enhance the capabilities of identifying key information in the input document. experiments done on the muc-4 and scirex dataset show new state-of-the-art results on ree (+3.26{\%}) binary re (+4.8{\%}) and 4-ary re (+2.7{\%}) in f1 score.
491,cecchini-etal-2018-challenges,"   Challenges in Converting the Index {T}homisticus Treebank into {U}niversal {D}ependencies"",
",this paper describes the changes applied to the original process used to convert the \textit{index thomisticus} treebank a corpus including texts in medieval latin by thomas aquinas into the annotation style of universal dependencies. the changes are made both to harmonise the universal dependencies version of the \textit{index thomisticus} treebank with the two other available latin treebanks and to fix errors and inconsistencies resulting from the original process. the paper details the treatment of different issues in pos tagging lemmatisation and assignment of dependency relations. finally it assesses the quality of the new conversion process by providing an evaluation against a gold standard.
492,dei-etal-2016-la,"   La reconnaissance des mots dans la parole accentu{\'e}e : Une {\'e}tude en laboratoire et {\`a} l{'}ext{\'e}rieur. (Mispronunciations slow down word recognition: A study using touchscreens in the lab and the real world)"",
",des travaux r{\'e}cents sugg{\`e}rent que les enfants et les adultes sont initialement ralentis dans leur compr{\'e}hension des mots qui n{'}ont pas {\'e}t{\'e} prononc{\'e}s de fa{\c{c}}on standard. n{\'e}anmoins quand ils font face {\`a} un interlocuteur qui {\`a} un discours accentu{\'e} ils d{\'e}veloppent rapidement des strat{\'e}gies sp{\'e}cifiques qui leur permettent de comprendre m{\^e}me des prononciations atypiques. cependant ces r{\'e}sultats sont typiquement issus de recherches en laboratoire o{\`u} l{'}attention des participants se concentre sur une t{\^a}che unique qui leur demande peu de ressources. afin de d{\'e}passer ces limitations nous avons men{\'e} une exp{\'e}rience de reconnaissance de mots sur tablette tactile en {\'e}valuant des enfants et des adultes en laboratoire et dans l{'}environnement naturel de chaque groupe. nous avons constat{\'e} que des d{\'e}viations de prononciation dans la parole accentu{\'e}e ralentissent la reconnaissance des mots chez des enfants et adultes tant dans le laboratoire que dans des environnements naturels.
493,hayashi-etal-1999-scalable,"   A scalable cross-language metasearch architecture for multilingual information access on the Web"",
",this position paper for the special session on ``multilingual information access'' comprises of three parts. the first part reviews possible demands for multilingual information access (hereafter mlia) on the web and examines required technical elements. among those we in the second part focus on cross-language information retrieval (hereafter clir) particularly a scalable architecture which enables clir in a number of language combinations. such a distributed architecture developed around xirch project (an international joint experimental project currently involves ntt krdl and kaist) is then described in a certain detail. the final part discusses some nlp/mt related issues associated with such a clir architecture.
494,mesa-murgado-etal-2021-identifying,"   Identifying professions {\&} occupations in Health-related Social Media using Natural Language Processing"",
",this paper describes the entry of the research group sinai at smm4h{'}s profner task on the identification of professions and occupations in social media related with health. specifically we have participated in task 7a: tweet binary classification to determine whether a tweet contains mentions of occupations or not as well as in task 7b: ner offset detection and classification aimed at predicting occupations mentions and classify them discriminating by professions and working statuses.
495,desot-etal-2020-corpus,"   Corpus Generation for Voice Command in Smart Home and the Effect of Speech Synthesis on End-to-End {SLU}"",
",massive amounts of annotated data greatly contributed to the advance of the machine learning field. however such large data sets are often unavailable for novel tasks performed in realistic environments such as smart homes. in this domain semantically annotated large voice command corpora for spoken language understanding (slu) are scarce especially for non-english languages. we present the automatic generation process of a synthetic semantically-annotated corpus of french commands for smart-home to train pipeline and end-to-end (e2e) slu models. slu is typically performed through automatic speech recognition (asr) and natural language understanding (nlu) in a pipeline. since errors at the asr stage reduce the nlu performance an alternative approach is end-to-end (e2e) slu to jointly perform asr and nlu. to that end the artificial corpus was fed to a text-to-speech (tts) system to generate synthetic speech data. all models were evaluated on voice commands acquired in a real smart home. we show that artificial data can be combined with real data within the same training set or used as a stand-alone training corpus. the synthetic speech quality was assessedby comparing it to real data using dynamic time warping (dtw).
496,peng-etal-2021-highly,"   Highly Efficient Knowledge Graph Embedding Learning with {O}rthogonal {P}rocrustes {A}nalysis"",
",knowledge graph embeddings (kges) have been intensively explored in recent years due to their promise for a wide range of applications. however existing studies focus on improving the final model performance without acknowledging the computational cost of the proposed approaches in terms of execution time and environmental impact. this paper proposes a simple yet effective kge framework which can reduce the training time and carbon footprint by orders of magnitudes compared with state-of-the-art approaches while producing competitive performance. we highlight three technical innovations: full batch learning via relational matrices closed-form orthogonal procrustes analysis for kges and non-negative-sampling training. in addition as the first kge method whose entity embeddings also store full relation information our trained models encode rich semantics and are highly interpretable. comprehensive experiments and ablation studies involving 13 strong baselines and two standard datasets verify the effectiveness and efficiency of our algorithm.
497,edman-etal-2020-data,"   Data Selection for Unsupervised Translation of {G}erman{--}{U}pper {S}orbian"",
",this paper describes the methods behind the systems submitted by the university of groningen for the wmt 2020 unsupervised machine translation task for german{--}upper sorbian. we investigate the usefulness of data selection in the unsupervised setting. we find that we can perform data selection using a pretrained model and show that the quality of a set of sentences or documents can have a great impact on the performance of the unmt system trained on it. furthermore we show that document-level data selection should be preferred for training the xlm model when possible. finally we show that there is a trade-off between quality and quantity of the data used to train unmt systems.
498,mou-etal-2021-complementary,"   Complementary Evidence Identification in Open-Domain Question Answering"",
",this paper proposes a new problem of complementary evidence identification for open-domain question answering (qa). the problem aims to efficiently find a small set of passages that covers full evidence from multiple aspects as to answer a complex question. to this end we proposes a method that learns vector representations of passages and models the sufficiency and diversity within the selected set in addition to the relevance between the question and passages. our experiments demonstrate that our method considers the dependence within the supporting evidence and significantly improves the accuracy of complementary evidence selection in qa domain.
499,kreutzer-etal-2018-reliability,"   Reliability and Learnability of Human Bandit Feedback for Sequence-to-Sequence Reinforcement Learning"",
",we present a study on reinforcement learning (rl) from human bandit feedback for sequence-to-sequence learning exemplified by the task of bandit neural machine translation (nmt). we investigate the reliability of human bandit feedback and analyze the influence of reliability on the learnability of a reward estimator and the effect of the quality of reward estimates on the overall rl task. our analysis of cardinal (5-point ratings) and ordinal (pairwise preferences) feedback shows that their intra- and inter-annotator α-agreement is comparable. best reliability is obtained for standardized cardinal feedback and cardinal feedback is also easiest to learn and generalize from. finally improvements of over 1 bleu can be obtained by integrating a regression-based reward estimator trained on cardinal feedback for 800 translations into rl for nmt. this shows that rl is possible even from small amounts of fairly reliable human feedback pointing to a great potential for applications at larger scale.
500,manchanda-grunin-2020-domain,"   Domain Informed Neural Machine Translation: Developing Translation Services for Healthcare Enterprise"",
",neural machine translation (nmt) is a deep learning based approach that has achieved outstanding results lately in the translation community. the performance of nmt systems however is dependent on the availability of large amounts of in-domain parallel corpora. the business enterprises in domains such as legal and healthcare require specialized vocabulary but translation systems trained for a general purpose do not cater to these needs. the data in these domains is either hard to acquire or is very small in comparison to public data sets. this is a detailed report of using an open-source library to implement a machine translation system and successfully customizing it for the needs of a particular client in the healthcare domain. this report details the chronological development of every component of this system namely extraction of data from in-domain healthcare documents a pre-processing pipeline for the data data alignment and augmentation training and a fully automated and robust deployment pipeline. this work proposes an efficient way for the continuous deployment of newly trained deep learning models. the deployed translation models are optimized for both inference time and cost.
501,pamungkas-etal-2020-really,"   Do You Really Want to Hurt Me? Predicting Abusive Swearing in Social Media"",
",swearing plays an ubiquitous role in everyday conversations among humans both in oral and textual communication and occurs frequently in social media texts typically featured by informal language and spontaneous writing. such occurrences can be linked to an abusive context when they contribute to the expression of hatred and to the abusive effect causing harm and offense. however swearing is multifaceted and is often used in casual contexts also with positive social functions. in this study we explore the phenomenon of swearing in twitter conversations taking the possibility of predicting the abusiveness of a swear word in a tweet context as the main investigation perspective. we developed the twitter english corpus swad (swear words abusiveness dataset) where abusive swearing is manually annotated at the word level. our collection consists of 1511 unique swear words from 1320 tweets. we developed models to automatically predict abusive swearing to provide an intrinsic evaluation of swad and confirm the robustness of the resource. we also present the results of a glass box ablation study in order to investigate which lexical syntactic and affective features are more informative towards the automatic prediction of the function of swearing.
502,goyal-etal-2020-automatic,"   Automatic Translation of Complex {E}nglish Sentences to {I}ndian {S}ign {L}anguage Synthetic Video Animations"",
",sign language is the natural way of expressing thoughts and feelings for the deaf community. sign language is a diagrammatic and non-verbal language used by the impaired community to communicate their feeling to their lookalike one. today we live in the era of technological development owing to which instant communication is quite easy but even then a lot of work needs to be done in the field of sign language automation to improve the quality of life among the deaf community. the traditional approaches used for representing the signs are in the form of videos or text that are expensive time-consuming and are not easy to use. in this research work an attempt is made for the conversion of complex and compound english sentences to indian sign language (isl) using synthetic video animations. the translation architecture includes a parsing module that parses the input complex or compound english sentences to their simplified versions by using complex to simple and compound to simple english grammar rules respectively. the simplified sentence is then forwarded to the conversion segment that rearranges the words of the english language into its corresponding isl using the devised grammar rules. the next segment constitutes the removal of unwanted words or stop words. this segment gets an input sentence generated by isl grammar rules. unwanted or unnecessary words are eliminated by this segment. this removal is important because isl needs only a meaningful sentence rather than unnecessary usage of linking verbs helping verbs and so on. after parsing through the eliminator segment the sentence is sent to the concordance segment. this segment checks each word in the sentence and translates them into their respective lemma. lemma is the basic requiring node of each word because sign language makes use of basic words irrespective of other languages that make use of gerund suffixes three forms of verbs different kinds of nouns adjectives pronouns in their sentence theory. all the words of the sentence are checked in the lexicon which contains the english word with its hamnosys notation and the words that are not in the lexicon are replaced by their synonym. the words of the sentence are replaced by their counter hamnosys code. in case the word is not present in the lexicon the hamnosys code will be taken for each alphabet of the word in sequence. the hamnosys code is converted into the sigml tags (a form of xml tags) and these sigml tags are then sent to the animation module which converts the sigml code into the synthetic animation using avatar (computer-generated animation character).
503,soldner-etal-2019-box,"   Box of Lies: Multimodal Deception Detection in Dialogues"",
",deception often takes place during everyday conversations yet conversational dialogues remain largely unexplored by current work on automatic deception detection. in this paper we address the task of detecting multimodal deceptive cues during conversational dialogues. we introduce a multimodal dataset containing deceptive conversations between participants playing the box of lies game from the tonight show starring jimmy fallon in which they try to guess whether an object description provided by their opponent is deceptive or not. we conduct annotations of multimodal communication behaviors including facial and linguistic behaviors and derive several learning features based on these annotations. initial classification experiments show promising results performing well above both a random and a human baseline and reaching up to 69{\%} accuracy in distinguishing deceptive and truthful behaviors.
504,shen-etal-2017-empirical,"   An Empirical Analysis of Multiple-Turn Reasoning Strategies in Reading Comprehension Tasks"",
",reading comprehension (rc) is a challenging task that requires synthesis of information across sentences and multiple turns of reasoning. using a state-of-the-art rc model we empirically investigate the performance of single-turn and multiple-turn reasoning on the squad and ms marco datasets. the rc model is an end-to-end neural network with iterative attention and uses reinforcement learning to dynamically control the number of turns. we find that multiple-turn reasoning outperforms single-turn reasoning for all question and answer types; further we observe that enabling a flexible number of turns generally improves upon a fixed multiple-turn strategy. {\%}across all question types and is particularly beneficial to questions with lengthy descriptive answers. we achieve results competitive to the state-of-the-art on these two datasets.
505,hayakawa-etal-2016-ilmt,"   The {ILMT}-s2s Corpus ― A Multimodal Interlingual Map Task Corpus"",
",this paper presents the multimodal interlingual map task corpus (ilmt-s2s corpus) collected at trinity college dublin and discuss some of the issues related to the collection and analysis of the data. the corpus design is inspired by the hcrc map task corpus which was initially designed to support the investigation of linguistic phenomena and has been the focus of a variety of studies of communicative behaviour. the simplicity of the task and the complexity of phenomena it can elicit make the map task an ideal object of study. although there are studies that used replications of the map task to investigate communication in computer mediated tasks this ilmt-s2s corpus is to the best of our knowledge the first investigation of communicative behaviour in the presence of three additional {``}filters{''}: automatic speech recognition (asr) machine translation (mt) and text to speech (tts) synthesis where the instruction giver and the instruction follower speak different languages. this paper details the data collection setup and completed annotation of the ilmt-s2s corpus and outlines preliminary results obtained from the data.
506,zhong-etal-2017-time,"   Time Expression Analysis and Recognition Using Syntactic Token Types and General Heuristic Rules"",
",extracting time expressions from free text is a fundamental task for many applications. we analyze the time expressions from four datasets and find that only a small group of words are used to express time information and the words in time expressions demonstrate similar syntactic behaviour. based on the findings we propose a type-based approach named syntime to recognize time expressions. specifically we define three main syntactic token types namely time token modifier and numeral to group time-related regular expressions over tokens. on the types we design general heuristic rules to recognize time expressions. in recognition syntime first identifies the time tokens from raw text then searches their surroundings for modifiers and numerals to form time segments and finally merges the time segments to time expressions. as a light-weight rule-based tagger syntime runs in real time and can be easily expanded by simply adding keywords for the text of different types and of different domains. experiment on benchmark datasets and tweets data shows that syntime outperforms state-of-the-art methods.
507,ramisch-etal-2016-deque,"   {D}e{Q}ue: A Lexicon of Complex Prepositions and Conjunctions in {F}rench"",
",we introduce deque a lexicon covering french complex prepositions (cpre) like {``}{\`a} partir de{''} (from) and complex conjunctions (cconj) like {``}bien que{''} (although). the lexicon includes fine-grained linguistic description based on empirical evidence. we describe the general characteristics of cpre and cconj in french with special focus on syntactic ambiguity. then we list the selection criteria used to build the lexicon and the corpus-based methodology employed to collect entries. finally we quantify the ambiguity of each construction by annotating around 100 sentences randomly taken from the frwac. in addition to its theoretical value the resource has many potential practical applications. we intend to employ deque for treebank annotation and to train a dependency parser that can takes complex constructions into account.
508,marcheggiani-titov-2020-graph,"   Graph Convolutions over Constituent Trees for Syntax-Aware Semantic Role Labeling"",
",semantic role labeling (srl) is the task of identifying predicates and labeling argument spans with semantic roles. even though most semantic-role formalisms are built upon constituent syntax and only syntactic constituents can be labeled as arguments (e.g. framenet and propbank) all the recent work on syntax-aware srl relies on dependency representations of syntax. in contrast we show how graph convolutional networks (gcns) can be used to encode constituent structures and inform an srl system. nodes in our spangcn correspond to constituents. the computation is done in 3 stages. first initial node representations are produced by {`}composing{'} word representations of the first and last words in the constituent. second graph convolutions relying on the constituent tree are performed yielding syntactically-informed constituent representations. finally the constituent representations are {`}decomposed{'} back into word representations which are used as input to the srl classifier. we evaluate spangcn against alternatives including a model using gcns over dependency trees and show its effectiveness on standard english srl benchmarks conll-2005 conll-2012 and framenet.
509,zhang-etal-2018-global,"   Global Attention for Name Tagging"",
",many name tagging approaches use local contextual information with much success but can fail when the local context is ambiguous or limited. we present a new framework to improve name tagging by utilizing local document-level and corpus-level contextual information. for each word we retrieve document-level context from other sentences within the same document and corpus-level context from sentences in other documents. we propose a model that learns to incorporate document-level and corpus-level contextual information alongside local contextual information via document-level and corpus-level attentions which dynamically weight their respective contextual information and determines the influence of this information through gating mechanisms. experiments on benchmark datasets show the effectiveness of our approach which achieves state-of-the-art results for dutch german and spanish on the conll-2002 and conll-2003 datasets. we will make our code and pre-trained models publicly available for research purposes.
510,walentynowicz-etal-2019-tagger,"   Tagger for {P}olish Computer Mediated Communication Texts"",
",in this paper we present a morpho-syntactic tagger dedicated to computer-mediated communication texts in polish. its construction is based on an expanded rnn-based neural network adapted to the work on noisy texts. among several techniques the tagger utilises fasttext embedding vectors sequential character embedding vectors and brown clustering for the coarse-grained representation of sentence structures. in addition a set of manually written rules was proposed for post-processing. the system was trained to disambiguate descriptions of words in relation to parts of speech tags together with the full morphological information in terms of values for the different grammatical categories. we present also evaluation of several model variants on the gold standard annotated cmc data comparison to the state-of-the-art taggers for polish and error analysis. the proposed tagger shows significantly better results in this domain and demonstrates the viability of adaptation.
511,xia-xia-2019-design,"   The Design and Construction of the Corpus of {C}hina {E}nglish"",
",the paper describes the development a corpus of an english variety i.e. china english in or-der to provide a linguistic resource for researchers in the field of china english. the corpus of china english (cce) was built with due consideration given to its representativeness and authenticity. it was composed of more than 13962102 tokens in 15333 texts evenly divided between the following four genres: newspapers magazines fiction and academic writings. the texts cover a wide range of domains such as news financial politics environment social culture technology sports education philosophy literary etc. it is a helpful resource for research on china english computational linguistics natural language processing corpus linguistics and english language education.
512,sanders-etal-2014-dutch,"   The {D}utch {LESLLA} Corpus"",
",this paper describes the dutch leslla data and its curation. leslla stands for low-educated second language and literacy acquisition. the data was collected for research in this field and would have been disappeared if it were not saved. within the clarin project data curation service the data was made into a spoken language resource and made available to other researchers.
513,aminian-etal-2017-transferring,"   Transferring Semantic Roles Using Translation and Syntactic Information"",
",our paper addresses the problem of annotation projection for semantic role labeling for resource-poor languages using supervised annotations from a resource-rich language through parallel data. we propose a transfer method that employs information from source and target syntactic dependencies as well as word alignment density to improve the quality of an iterative bootstrapping method. our experiments yield a 3.5 absolute labeled f-score improvement over a standard annotation projection method.
514,bhat-etal-2020-towards,"   Towards Modeling Revision Requirements in wiki{H}ow Instructions"",
",wikihow is a resource of how-to guidesthat describe the steps necessary to accomplish a goal. guides in this resource are regularly edited by a community of users who try to improve instructions in terms of style clarity and correctness. in this work we test whether the need for such edits can be predicted automatically. for this task we extend an existing resource of textual edits with a complementary set of approx. 4 million sentences that remain unedited over time and report on the outcome of two revision modeling experiments.
515,slaughter-etal-2018-enchancing,"   Enchancing the Collaborative Interlingual Index for Digital Humanities: Cross-linguistic Analysis in the Domain of Theology"",
",we aim to support digital humanities work related to the study of sacred texts. to do this we propose to build a cross-lingual wordnet within the do-main of theology. we target the collaborative interlingual index (cili) directly instead of each individual wordnet. the paper presents background for this proposal: (1) an overview of concepts relevant to theology and (2) a summary of the domain-associated issues observed in the princeton wordnet (pwn). we have found that definitions for concepts in this domain can be too restrictive inconsistent and unclear. necessary synsets are missing with the pwn being skewed towards christianity. we argue that tackling problems in a single domain is a better method for improving cili. by focusing on a single topic rather than a single language this will result in the proper construction of definitions romanization/translation of lemmas and also improvements in use of/creation of a cross-lingual domain hierarchy.
516,bach-etal-2007-cmu,"   The {CMU} {T}rans{T}ac 2007 eyes-free two-way speech-to-speech translation system"",
",the paper describes our portable two-way speech-to-speech translation system using a completely eyes-free/hands-free user interface. this system translates between the language pair english and iraqi arabic as well as between english and farsi and was built within the framework of the darpa transtac program. the farsi language support was developed within a 90-day period testing our ability to rapidly support new languages. the paper gives an overview of the system{'}s components along with the individual component objective measures and a discussion of issues relevant for the overall usage of the system. we found that usability flexibility and robustness serve as severe constraints on system architecture and design.
517,angioni-etal-2018-sardanet,"   {S}arda{N}et: a Linguistic Resource for {S}ardinian Language"",
",this paper describes the process of building sardanet a linguistic resource for sardinian language including the different linguistic varieties in sardinia. sardanet aims at identifying the semantic relations between sardinian terms by manually mapping existing wordnet entries to sardinian word senses. the work still in progress has been developed in collaboration with the university of cagliari. after discussing some linguistic peculiarities the paper presents the basic steps of the construction process the method and the tools involved the issues encountered during the development and the current version of sardanet.
518,gavalda-2000-soup,"   {SOUP}: A Parser for Real-world Spontaneous Speech"",
",this paper describes the key features of soup a stochastic chart-based top-down parser especially engineered for real-time analysis of spoken language with very large multi-domain semantic grammars. soup achieves flexibility by encoding context-free grammars specified for example in the java speech grammar format as probabilistic recursive transition networks and robustness by allowing skipping of input words at any position and producing ranked interpretations that may consist of multiple parse trees. moreover soup is very efficient which allows for practically instantaneous backend response.
519,ko-etal-2021-adapting,"   Adapting High-resource {NMT} Models to Translate Low-resource Related Languages without Parallel Data"",
",the scarcity of parallel data is a major obstacle for training high-quality machine translation systems for low-resource languages. fortunately some low-resource languages are linguistically related or similar to high-resource languages; these related languages may share many lexical or syntactic structures. in this work we exploit this linguistic overlap to facilitate translating to and from a low-resource language with only monolingual data in addition to any parallel data in the related high-resource language. our method nmt-adapt combines denoising autoencoding back-translation and adversarial objectives to utilize monolingual data for low-resource adaptation. we experiment on 7 languages from three different language families and show that our technique significantly improves translation into low-resource language compared to other translation baselines.
520,eckart-de-castilho-etal-2017-representation,"   Representation and Interchange of Linguistic Annotation. An In-Depth, Side-by-Side Comparison of Three Designs"",
",for decades most self-respecting linguistic engineering initiatives have designed and implemented custom representations for various layers of for example morphological syntactic and semantic analysis. despite occasional efforts at harmonization or even standardization our field today is blessed with a multitude of ways of encoding and exchanging linguistic annotations of these types both at the levels of {`}abstract syntax{'} naming choices and of course file formats. to a large degree it is possible to work within and across design plurality by conversion and often there may be good reasons for divergent design reflecting differences in use. however it is likely that some abstract commonalities across choices of representation are obscured by more superficial differences and conversely there is no obvious procedure to tease apart what actually constitute contentful vs. mere technical divergences. in this study we seek to conceptually align three representations for common types of morpho-syntactic analysis pinpoint what in our view constitute contentful differences and reflect on the underlying principles and specific requirements that led to individual choices. we expect that a more in-depth understanding of these choices across designs may led to increased harmonization or at least to more informed design of future representations.
521,das-etal-2017-web,"   Web-Scale Language-Independent Cataloging of Noisy Product Listings for {E}-Commerce"",
",the cataloging of product listings through taxonomy categorization is a fundamental problem for any e-commerce marketplace with applications ranging from personalized search recommendations to query understanding. however manual and rule based approaches to categorization are not scalable. in this paper we compare several classifiers for categorizing listings in both english and japanese product catalogs. we show empirically that a combination of words from product titles navigational breadcrumbs and list prices when available improves results significantly. we outline a novel method using correspondence topic models and a lightweight manual process to reduce noise from mis-labeled data in the training set. we contrast linear models gradient boosted trees (gbts) and convolutional neural networks (cnns) and show that gbts and cnns yield the highest gains in error reduction. finally we show gbts applied in a language-agnostic way on a large-scale japanese e-commerce dataset have improved taxonomy categorization performance over current state-of-the-art based on deep belief network models.
522,guo-etal-2018-multi,"   Multi-Source Domain Adaptation with Mixture of Experts"",
",we propose a mixture-of-experts approach for unsupervised domain adaptation from multiple sources. the key idea is to explicitly capture the relationship between a target example and different source domains. this relationship expressed by a point-to-set metric determines how to combine predictors trained on various domains. the metric is learned in an unsupervised fashion using meta-training. experimental results on sentiment analysis and part-of-speech tagging demonstrate that our approach consistently outperforms multiple baselines and can robustly handle negative transfer.
523,indurkhya-etal-2021-evaluating,"   Evaluating {U}niversal {D}ependency Parser Recovery of Predicate Argument Structure via {C}omp{C}hain Analysis"",
",accurate recovery of predicate-argument structure from a universal dependency (ud) parse is central to downstream tasks such as extraction of semantic roles or event representations. this study introduces compchains a categorization of the hierarchy of predicate dependency relations present within a ud parse. accuracy of compchain classification serves as a proxy for measuring accurate recovery of predicate-argument structure from sentences with embedding. we analyzed the distribution of compchains in three ud english treebanks ewt gum and lines revealing that these treebanks are sparse with respect to sentences with predicate-argument structure that includes predicate-argument embedding. we evaluated the conll 2018 shared task udpipe (v1.2) baseline (dependency parsing) models as compchain classifiers for the ewt gums and lines ud treebanks. our results indicate that these three baseline models exhibit poorer performance on sentences with predicate-argument structure with more than one level of embedding; we used compchains to characterize the errors made by these parsers and present examples of erroneous parses produced by the parser that were identified using compchains. we also analyzed the distribution of compchains in 58 non-english ud treebanks and then used compchains to evaluate the conll{'}18 shared task baseline model for each of these treebanks. our analysis shows that performance with respect to compchain classification is only weakly correlated with the official evaluation metrics (las mlas and blex). we identify gaps in the distribution of compchains in several of the ud treebanks thus providing a roadmap for how these treebanks may be supplemented. we conclude by discussing how compchains provide a new perspective on the sparsity of training data for ud parsers as well as the accuracy of the resulting ud parses.
524,pannitto-etal-2021-teaching,"   Pretraining the Noisy Channel Model for Task-Oriented Dialogue"",
",abstract direct decoding for task-oriented dialogue is known to suffer from the explaining-away effect manifested in models that prefer short and generic responses. here we argue for the use of bayes{'} theorem to factorize the dialogue task into two models the distribution of the context given the response and the prior for the response itself. this approach an instantiation of the noisy channel model both mitigates the explaining-away effect and allows the principled incorporation of large pretrained models for the response prior. we present extensive experiments showing that a noisy channel model decodes better responses compared to direct decoding and that a two-stage pretraining strategy employing both open-domain and task-oriented dialogue data improves over randomly initialized models.
525,maegaard-etal-2008-medar,"   {MEDAR}: Collaboration between {E}uropean and Mediterranean {A}rabic Partners to Support the Development of Language Technology for {A}rabic"",
",after the successful completion of the nemlar project 2003-2005 a new opportunity for a project was opened by the european commission and a group of largely the same partners is now executing the medar project. medar will be updating the surveys and blark for arabic already made and will then focus on machine translation (and other tools for translation) and information retrieval with a focus on language resources tools and evaluation for these applications. a very important part of the medar project is to reinforce and extend the nemlar network and to create a cooperation roadmap for human language technologies for arabic. it is expected that the cooperation roadmap will attract wide attention from other parties and that it can help create a larger platform for collaborative projects. finally the project will focus on dissemination of knowledge about existing resources and tools as well as actors and activities; this will happen through newsletter website and an international conference which will follow up on the cairo conference of 2004. dissemination to user communities will also be important e.g. through participation in translators? conferences. the goal of these activities is to create a stronger and lasting collaboration between eu countries and arabic speaking countries.
526,correia-martins-2019-simple,"   A Simple and Effective Approach to Automatic Post-Editing with Transfer Learning"",
",automatic post-editing (ape) seeks to automatically refine the output of a black-box machine translation (mt) system through human post-edits. ape systems are usually trained by complementing human post-edited data with large artificial data generated through back-translations a time-consuming process often no easier than training a mt system from scratch. in this paper we propose an alternative where we fine-tune pre-trained bert models on both the encoder and decoder of an ape system exploring several parameter sharing strategies. by only training on a dataset of 23k sentences for 3 hours on a single gpu we obtain results that are competitive with systems that were trained on 5m artificial sentences. when we add this artificial data our method obtains state-of-the-art results.
527,lendvai-hunt-2008-field,"   From Field Notes towards a Knowledge Base"",
",we describe the process of converting plain text cultural heritage data to elements of a domain-specific knowledge base using general machine learning techniques. first digitised expedition field notes are segmented and labelled automatically. in order to obtain perfect records we create an annotation tool that features selective sampling allowing domain experts to validate automatically labelled text which is then stored in a database. next the records are enriched with semi-automatically derived secondary metadata. metadata enable fine-grained querying the results of which are additionally visualised using maps and photos.
528,gendrot-etal-2020-informations,"   Informations segmentales pour la caract{\'e}risation phon{\'e}tique du locuteur : variabilit{\'e} inter- et intra-locuteurs (An automatic classification task involving 44 speakers was performed using convolutional neural networks ({CNN}) on broadband spectrograms extracted from 2-second sequences of a spontaneous speech corpus ({NCCF}r))"",
",nous avons effectu{\'e} une classification automatique de 44 locuteurs {\`a} partir de r{\'e}seaux de neurones convolutifs (cnn) sur la base de spectrogrammes {\`a} bandes larges calcul{\'e}s sur des s{\'e}quences de 2 secondes extraites d{'}un corpus de parole spontan{\'e}e (nccfr). apr{\`e}s obtention d{'}un taux de classification moyen de 937 {\%} les diff{\'e}rentes classes phon{\'e}miques composant chaque s{\'e}quence ont {\'e}t{\'e} masqu{\'e}es afin de tester leur impact sur le mod{\`e}le. les r{\'e}sultats montrent que les voyelles orales influent avant toute autre classe sur le taux de classification suivies ensuite par les occlusives orales. ces r{\'e}sultats sont expliqu{\'e}s principalement par la repr{\'e}sentation temporelle pr{\'e}dominante des voyelles orales. une variabilit{\'e} inter-locuteurs se manifeste par l{'}existence de locuteurs attracteurs qui attirent un grand nombre de faux positifs et qui ne sont pas sensibles au masquage effectu{\'e}. nous mettons en avant dans la discussion des r{\'e}alisations acoustiques qui pourraient expliquer les sp{\'e}cificit{\'e}s de ces locuteurs.
529,miletic-etal-2021-detecting,"   Detecting Contact-Induced Semantic Shifts: {W}hat Can Embedding-Based Methods Do in Practice?"",
",this study investigates the applicability of semantic change detection methods in descriptively oriented linguistic research. it specifically focuses on contact-induced semantic shifts in quebec english. we contrast synchronic data from different regions in order to identify the meanings that are specific to quebec and potentially related to language contact. type-level embeddings are used to detect new semantic shifts and token-level embeddings to isolate regionally specific occurrences. we introduce a new 80-item test set and conduct both quantitative and qualitative evaluations. we demonstrate that diachronic word embedding methods can be applied to contact-induced semantic shifts observed in synchrony obtaining results comparable to the state of the art on similar tasks in diachrony. however we show that encouraging evaluation results do not translate to practical value in detecting new semantic shifts. finally our application of token-level embeddings accelerates manual data exploration and provides an efficient way of scaling up sociolinguistic analyses.
530,zhang-etal-2020-learning-represent,"   Learning to Represent Image and Text with Denotation Graph"",
",learning to fuse vision and language information and representing them is an important research problem with many applications. recent progresses have leveraged the ideas of pre-training (from language modeling) and attention layers in transformers to learn representation from datasets containing images aligned with linguistic expressions that describe the images. in this paper we propose learning representations from a set of implied visually grounded expressions between image and text automatically mined from those datasets. in particular we use denotation graphs to represent how specific concepts (such as sentences describing images) can be linked to abstract and generic concepts (such as short phrases) that are also visually grounded. this type of generic-to-specific relations can be discovered using linguistic analysis tools. we propose methods to incorporate such relations into learning representation. we show that state-of-the-art multimodal learning models can be further improved by leveraging automatically harvested structural relations. the representations lead to stronger empirical results on downstream tasks of cross-modal image retrieval referring expression and compositional attribute-object recognition. both our codes and the extracted denotation graphs on the flickr30k and the coco datasets are publically available on https://sha-lab.github.io/dg.
531,bekoulis-etal-2018-adversarial,"   Adversarial training for multi-context joint entity and relation extraction"",
",adversarial training (at) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data. we show how to use at for the tasks of entity recognition and relation extraction. in particular we demonstrate that applying at to a general purpose baseline model for jointly extracting entities and relations allows improving the state-of-the-art effectiveness on several datasets in different contexts (i.e. news biomedical and real estate data) and for different languages (english and dutch).
532,le-sadat-2021-towards,"   Towards a First Automatic Unsupervised Morphological Segmentation for {I}nuinnaqtun"",
",low-resource polysynthetic languages pose many challenges in nlp tasks such as morphological analysis and machine translation due to available resources and tools and the morphologically complex languages. this research focuses on the morphological segmentation while adapting an unsupervised approach based on adaptor grammars in low-resource setting. experiments and evaluations on inuinnaqtun one of inuit language family in northern canada considered a language that will be extinct in less than two generations have shown promising results.
533,lopez-espejel-etal-2021-gesera,"   {G}e{SERA}: General-domain Summary Evaluation by Relevance Analysis"",
",we present gesera an open-source improved version of sera for evaluating automatic extractive and abstractive summaries from the general domain. sera is based on a search engine that compares candidate and reference summaries (called queries) against an information retrieval document base (called index). sera was originally designed for the biomedical domain only where it showed a better correlation with manual methods than the widely used lexical-based rouge method. in this paper we take out sera from the biomedical domain to the general one by adapting its content-based method to successfully evaluate summaries from the general domain. first we improve the query reformulation strategy with pos tags analysis of general-domain corpora. second we replace the biomedical index used in sera with two article collections from aquaint-2 and wikipedia. we conduct experiments with tac2008 tac2009 and cnndm datasets. results show that in most cases gesera achieves higher correlations with manual evaluation methods than sera while it reduces its gap with rouge for general-domain summary evaluation. gesera even surpasses rouge in two cases of tac2009. finally we conduct extensive experiments and provide a comprehensive study of the impact of human annotators and the index size on summary evaluation with sera and gesera.
534,cheong-etal-2021-intrinsic,"   Intrinsic evaluation of language models for code-switching"",
",language models used in speech recognition are often either evaluated intrinsically using perplexity on test data or extrinsically with an automatic speech recognition (asr) system. the former evaluation does not always correlate well with asr performance while the latter could be specific to particular asr systems. recent work proposed to evaluate language models by using them to classify ground truth sentences among alternative phonetically similar sentences generated by a fine state transducer. underlying such an evaluation is the assumption that the generated sentences are linguistically incorrect. in this paper we first put this assumption into question and observe that alternatively generated sentences could often be linguistically correct when they differ from the ground truth by only one edit. secondly we showed that by using multi-lingual bert we can achieve better performance than previous work on two code-switching data sets. our implementation is publicly available on github at https://github.com/sikfeng/language-modelling-for-code-switching.
535,heinecke-2020-hybrid,"   Hybrid Enhanced {U}niversal {D}ependencies Parsing"",
",this paper describes our system to predict enhanced dependencies for universal dependencies (ud) treebanks which ranked 2nd in the shared task on enhanced dependency parsing with an average elas of 82.60{\%}. our system uses a hybrid two-step approach. first we use a graph-based parser to extract a basic syntactic dependency tree. then we use a set of linguistic rules which generate the enhanced dependencies for the syntactic tree. the application of these rules is optimized using a classifier which predicts their suitability in the given context. a key advantage of this approach is its language independence as rules rely solely on dependency trees and upos tags which are shared across all languages.
536,kranzlein-etal-2021-making-heads,"   Making Heads and Tails of Models with Marginal Calibration for Sparse Tagsets"",
",for interpreting the behavior of a probabilistic model it is useful to measure a model{'}s calibration{---}the extent to which it produces reliable confidence scores. we address the open problem of calibration for tagging models with sparse tagsets and recommend strategies to measure and reduce calibration error (ce) in such models. we show that several post-hoc recalibration techniques all reduce calibration error across the marginal distribution for two existing sequence taggers. moreover we propose tag frequency grouping (tfg) as a way to measure calibration error in different frequency bands. further recalibrating each group separately promotes a more equitable reduction of calibration error across the tag frequency spectrum.
537,castellana-bacciu-2020-learning,"   Learning from Non-Binary Constituency Trees via Tensor Decomposition"",
",processing sentence constituency trees in binarised form is a common and popular approach in literature. however constituency trees are non-binary by nature. the binarisation procedure changes deeply the structure furthering constituents that instead are close. in this work we introduce a new approach to deal with non-binary constituency trees which leverages tensor-based models. in particular we show how a powerful composition function based on the canonical tensor decomposition can exploit such a rich structure. a key point of our approach is the weight sharing constraint imposed on the factor matrices which allows limiting the number of model parameters. finally we introduce a tree-lstm model which takes advantage of this composition function and we experimentally assess its performance on different nlp tasks.
538,mutlu-etal-2019-team,"   Team {H}oward {B}eale at {S}em{E}val-2019 Task 4: Hyperpartisan News Detection with {BERT}"",
",this paper describes our system for semeval-2019 task 4: hyperpartisan news detection (kiesel et al. 2019). we use pretrained bert (devlin et al. 2018) architecture and investigate the effect of different fine tuning regimes on the final classification task. we show that additional pretraining on news domain improves the performance on the hyperpartisan news detection task. our system ranked 8th out of 42 teams with 78.3{\%} accuracy on the held-out test dataset.
539,ma-etal-2018-forest,"   Forest-Based Neural Machine Translation"",
",tree-based neural machine translation (nmt) approaches although achieved impressive performance suffer from a major drawback: they only use the 1-best parse tree to direct the translation which potentially introduces translation mistakes due to parsing errors. for statistical machine translation (smt) forest-based methods have been proven to be effective for solving this problem while for nmt this kind of approach has not been attempted. this paper proposes a forest-based nmt method that translates a linearized packed forest under a simple sequence-to-sequence framework (i.e. a forest-to-sequence nmt model). the bleu score of the proposed method is higher than that of the sequence-to-sequence nmt tree-based nmt and forest-based smt systems.
540,alkaoud-syed-2020-importance,"   On the Importance of Tokenization in {A}rabic Embedding Models"",
",arabic like other highly inflected languages encodes a large amount of information in its morphology and word structure. in this work we propose two embedding strategies that modify the tokenization phase of traditional word embedding models (word2vec) and contextual word embedding models (bert) to take into account arabic{'}s relatively complex morphology. in word2vec we segment words into subwords during training time and then compose word-level representations from the subwords during test time. we train our embeddings on arabic wikipedia and show that they perform better than a word2vec model on multiple arabic natural language processing datasets while being approximately 60{\%} smaller in size. moreover we showcase our embeddings{'} ability to produce accurate representations of some out-of-vocabulary words that were not encountered before. in bert we modify the tokenization layer of google{'}s pretrained multilingual bert model by incorporating information on morphology. by doing so we achieve state of the art performance on two arabic nlp datasets without pretraining.
541,chen-etal-2018-accurate,"   Accurate {SHRG}-Based Semantic Parsing"",
",we demonstrate that an shrg-based parser can produce semantic graphs much more accurately than previously shown by relating synchronous production rules to the syntacto-semantic composition process. our parser achieves an accuracy of 90.35 for eds (89.51 for dmrs) in terms of elementary dependency match which is a 4.87 (5.45) point improvement over the best existing data-driven model indicating in our view the importance of linguistically-informed derivation for data-driven semantic parsing. this accuracy is equivalent to that of english resource grammar guided models suggesting that (recurrent) neural network models are able to effectively learn deep linguistic knowledge from annotations.
542,gu-etal-2019-extract,"   Extract, Transform and Filling: A Pipeline Model for Question Paraphrasing based on Template"",
",question paraphrasing aims to restate a given question with different expressions but keep the original meaning. recent approaches are mostly based on neural networks following a sequence-to-sequence fashion however these models tend to generate unpredictable results. to overcome this drawback we propose a pipeline model based on templates. it follows three steps a) identifies template from the input question b) retrieves candidate templates c) fills candidate templates with original topic words. experiment results on two self-constructed datasets show that our model outperforms the sequence-to-sequence model in a large margin and the advantage is more promising when the size of training sample is small.
543,eiselen-puttkammer-2014-developing,"   Developing Text Resources for Ten {S}outh {A}frican Languages"",
",the development of linguistic resources for use in natural language processing is of utmost importance for the continued growth of research and development in the field especially for resource-scarce languages. in this paper we describe the process and challenges of simultaneously developing multiple linguistic resources for ten of the official languages of south africa. the project focussed on establishing a set of foundational resources that can foster further development of both resources and technologies for the nlp industry in south africa. the development efforts during the project included creating monolingual unannotated corpora of which a subset of the corpora for each language was annotated on token orthographic morphological and morphosyntactic layers. the annotated subsets includes both development and test sets and were used in the creation of five core-technologies viz. a tokeniser sentenciser lemmatiser part of speech tagger and morphological decomposer for each language. we report on the quality of these tools for each language and discuss the importance of the resources within the south african context.
544,du-ji-2019-empirical,"   An Empirical Comparison on Imitation Learning and Reinforcement Learning for Paraphrase Generation"",
",generating paraphrases from given sentences involves decoding words step by step from a large vocabulary. to learn a decoder supervised learning which maximizes the likelihood of tokens always suffers from the exposure bias. although both reinforcement learning (rl) and imitation learning (il) have been widely used to alleviate the bias the lack of direct comparison leads to only a partial image on their benefits. in this work we present an empirical study on how rl and il can help boost the performance of generating paraphrases with the pointer-generator as a base model. experiments on the benchmark datasets show that (1) imitation learning is constantly better than reinforcement learning; and (2) the pointer-generator models with imitation learning outperform the state-of-the-art methods with a large margin.
545,caines-etal-2017-collecting,"   Collecting fluency corrections for spoken learner {E}nglish"",
",we present crowdsourced collection of error annotations for transcriptions of spoken learner english. our emphasis in data collection is on fluency corrections a more complete correction than has traditionally been aimed for in grammatical error correction research (gec). fluency corrections require improvements to the text taking discourse and utterance level semantics into account: the result is a more naturalistic holistic version of the original. we propose that this shifted emphasis be reflected in a new name for the task: {`}holistic error correction{'} (hec). we analyse crowdworker behaviour in hec and conclude that the method is useful with certain amendments for future work.
546,liu-etal-2017-heterogeneous,"   Heterogeneous Supervision for Relation Extraction: A Representation Learning Approach"",
",relation extraction is a fundamental task in information extraction. most existing methods have heavy reliance on annotations labeled by human experts which are costly and time-consuming. to overcome this drawback we propose a novel framework rehession to conduct relation extractor learning using annotations from heterogeneous information source e.g. knowledge base and domain heuristics. these annotations referred as heterogeneous supervision often conflict with each other which brings a new challenge to the original relation extraction task: how to infer the true label from noisy labels for a given instance. identifying context information as the backbone of both relation extraction and true label discovery we adopt embedding techniques to learn the distributed representations of context which bridges all components with mutual enhancement in an iterative fashion. extensive experimental results demonstrate the superiority of rehession over the state-of-the-art.
547,baccar-etal-2008-modelisation,"   Mod{\'e}lisation normalis{\'e}e {LMF} des dictionnaires {\'e}lectroniques {\'e}ditoriaux de l{'}arabe"",
",le pr{\'e}sent papier s{'}int{\'e}resse {\`a} l{'}{\'e}laboration des dictionnaires {\'e}lectroniques arabes {\`a} usage {\'e}ditorial. il propose un mod{\`e}le unifi{\'e} et normalis{\'e} de ces dictionnaires en se r{\'e}f{\'e}rant {\`a} la future norme lmf (lexical markup framework) iso 24613. ce mod{\`e}le permet de construire des dictionnaires extensibles sur lesquels on peut r{\'e}aliser gr{\^a}ce {\`a} une structuration fine et standard des fonctions de consultation g{\'e}n{\'e}riques adapt{\'e}es aux besoins des utilisateurs. la mise en oeuvre du mod{\`e}le propos{\'e} est test{\'e}e sur des dictionnaires existants de la langue arabe en utilisant pour la consultation le syst{\`e}me adiqto (arabic dictionary query tools) que nous avons d{\'e}velopp{\'e} pour l{'}interrogation g{\'e}n{\'e}rique des dictionnaires normalis{\'e}s de l{'}arabe.
548,cai-xiong-2020-test,"   A Test Suite for Evaluating Discourse Phenomena in Document-level Neural Machine Translation"",
",the need to evaluate the ability of context-aware neural machine translation (nmt) models in dealing with specific discourse phenomena arises in document-level nmt. however test sets that satisfy this need are rare. in this paper we propose a test suite to evaluate three common discourse phenomena in english-chinese translation: pronoun discourse connective and ellipsis where discourse divergences lie across the two languages. the test suite contains 1200 instances 400 for each type of discourse phenomena. we perform both automatic and human evaluation with three state-of-the-art context-aware nmt models on the proposed test suite. results suggest that our test suite can be used as a challenging benchmark test bed for evaluating document-level nmt. the test suite will be publicly available soon.
549,kano-2016-answering,"   Answering Yes-No Questions by Penalty Scoring in History Subjects of University Entrance Examinations"",
",answering yes{--}no questions is more difficult than simply retrieving ranked search results. to answer yes{--}no questions especially when the correct answer is no one must find an objectionable keyword that makes the question{'}s answer no. existing systems such as factoid-based ones cannot answer yes{--}no questions very well because of insufficient handling of such objectionable keywords. we suggest an algorithm that answers yes{--}no questions by assigning an importance to objectionable keywords. concretely speaking we suggest a penalized scoring method that finds and makes lower score for parts of documents that include such objectionable keywords. we check a keyword distribution for each part of a document such as a paragraph calculating the keyword density as a basic score. then we use an objectionable keyword penalty when a keyword does not appear in a target part but appears in other parts of the document. our algorithm is robust for open domain problems because it requires no training. we achieved 4.45 point better results in f1 scores than the best score of the ntcir-10 rite2 shared task also obtained the best score in 2014 mock university examination challenge of the todai robot project.
550,lin-etal-2021-differentiable,"   Differentiable Open-Ended Commonsense Reasoning"",
",current commonsense reasoning research focuses on developing models that use commonsense knowledge to answer multiple-choice questions. however systems designed to answer multiple-choice questions may not be useful in applications that do not provide a small list of candidate answers to choose from. as a step towards making commonsense reasoning research more realistic we propose to study open-ended commonsense reasoning (opencsr) {---} the task of answering a commonsense question without any pre-defined choices {---} using as a resource only a corpus of commonsense facts written in natural language. opencsr is challenging due to a large decision space and because many questions require implicit multi-hop reasoning. as an approach to opencsr we propose drfact an efficient differentiable model for multi-hop reasoning over knowledge facts. to evaluate opencsr methods we adapt several popular commonsense reasoning benchmarks and collect multiple new answers for each test question via crowd-sourcing. experiments show that drfact outperforms strong baseline methods by a large margin.
551,xu-etal-2019-recognising,"   Recognising Agreement and Disagreement between Stances with Reason Comparing Networks"",
",we identify agreement and disagreement between utterances that express stances towards a topic of discussion. existing methods focus mainly on conversational settings where dialogic features are used for (dis)agreement inference. we extend this scope and seek to detect stance (dis)agreement in a broader setting where independent stance-bearing utterances which prevail in many stance corpora and real-world scenarios are compared. to cope with such non-dialogic utterances we find that the reasons uttered to back up a specific stance can help predict stance (dis)agreements. we propose a reason comparing network (rcn) to leverage reason information for stance comparison. empirical results on a well-known stance corpus show that our method can discover useful reason information enabling it to outperform several baselines in stance (dis)agreement detection.
552,paroubek-etal-2006-data,"   Data, Annotations and Measures in {EASY} the Evaluation Campaign for Parsers of {F}rench."",
",this paper presents the protocol of easy the evaluation campaign for syntactic parsers of french in the evalda project of the technolangue program. we describe the participants the corpus and its genre partitioning the annotation scheme which allows for the annotation of both constituents and relations the evaluation methodology and as an illustration the results obtained by one participant on half of the corpus.
553,germann-etal-2018-summa,"   The {SUMMA} Platform: A Scalable Infrastructure for Multi-lingual Multi-media Monitoring"",
",the open-source summa platform is a highly scalable distributed architecture for monitoring a large number of media broadcasts in parallel with a lag behind actual broadcast time of at most a few minutes. the platform offers a fully automated media ingestion pipeline capable of recording live broadcasts detection and transcription of spoken content translation of all text (original or transcribed) into english recognition and linking of named entities topic detection clustering and cross-lingual multi-document summarization of related media items and last but not least extraction and storage of factual claims in these news items. browser-based graphical user interfaces provide humans with aggregated information as well as structured access to individual news items stored in the platform{'}s database. this paper describes the intended use cases and provides an overview over the system{'}s implementation.
554,romanov-khusainova-2019-evaluation,"   Evaluation of Morphological Embeddings for {E}nglish and {R}ussian Languages"",
",this paper evaluates morphology-based embeddings for english and russian languages. despite the interest and introduction of several morphology based word embedding models in the past and acclaimed performance improvements on word similarity and language modeling tasks in our experiments we did not observe any stable preference over two of our baseline models - skipgram and fasttext. the performance exhibited by morphological embeddings is the average of the two baselines mentioned above.
555,nguyen-nguyen-2017-ensemble,"   An Ensemble Method with Sentiment Features and Clustering Support"",
",deep learning models have recently been applied successfully in natural language processing especially sentiment analysis. each deep learning model has a particular advantage but it is difficult to combine these advantages into one model especially in the area of sentiment analysis. in our approach convolutional neural network (cnn) and long short term memory (lstm) were utilized to learn sentiment-specific features in a freezing scheme. this scenario provides a novel and efficient way for integrating advantages of deep learning models. in addition we also grouped documents into clusters by their similarity and applied the prediction score of naive bayes svm (nbsvm) method to boost the classification accuracy of each group. the experiments show that our method achieves the state-of-the-art performance on two well-known datasets: imdb large movie reviews for document level and pang {\&} lee movie reviews for sentence level.
556,arora-etal-2020-contextual,"   Contextual Embeddings: When Are They Worth It?"",
",we study the settings for which deep contextual embeddings (e.g. bert) give large improvements in performance relative to classic pretrained embeddings (e.g. glove) and an even simpler baseline{---}random word embeddings{---}focusing on the impact of the training set size and the linguistic properties of the task. surprisingly we find that both of these simpler baselines can match contextual embeddings on industry-scale data and often perform within 5 to 10{\%} accuracy (absolute) on benchmark tasks. furthermore we identify properties of data for which contextual embeddings give particularly large gains: language containing complex structure ambiguous word usage and words unseen in training.
557,zhao-etal-2019-simple,"   Simple Question Answering with Subgraph Ranking and Joint-Scoring"",
",knowledge graph based simple question answering (kbsqa) is a major area of research within question answering. although only dealing with simple questions i.e. questions that can be answered through a single knowledge base (kb) fact this task is neither simple nor close to being solved. targeting on the two main steps subgraph selection and fact selection the literature has developed sophisticated approaches. however the importance of subgraph ranking and leveraging the subject{--}relation dependency of a kb fact have not been sufficiently explored. motivated by this we present a unified framework to describe and analyze existing approaches. using this framework as a starting point we focus on two aspects: improving subgraph selection through a novel ranking method and leveraging the subject{--}relation dependency by proposing a joint scoring cnn model with a novel loss function that enforces the well-order of scores. our methods achieve a new state of the art (85.44{\%} in accuracy) on the simplequestions dataset.
558,potthast-etal-2018-crowdsourcing,"   Crowdsourcing a Large Corpus of Clickbait on {T}witter"",
",clickbait has become a nuisance on social media. to address the urging task of clickbait detection we constructed a new corpus of 38517 annotated twitter tweets the webis clickbait corpus 2017. to avoid biases in terms of publisher and topic tweets were sampled from the top 27 most retweeted news publishers covering a period of 150 days. each tweet has been annotated on 4-point scale by five annotators recruited at amazon{'}s mechanical turk. the corpus has been employed to evaluate 12 clickbait detectors submitted to the clickbait challenge 2017. download: https://webis.de/data/webis-clickbait-17.html challenge: https://clickbait-challenge.org
559,liao-etal-2017-ynu,"   {YNU}-{HPCC} at {IJCNLP}-2017 Task 1: {C}hinese Grammatical Error Diagnosis Using a Bi-directional {LSTM}-{CRF} Model"",
",building a system to detect chinese grammatical errors is a challenge for natural-language processing researchers. as chinese learners are increasing developing such a system can help them study chinese more easily. this paper introduces a bi-directional long short-term memory (bilstm) - conditional random field (crf) model to produce the sequences that indicate an error type for every position of a sentence since we regard chinese grammatical error diagnosis (cged) as a sequence-labeling problem.
560,liu-etal-2020-rikinet,"   {R}iki{N}et: Reading {W}ikipedia Pages for Natural Question Answering"",
",reading long documents to answer open-domain questions remains challenging in natural language understanding. in this paper we introduce a new model called rikinet which reads wikipedia pages for natural question answering. rikinet contains a dynamic paragraph dual-attention reader and a multi-level cascaded answer predictor. the reader dynamically represents the document and question by utilizing a set of complementary attention mechanisms. the representations are then fed into the predictor to obtain the span of the short answer the paragraph of the long answer and the answer type in a cascaded manner. on the natural questions (nq) dataset a single rikinet achieves 74.3 f1 and 57.9 f1 on long-answer and short-answer tasks. to our best knowledge it is the first single model that outperforms the single human performance. furthermore an ensemble rikinet obtains 76.1 f1 and 61.3 f1 on long-answer and short-answer tasks achieving the best performance on the official nq leaderboard.
561,wang-etal-2020-mongolian,"   {M}ongolian Questions Classification Based on Mulit-Head Attention"",
",question classification is a crucial subtask in question answering system. mongolian is a kind of few resource language. it lacks public labeled corpus. and the complex morphological structure of mongolian vocabulary makes the data-sparse problem. this paper proposes a classification model which combines the bi-lstm model with the multi-head attention mechanism. the multi-head attention mechanism extracts relevant information from different dimensions and representation subspace. according to the characteristics of mongolian word-formation this paper introduces mongolian morphemes representation in the embedding layer. morpheme vector focuses on the semantics of the mongolian word. in this paper character vector and morpheme vector are concatenated to get word vector which sends to the bi-lstm getting context representation. finally the multi-head attention obtains global information for classification. the model experimented on the mongolian corpus. experimental results show that our proposed model significantly outperforms baseline systems.
562,huang-bai-2021-hub-dravidianlangtech,"   {HUB}@{D}ravidian{L}ang{T}ech-{EACL}2021: Meme Classification for {T}amil Text-Image Fusion"",
",this article describes our system for task dravidianlangtech - eacl2021: meme classification for tamil. in recent years we have witnessed the rapid development of the internet and social media. compared with traditional tv and radio media platforms there are not so many restrictions on the use of online social media for individuals and many functions of online social media platforms are free. based on this feature of social media it is difficult for people{'}s posts/comments on social media to be strictly and effectively controlled like tv and radio content. therefore the detection of negative information in social media has attracted attention from academic and industrial fields in recent years. the task of classifying memes is also driven by offensive posts/comments prevalent on social media. the data of the meme classification task is the fusion data of text and image information. to identify the content expressed by the meme we develop a system that combines bigru and cnn. it can fuse visual features and text features to achieve the purpose of using multi-modal information from memetic data. in this article we discuss our methods models experiments and results.
563,muller-etal-2021-unseen,"   When Being Unseen from m{BERT} is just the Beginning: Handling New Languages With Multilingual Language Models"",
",transfer learning based on pretraining language models on a large amount of raw data has become a new norm to reach state-of-the-art performance in nlp. still it remains unclear how this approach should be applied for unseen languages that are not covered by any available large-scale multilingual language model and for which only a small amount of raw data is generally available. in this work by comparing multilingual and monolingual models we show that such models behave in multiple ways on unseen languages. some languages greatly benefit from transfer learning and behave similarly to closely related high resource languages whereas others apparently do not. focusing on the latter we show that this failure to transfer is largely related to the impact of the script used to write such languages. we show that transliterating those languages significantly improves the potential of large-scale multilingual language models on downstream tasks. this result provides a promising direction towards making these massively multilingual models useful for a new set of unseen languages.
564,del-tredici-fernandez-2018-road,"   The Road to Success: Assessing the Fate of Linguistic Innovations in Online Communities"",
",we investigate the birth and diffusion of lexical innovations in a large dataset of online social communities. we build on sociolinguistic theories and focus on the relation between the spread of a novel term and the social role of the individuals who use it uncovering characteristics of innovators and adopters. finally we perform a prediction task that allows us to anticipate whether an innovation will successfully spread within a community.
565,christodoulides-etal-2014-dismo,"   {D}is{M}o: A Morphosyntactic, Disfluency and Multi-Word Unit Annotator. An Evaluation on a Corpus of {F}rench Spontaneous and Read Speech"",
",we present dismo a multi-level annotator for spoken language corpora that integrates part-of-speech tagging with basic disfluency detection and annotation and multi-word unit recognition. dismo is a hybrid system that uses a combination of lexical resources rules and statistical models based on conditional random fields (crf). in this paper we present the first public version of dismo for french. the system is trained and its performance evaluated on a 57k-token corpus including different varieties of french spoken in three countries (belgium france and switzerland). dismo supports a multi-level annotation scheme in which the tokenisation to minimal word units is complemented with multi-word unit groupings (each having associated pos tags) as well as separate levels for annotating disfluencies and discourse phenomena. we present the systems architecture linguistic resources and its hierarchical tag-set. results show that dismo achieves a precision of 95{\%} (finest tag-set) to 96.8{\%} (coarse tag-set) in pos-tagging non-punctuated sound-aligned transcriptions of spoken french while also offering substantial possibilities for automated multi-level annotation.
566,zhang-etal-2019-cluster,"   Cluster-Gated Convolutional Neural Network for Short Text Classification"",
",text classification plays a crucial role for understanding natural language in a wide range of applications. most existing approaches mainly focus on long text classification (e.g. blogs documents paragraphs). however they cannot easily be applied to short text because of its sparsity and lack of context. in this paper we propose a new model called cluster-gated convolutional neural network (cgcnn) which jointly explores word-level clustering and text classification in an end-to-end manner. specifically the proposed model firstly uses a bi-directional long short-term memory to learn word representations. then it leverages a soft clustering method to explore their semantic relation with the cluster centers and takes linear transformation on text representations. it develops a cluster-dependent gated convolutional layer to further control the cluster-dependent feature flows. experimental results on five commonly used datasets show that our model outperforms state-of-the-art models.
567,oard-1998-comparative,"   A comparative study of query and document translation for cross-language information retrieval"",
",cross-language retrieval systems use queries in one natural language to guide retrieval of documents that might be written in another. acquisition and representation of translation knowledge plays a central role in this process. this paper explores the utility of two sources of translation knowledge for cross-language retrieval. we have implemented six query translation techniques that use bilingual term lists and one based on direct use of the translation output from an existing machine translation system; these are compared with a document translation technique that uses output from the same machine translation system. average precision measures on a trec collection suggest that arbitrarily selecting a single dictionary translation is typically no less effective than using every translation in the dictionary that query translation using a machine translation system can achieve somewhat better effectiveness than simpler techniques and that document translation may result in further improvements in retrieval effectiveness under some conditions.
568,sekine-grishman-1995-corpus,"   A Corpus-based Probabilistic Grammar with Only Two Non-terminals"",
",the availability of large syntactically-bracketed corpora such as the penn tree bank affords us the opportunity to automatically build or train broad-coverage grammars and in particular to train probabilistic grammars. a number of recent parsing experiments have also indicated that grammars whose production probabilities are dependent on the context can be more effective than context-free grammars in selecting a correct parse. to make maximal use of context we have automatically constructed from the penn tree bank version 2 a grammar in which the symbols s and np are the only real nonterminals and the other non-terminals or grammatical nodes are in effect embedded into the right-hand-sides of the s and np rules. for example one of the rules extracted from the tree bank would be s -{\textgreater} np vbx jj cc vbx np [1] ( where np is a non-terminal and the other symbols are terminals {--} part-of-speech tags of the tree bank). the most common structure in the tree bank associated with this expansion is (s np (vp (vp vbx (adj jj) cc (vp vbx np)))) [2]. so if our parser uses rule [1] in parsing a sentence it will generate structure [2] for the corresponding part of the sentence. using 94{\%} of the penn tree bank for training we extracted 32296 distinct rules ( 23386 for s and 8910 for np). we also built a smaller version of the grammar based on higher frequency patterns for use as a back-up when the larger grammar is unable to produce a parse due to memory limitation. we applied this parser to 1989 wall street journal sentences (separate from the training set and with no limit on sentence length). of the parsed sentences (1899) the percentage of no-crossing sentences is 33.9{\%} and parseval recall and precision are 73.43{\%} and 72 .61{\%}.
569,khatri-etal-2021-language,"   Language Relatedness and Lexical Closeness can help Improve Multilingual {NMT}: {IITB}ombay@{M}ulti{I}ndic{NMT} {WAT}2021"",
",multilingual neural machine translation has achieved remarkable performance by training a single translation model for multiple languages. this paper describes our submission (team id: cfilt-iitb) for the multiindicmt: an indic language multilingual task at wat 2021. we train multilingual nmt systems by sharing encoder and decoder parameters with language embedding associated with each token in both encoder and decoder. furthermore we demonstrate the use of transliteration (script conversion) for indic languages in reducing the lexical gap for training a multilingual nmt system. further we show improvement in performance by training a multilingual nmt system using languages of the same family i.e. related languages.
570,yang-etal-2021-learning-answer,"   Learning to Answer Psychological Questionnaire for Personality Detection"",
",existing text-based personality detection research mostly relies on data-driven approaches to implicitly capture personality cues in online posts lacking the guidance of psychological knowledge. psychological questionnaire which contains a series of dedicated questions highly related to personality traits plays a critical role in self-report personality assessment. we argue that the posts created by a user contain critical contents that could help answer the questions in a questionnaire resulting in an assessment of his personality by linking the texts and the questionnaire. to this end we propose a new model named psychological questionnaire enhanced network (pq-net) to guide personality detection by tracking critical information in texts with a questionnaire. specifically pq-net contains two streams: a context stream to encode each piece of text into a contextual text representation and a questionnaire stream to capture relevant information in the contextual text representation to generate potential answer representations for a questionnaire. the potential answer representations are used to enhance the contextual text representation and to benefit personality prediction. experimental results on two datasets demonstrate the superiority of pq-net in capturing useful cues from the posts for personality detection.
571,zhang-chai-2021-coin,"   {COIN}: Conversational Interactive Networks for Emotion Recognition in Conversation"",
",emotion recognition in conversation has received considerable attention recently because of its practical industrial applications. existing methods tend to overlook the immediate mutual interaction between different speakers in the speaker-utterance level or apply single speaker-agnostic rnn for utterances from different speakers. we propose coin a conversational interactive model to mitigate this problem by applying state mutual interaction within history contexts. in addition we introduce a stacked global interaction module to capture the contextual and inter-dependency representation in a hierarchical manner. to improve the robustness and generalization during training we generate adversarial examples by applying the minor perturbations on multimodal feature inputs unveiling the benefits of adversarial examples for emotion detection. the proposed model empirically achieves the current state-of-the-art results on the iemocap benchmark dataset.
572,shibata-etal-2014-large,"   A Large Scale Database of Strongly-related Events in {J}apanese"",
",the knowledge about the relation between events is quite useful for coreference resolution anaphora resolution and several nlp applications such as dialogue system. this paper presents a large scale database of strongly-related events in japanese which has been acquired with our proposed method (shibata and kurohashi 2011). in languages where omitted arguments or zero anaphora are often utilized such as japanese the coreference-based event extraction methods are hard to be applied and so our method extracts strongly-related events in a two-phrase construct. this method first calculates the co-occurrence measure between predicate-arguments (events) and regards an event pair whose mutual information is high as strongly-related events. to calculate the co-occurrence measure efficiently we adopt an association rule mining method. then we identify the remaining arguments by using case frames. the database contains approximately 100000 unique events with approximately 340000 strongly-related event pairs which is much larger than an existing automatically-constructed event database. we evaluated randomly-chosen 100 event pairs and the accuracy was approximately 68{\%}.
573,declerck-2008-framework,"   A Framework for Standardized Syntactic Annotation"",
",this poster presents an iso framework for the standardization of syntactic annotation (synaf). the normative part synaf is concerned with a metamodel for syntactic annotation that covers both dimensions of constituency and dependency and propose thus a multi-layered annotation framework that allows the combined and interrelated annotation of language data along both lines of consideration. this standard is designed to be used in close conjuncion with the metamodel presented in the linguistic annotation framework (laf) and with iso 12620 terminology and other language resources - data categories.
574,goncalves-etal-2012-treebanking,"   Treebanking by Sentence and Tree Transformation: Building a Treebank to support Question Answering in {P}ortuguese"",
",this paper presents cintil-qatreebank a treebank composed of portuguese sentences that can be used to support the development of question answering systems. to create this treebank we use declarative sentences from the pre-existing cintil-treebank and manually transform their syntactic structure into a non-declarative sentence. our corpus includes two clause types: interrogative and imperative clauses. cintil-qatreebank can be used in language science and techology general research but it was developed particularly for the development of automatic question answering systems. the non-declarative entences are annotated with several layers of linguistic information namely (i) trees with information on constituency and grammatical function; (ii) sentence type; (iii) interrogative pronoun; (iv) question type; and (v) semantic type of expected answer. moreover these non-declarative sentences are paired with their declarative counterparts and associated with the expected answer snippets.
575,dragoni-2018-neurosent-pdi,"   {NEUROSENT}-{PDI} at {S}em{E}val-2018 Task 3: Understanding Irony in Social Networks Through a Multi-Domain Sentiment Model"",
",this paper describes the neurosent system that participated in semeval 2018 task 3. our system takes a supervised approach that builds on neural networks and word embeddings. word embeddings were built by starting from a repository of user generated reviews. thus they are specific for sentiment analysis tasks. then tweets are converted in the corresponding vector representation and given as input to the neural network with the aim of learning the different semantics contained in each emotion taken into account by the semeval task. the output layer has been adapted based on the characteristics of each subtask. preliminary results obtained on the provided training set are encouraging for pursuing the investigation into this direction.
576,gong-etal-2018-convolutional,"   Convolutional Interaction Network for Natural Language Inference"",
",attention-based neural models have achieved great success in natural language inference (nli). in this paper we propose the convolutional interaction network (cin) a general model to capture the interaction between two sentences which can be an alternative to the attention mechanism for nli. specifically cin encodes one sentence with the filters dynamically generated based on another sentence. since the filters may be designed to have various numbers and sizes cin can capture more complicated interaction patterns. experiments on three large datasets demonstrate cin{'}s efficacy.
577,glavas-etal-2020-semeval,"   {S}em{E}val-2020 Task 2: Predicting Multilingual and Cross-Lingual (Graded) Lexical Entailment"",
",lexical entailment (le) is a fundamental asymmetric lexico-semantic relation supporting the hierarchies in lexical resources (e.g. wordnet conceptnet) and applications like natural language inference and taxonomy induction. multilingual and cross-lingual nlp applications warrant models for le detection that go beyond language boundaries. as part of semeval 2020 we carried out a shared task (task 2) on multilingual and cross-lingual le. the shared task spans three dimensions: (1) monolingual vs. cross-lingual le (2) binary vs. graded le and (3) a set of 6 diverse languages (and 15 corresponding language pairs). we offered two different evaluation tracks: (a) dist: for unsupervised fully distributional models that capture le solely on the basis of unannotated corpora and (b) any: for externally informed models allowed to leverage any resources including lexico-semantic networks (e.g. wordnet or babelnet). in the any track we recieved runs that push state-of-the-art across all languages and language pairs for both binary le detection and graded le prediction.
578,lemmens-etal-2021-improving,"   Improving Hate Speech Type and Target Detection with Hateful Metaphor Features"",
",we study the usefulness of hateful metaphorsas features for the identification of the type and target of hate speech in dutch facebook comments. for this purpose all hateful metaphors in the dutch lilah corpus were annotated and interpreted in line with conceptual metaphor theory and critical metaphor analysis. we provide svm and bert/roberta results and investigate the effect of different metaphor information encoding methods on hate speech type and target detection accuracy. the results of the conducted experiments show that hateful metaphor features improve model performance for the both tasks. to our knowledge it is the first time that the effectiveness of hateful metaphors as an information source for hatespeech classification is investigated.
579,wang-etal-2018-bridge,"   Bridge Video and Text with Cascade Syntactic Structure"",
",we present a video captioning approach that encodes features by progressively completing syntactic structure (lstm-css). to construct basic syntactic structure (i.e. subject predicate and object) we use a conditional random field to label semantic representations (i.e. motions objects). we argue that in order to improve the comprehensiveness of the description the local features within object regions can be used to generate complementary syntactic elements (e.g. attribute adverbial). inspired by redundancy of human receptors we utilize a region proposal network to focus on the object regions. to model the final temporal dynamics recurrent neural network with path embeddings is adopted. we demonstrate the effectiveness of lstm-css on generating natural sentences: 42.3{\%} and 28.5{\%} in terms of bleu@4 and meteor. superior performance when compared to state-of-the-art methods are reported on a large video description dataset (i.e. msr-vtt-2016).
580,cao-etal-2020-unsupervised-dual,"   Unsupervised Dual Paraphrasing for Two-stage Semantic Parsing"",
",one daunting problem for semantic parsing is the scarcity of annotation. aiming to reduce nontrivial human labor we propose a two-stage semantic parsing framework where the first stage utilizes an unsupervised paraphrase model to convert an unlabeled natural language utterance into the canonical utterance. the downstream naive semantic parser accepts the intermediate output and returns the target logical form. furthermore the entire training process is split into two phases: pre-training and cycle learning. three tailored self-supervised tasks are introduced throughout training to activate the unsupervised paraphrase model. experimental results on benchmarks overnight and geogranno demonstrate that our framework is effective and compatible with supervised training.
581,dutt-etal-2020-keeping,"   Keeping Up Appearances: Computational Modeling of Face Acts in Persuasion Oriented Discussions"",
",the notion of face refers to the public self-image of an individual that emerges both from the individual{'}s own actions as well as from the interaction with others. modeling face and understanding its state changes throughout a conversation is critical to the study of maintenance of basic human needs in and through interaction. grounded in the politeness theory of brown and levinson (1978) we propose a generalized framework for modeling face acts in persuasion conversations resulting in a reliable coding manual an annotated corpus and computational models. the framework reveals insights about differences in face act utilization between asymmetric roles in persuasion conversations. using computational models we are able to successfully identify face acts as well as predict a key conversational outcome (e.g. donation success). finally we model a latent representation of the conversational state to analyze the impact of predicted face acts on the probability of a positive conversational outcome and observe several correlations that corroborate previous findings.
582,tourille-etal-2016-extraction,"   Extraction de relations temporelles dans des dossiers {\'e}lectroniques patient (Extracting Temporal Relations from Electronic Health Records)"",
",l{'}analyse temporelle des documents cliniques permet d{'}obtenir des repr{\'e}sentations riches des informations contenues dans les dossiers {\'e}lectroniques patient. cette analyse repose sur l{'}extraction d{'}{\'e}v{\'e}nements d{'}expressions temporelles et des relations entre eux. dans ce travail nous consid{\'e}rons que nous disposons des {\'e}v{\'e}nements et des expressions temporelles pertinents et nous nous int{\'e}ressons aux relations temporelles entre deux {\'e}v{\'e}nements ou entre un {\'e}v{\'e}nement et une expression temporelle. nous pr{\'e}sentons des mod{\`e}les de classification supervis{\'e}e pour l{'}extraction de des relations en fran{\c{c}}ais et en anglais. les performances obtenues sont comparables dans les deux langues sugg{\'e}rant ainsi que diff{\'e}rents domaines cliniques et diff{\'e}rentes langues pourraient {\^e}tre abord{\'e}s de mani{\`e}re similaire.
583,schluter-2015-critical,"   A critical survey on measuring success in rank-based keyword assignment to documents"",
",evaluation approaches for unsupervised rank-based keyword assignment are nearly as numerous as are the existing systems. the prolific production of each newly used metric (or metric twist) seems to stem from general dis-satisfaction with the previous one and the source of that dissatisfaction has not previously been discussed in the literature. the difficulty may stem from a poor specification of the keyword assignment task in view of the rank-based approach. with a more complete specification of this task we aim to show why the previous evaluation metrics fail to satisfy researchers{'} goals to distinguish and detect good rank-based keyword assignment systems. we put forward a characterisation of an ideal evaluation metric and discuss the consistency of the evaluation metrics with this ideal finding that the average standard normalised cumulative gain metric is most consistent with this ideal.
584,dubossarsky-etal-2017-outta,"   Outta Control: Laws of Semantic Change and Inherent Biases in Word Representation Models"",
",this article evaluates three proposed laws of semantic change. our claim is that in order to validate a putative law of semantic change the effect should be observed in the genuine condition but absent or reduced in a suitably matched control condition in which no change can possibly have taken place. our analysis shows that the effects reported in recent literature must be substantially revised: (i) the proposed negative correlation between meaning change and word frequency is shown to be largely an artefact of the models of word representation used; (ii) the proposed negative correlation between meaning change and prototypicality is shown to be much weaker than what has been claimed in prior art; and (iii) the proposed positive correlation between meaning change and polysemy is largely an artefact of word frequency. these empirical observations are corroborated by analytical proofs that show that count representations introduce an inherent dependence on word frequency and thus word frequency cannot be evaluated as an independent factor with these representations.
585,wu-etal-2019-different,"   Different Absorption from the Same Sharing: Sifted Multi-task Learning for Fake News Detection"",
",recently neural networks based on multi-task learning have achieved promising performance on fake news detection which focuses on learning shared features among tasks as complementarity features to serve different tasks. however in most of the existing approaches the shared features are completely assigned to different tasks without selection which may lead to some useless and even adverse features integrated into specific tasks. in this paper we design a sifted multi-task learning method with a selected sharing layer for fake news detection. the selected sharing layer adopts gate mechanism and attention mechanism to filter and select shared feature flows between tasks. experiments on two public and widely used competition datasets i.e. rumoureval and pheme demonstrate that our proposed method achieves the state-of-the-art performance and boosts the f1-score by more than 0.87{\%} 1.31{\%} respectively.
586,chakraborty-etal-2019-sparse,"   Sparse Victory {--} A Large Scale Systematic Comparison of count-based and prediction-based vectorizers for text classification"",
",in this paper we study the performance of several text vectorization algorithms on a diverse collection of 73 publicly available datasets. traditional sparse vectorizers like tf-idf and feature hashing have been systematically compared with the latest state of the art neural word embeddings like word2vec glove fasttext and character embeddings like elmo flair. we have carried out an extensive analysis of the performance of these vectorizers across different dimensions like classification metrics (.i.e. precision recall accuracy) dataset-size and imbalanced data (in terms of the distribution of the number of class labels). our experiments reveal that the sparse vectorizers beat the neural word and character embedding models on 61 of the 73 datasets by an average margin of 3-5{\%} (in terms of macro f1 score) and this performance is consistent across the different dimensions of comparison.
587,ansell-etal-2021-polylm,"   {P}oly{LM}: Learning about Polysemy through Language Modeling"",
",to avoid the {``}meaning conflation deficiency{''} of word embeddings a number of models have aimed to embed individual word senses. these methods at one time performed well on tasks such as word sense induction (wsi) but they have since been overtaken by task-specific techniques which exploit contextualized embeddings. however sense embeddings and contextualization need not be mutually exclusive. we introduce polylm a method which formulates the task of learning sense embeddings as a language modeling problem allowing contextualization techniques to be applied. polylm is based on two underlying assumptions about word senses: firstly that the probability of a word occurring in a given context is equal to the sum of the probabilities of its individual senses occurring; and secondly that for a given occurrence of a word one of its senses tends to be much more plausible in the context than the others. we evaluate polylm on wsi showing that it performs considerably better than previous sense embedding techniques and matches the current state-of-the-art specialized wsi method despite having six times fewer parameters. code and pre-trained models are available at https://github.com/alanansell/polylm.
588,rei-sogaard-2018-zero,"   Zero-Shot Sequence Labeling: Transferring Knowledge from Sentences to Tokens"",
",can attention- or gradient-based visualization techniques be used to infer token-level labels for binary sequence tagging problems using networks trained only on sentence-level labels? we construct a neural network architecture based on soft attention train it as a binary sentence classifier and evaluate against token-level annotation on four different datasets. inferring token labels from a network provides a method for quantitatively evaluating what the model is learning along with generating useful feedback in assistance systems. our results indicate that attention-based methods are able to predict token-level labels more accurately compared to gradient-based methods sometimes even rivaling the supervised oracle network.
589,yang-etal-2021-pos,"   {POS}-{C}onstrained {P}arallel {D}ecoding for {N}on-autoregressive {G}eneration"",
",the multimodality problem has become a major challenge of existing non-autoregressive generation (nag) systems. a common solution often resorts to sequence-level knowledge distillation by rebuilding the training dataset through autoregressive generation (hereinafter known as {``}teacher ag{''}). the success of such methods may largely depend on a latent assumption i.e. the teacher ag is superior to the nag model. however in this work we experimentally reveal that this assumption does not always hold for the text generation tasks like text summarization and story ending generation. to provide a feasible solution to the multimodality problem of nag we propose incorporating linguistic structure (part-of-speech sequence in particular) into nag inference instead of relying on teacher ag. more specifically the proposed pos-constrained parallel decoding (pospd) method aims at providing a specific pos sequence to constrain the nag model during decoding. our experiments demonstrate that pospd consistently improves nag models on four text generation tasks to a greater extent compared to knowledge distillation. this observation validates the necessity of exploring the alternatives for sequence-level knowledge distillation.
590,zhang-etal-2014-probabilistic,"   A probabilistic feature-based fill-up for {SMT}"",
",in this paper we describe an effective translation model combination approach based on the estimation of a probabilistic support vector machine (svm). we collect domain knowledge from both in-domain and general-domain corpora inspired by a commonly used data selection algorithm which we then use as features for the svm training. drawing on previous work on binary-featured phrase table fill-up (nakov 2008; bisazza et al. 2011) we substitute the binary feature in the original work with our probabilistic domain-likeness feature. later we design two experiments to evaluate the proposed probabilistic feature-based approach on the french-to-english language pair using data provided at wmt07 wmt13 and iwlst11 translation tasks. our experiments demonstrate that translation performance can gain significant improvements of up to +0.36 and +0.82 bleu scores by using our probabilistic feature-based translation model fill-up approach compared with the binary featured fill-up approach in both experiments.
591,mowery-etal-2017-investigating,"   Investigating the Documentation of Electronic Cigarette Use in the Veteran Affairs Electronic Health Record: A Pilot Study"",
",in this paper we present pilot work on characterising the documentation of electronic cigarettes (e-cigarettes) in the united states veterans administration electronic health record. the veterans health administration is the largest health care system in the united states with 1233 health care facilities nationwide serving 8.9 million veterans per year. we identified a random sample of 2000 veterans administration patients coded as current tobacco users from 2008 to 2014. using simple keyword matching techniques combined with qualitative analysis we investigated the prevalence and distribution of e-cigarette terms in these clinical notes discovering that for current smokers 11.9{\%} of patient records contain an e-cigarette related term.
592,han-etal-2021-exploring,"   Exploring Task Difficulty for Few-Shot Relation Extraction"",
",few-shot relation extraction (fsre) focuses on recognizing novel relations by learning with merely a handful of annotated instances. meta-learning has been widely adopted for such a task which trains on randomly generated few-shot tasks to learn generic data representations. despite impressive results achieved existing models still perform suboptimally when handling hard fsre tasks where the relations are fine-grained and similar to each other. we argue this is largely because existing models do not distinguish hard tasks from easy ones in the learning process. in this paper we introduce a novel approach based on contrastive learning that learns better representations by exploiting relation label information. we further design a method that allows the model to adaptively learn how to focus on hard tasks. experiments on two standard datasets demonstrate the effectiveness of our method.
593,hagiwara-etal-2019-teaspn,"   {TEASPN}: Framework and Protocol for Integrated Writing Assistance Environments"",
",language technologies play a key role in assisting people with their writing. although there has been steady progress in e.g. grammatical error correction (gec) human writers are yet to benefit from this progress due to the high development cost of integrating with writing software. we propose teaspn a protocol and an open-source framework for achieving integrated writing assistance environments. the protocol standardizes the way writing software communicates with servers that implement such technologies allowing developers and researchers to integrate the latest developments in natural language processing (nlp) with low cost. as a result users can enjoy the integrated experience in their favorite writing software. the results from experiments with human participants show that users use a wide range of technologies and rate their writing experience favorably allowing them to write more fluent text.
594,gregori-panunzi-2017-measuring,"   Measuring the {I}talian-{E}nglish lexical gap for action verbs and its impact on translation"",
",this paper describes a method to measure the lexical gap of action verbs in italian and english by using the imagact ontology of action. the fine-grained categorization of action concepts of the data source allowed to have wide overview of the relation between concepts in the two languages. the calculated lexical gap for both english and italian is about 30{\%} of the action concepts much higher than previous results. beyond this general numbers a deeper analysis has been performed in order to evaluate the impact that lexical gaps can have on translation. in particular a distinction has been made between the cases in which the presence of a lexical gap affects translation correctness and completeness at a semantic level. the results highlight a high percentage of concepts that can be considered hard to translate (about 18{\%} from english to italian and 20{\%} from italian to english) and confirms that action verbs are a critical lexical class for translation tasks.
595,araki-mitamura-2018-open,"   Open-Domain Event Detection using Distant Supervision"",
",this paper introduces open-domain event detection a new event detection paradigm to address issues of prior work on restricted domains and event annotation. the goal is to detect all kinds of events regardless of domains. given the absence of training data we propose a distant supervision method that is able to generate high-quality training data. using a manually annotated event corpus as gold standard our experiments show that despite no direct supervision the model outperforms supervised models. this result indicates that the distant supervision enables robust event detection in various domains while obviating the need for human annotation of events.
596,ciora-etal-2021-examining,"   Examining Covert Gender Bias: A Case Study in {T}urkish and {E}nglish Machine Translation Models"",
",as machine translation (mt) has become increasingly more powerful accessible and widespread the potential for the perpetuation of bias has grown alongside its advances. while overt indicators of bias have been studied in machine translation we argue that covert biases expose a problem that is further entrenched. through the use of the gender-neutral language turkish and the gendered language english we examine cases of both overt and covert gender bias in mt models. specifically we introduce a method to investigate asymmetrical gender markings. we also assess bias in the attribution of personhood and examine occupational and personality stereotypes through overt bias indicators in mt models. our work explores a deeper layer of bias in mt models and demonstrates the continued need for language-specific interdisciplinary methodology in mt model development.
597,liu-etal-2019-incorporating,"   Incorporating Interlocutor-Aware Context into Response Generation on Multi-Party Chatbots"",
",conventional chatbots focus on two-party response generation which simplifies the real dialogue scene. in this paper we strive toward a novel task of response generation on multi-party chatbot (rgmpc) where the generated responses heavily rely on the interlocutors{'} roles (e.g. speaker and addressee) and their utterances. unfortunately complex interactions among the interlocutors{'} roles make it challenging to precisely capture conversational contexts and interlocutors{'} information. facing this challenge we present a response generation model which incorporates interlocutor-aware contexts into recurrent encoder-decoder frameworks (icred) for rgmpc. specifically we employ interactive representations to capture dialogue contexts for different interlocutors. moreover we leverage an addressee memory to enhance contextual interlocutor information for the target addressee. finally we construct a corpus for rgmpc based on an existing open-access dataset. automatic and manual evaluations demonstrate that the icred remarkably outperforms strong baselines.
598,sheang-saggion-2021-controllable,"   Controllable Sentence Simplification with a Unified Text-to-Text Transfer Transformer"",
",recently a large pre-trained language model called t5 (a unified text-to-text transfer transformer) has achieved state-of-the-art performance in many nlp tasks. however no study has been found using this pre-trained model on text simplification. therefore in this paper we explore the use of t5 fine-tuning on text simplification combining with a controllable mechanism to regulate the system outputs that can help generate adapted text for different target audiences. our experiments show that our model achieves remarkable results with gains of between +0.69 and +1.41 over the current state-of-the-art (bart+access). we argue that using a pre-trained model such as t5 trained on several tasks with large amounts of data can help improve text simplification.
599,xue-zhang-2014-buy,"   Buy one get one free: Distant annotation of {C}hinese tense, event type and modality"",
",we describe a {``}distant annotation{''} method where we mark up the semantic tense event type and modality of chinese events via a word-aligned parallel corpus. we first map chinese verbs to their english counterparts via word alignment and then annotate the resulting english text spans with coarse-grained categories for semantic tense event type and modality that we believe apply to both english and chinese. because english has richer morpho-syntactic indicators for semantic tense event type and modality than chinese our intuition is that this distant annotation approach will yield more consistent annotation than if we annotate the chinese side directly. we report experimental results that show stable annotation agreement statistics and that event type and modality have significant influence on tense prediction. we also report the size of the annotated corpus that we have obtained and how different domains impact annotation consistency.
600,gui-etal-2017-question,"   A Question Answering Approach for Emotion Cause Extraction"",
",emotion cause extraction aims to identify the reasons behind a certain emotion expressed in text. it is a much more difficult task compared to emotion classification. inspired by recent advances in using deep memory networks for question answering (qa) we propose a new approach which considers emotion cause identification as a reading comprehension task in qa. inspired by convolutional neural networks we propose a new mechanism to store relevant context in different memory slots to model context information. our proposed approach can extract both word level sequence features and lexical features. performance evaluation shows that our method achieves the state-of-the-art performance on a recently released emotion cause dataset outperforming a number of competitive baselines by at least 3.01{\%} in f-measure.
601,marchisio-etal-2019-johns,"   {J}ohns {H}opkins {U}niversity Submission for {WMT} News Translation Task"",
",we describe the work of johns hopkins university for the shared task of news translation organized by the fourth conference on machine translation (2019). we submitted systems for both directions of the english-german language pair. the systems combine multiple techniques {--} sampling filtering iterative backtranslation and continued training {--} previously used to improve performance of neural machine translation models. at submission time we achieve a bleu score of 38.1 for de-en and 42.5 for en-de translation directions on newstest2019. post-submission the score is 38.4 for de-en and 42.8 for en-de. various experiments conducted in the process are also described.
602,wei-etal-2021-linguistic,"   Linguistic Complexity Loss in Text-Based Therapy"",
",the complexity loss paradox which posits that individuals suffering from disease exhibit surprisingly predictable behavioral dynamics has been observed in a variety of both human and animal physiological systems. the recent advent of online text-based therapy presents a new opportunity to analyze the complexity loss paradox in a novel operationalization: linguistic complexity loss in text-based therapy conversations. in this paper we analyze linguistic complexity correlates of mental health in the online therapy messages sent between therapists and 7170 clients who provided 30437 corresponding survey responses on their anxiety. we found that when clients reported more anxiety they showed reduced lexical diversity as estimated by the moving average type-token ratio. therapists on the other hand used language of higher reading difficulty syntactic complexity and age of acquisition when clients were more anxious. finally we found that clients and to an even greater extent therapists exhibited consistent levels of many linguistic complexity measures. these results demonstrate how linguistic analysis of text-based communication can be leveraged as a marker for anxiety an exciting prospect in a time of both increased online communication and increased mental health issues.
603,kim-etal-2016-frustratingly,"   Frustratingly Easy Neural Domain Adaptation"",
",popular techniques for domain adaptation such as the feature augmentation method of daum{\'e} iii (2009) have mostly been considered for sparse binary-valued features but not for dense real-valued features such as those used in neural networks. in this paper we describe simple neural extensions of these techniques. first we propose a natural generalization of the feature augmentation method that uses k + 1 lstms where one model captures global patterns across all k domains and the remaining k models capture domain-specific information. second we propose a novel application of the framework for learning shared structures by ando and zhang (2005) to domain adaptation and also provide a neural extension of their approach. in experiments on slot tagging over 17 domains our methods give clear performance improvement over daum{\'e} iii (2009) applied on feature-rich crfs.
604,bhargava-penn-2021-proof,"   Proof Net Structure for Neural {L}ambek Categorial Parsing"",
",in this paper we present the first statistical parser for lambek categorial grammar (lcg) a grammatical formalism for which the graphical proof method known as *proof nets* is applicable. our parser incorporates proof net structure and constraints into a system based on self-attention networks via novel model elements. our experiments on an english lcg corpus show that incorporating term graph structure is helpful to the model improving both parsing accuracy and coverage. moreover we derive novel loss functions by expressing proof net constraints as differentiable functions of our model output enabling us to train our parser without ground-truth derivations.
605,wang-etal-2020-tencent,"   Tencent {AI} Lab Machine Translation Systems for {WMT}20 Chat Translation Task"",
",this paper describes the tencent ai lab{'}s submission of the wmt 2020 shared task on chat translation in english-german. our neural machine translation (nmt) systems are built on sentence-level document-level non-autoregressive (nat) and pretrained models. we integrate a number of advanced techniques into our systems including data selection back/forward translation larger batch learning model ensemble finetuning as well as system combination. specifically we proposed a hybrid data selection method to select high-quality and in-domain sentences from out-of-domain data. to better capture the source contexts we exploit to augment nat models with evolved cross-attention. furthermore we explore to transfer general knowledge from four different pre-training language models to the downstream translation task. in general we present extensive experimental results for this new translation task. among all the participants our german-to-english primary system is ranked the second in terms of bleu scores.
606,gero-chilton-2018-challenges,"   Challenges in Finding Metaphorical Connections"",
",poetry is known for its novel expression using figurative language. we introduce a writing task that contains the essential challenges of generating meaningful figurative language and can be evaluated. we investigate how to find metaphorical connections between abstract themes and concrete domains by asking people to write four-line poems on a given metaphor such as {``}death is a rose{''} or {``}anger is wood{''}. we find that only 21{\%} of poems successfully make a metaphorical connection. we present five alternate ways people respond to the prompt and release our dataset of 100 categorized poems. we suggest opportunities for computational approaches.
607,mohiuddin-etal-2021-rethinking,"   Rethinking Coherence Modeling: Synthetic vs. Downstream Tasks"",
",although coherence modeling has come a long way in developing novel models their evaluation on downstream applications for which they are purportedly developed has largely been neglected. with the advancements made by neural approaches in applications such as machine translation (mt) summarization and dialog systems the need for coherence evaluation of these tasks is now more crucial than ever. however coherence models are typically evaluated only on synthetic tasks which may not be representative of their performance in downstream applications. to investigate how representative the synthetic tasks are of downstream use cases we conduct experiments on benchmarking well-known traditional and neural coherence models on synthetic sentence ordering tasks and contrast this with their performance on three downstream applications: coherence evaluation for mt and summarization and next utterance prediction in retrieval-based dialog. our results demonstrate a weak correlation between the model performances in the synthetic tasks and the downstream applications motivating alternate training and evaluation methods for coherence models.
608,henia-etal-2021-icompass,"   i{C}ompass at {NLP}4{IF}-2021{--}Fighting the {COVID}-19 Infodemic"",
",this paper provides a detailed overview of the system and its outcomes which were produced as part of the nlp4if shared task on fighting the covid-19 infodemic at naacl 2021. this task is accomplished using a variety of techniques. we used state-of-the-art contextualized text representation models that were fine-tuned for the downstream task in hand. arbert marbertarabert arabic albert and bert-base-arabic were used. according to the results bert-base-arabic had the highest 0.784 f1 score on the test set.
609,sitbon-2007-combinaison,"   Combinaison de ressources linguistiques pour l{'}aide {\`a} l{'}acc{\`e}s lexical : {\'e}tude de faisabilit{\'e}"",
",cet article propose une {\'e}valuation combin{\'e}e et comparative de 5 ressources (descriptive paradigmatique et syntagmatiques) pour l{'}aide {\`a} l{'}acc{\`e}s lexical en situation de {``}mot sur le bout de la langue{''} en vue de la cr{\'e}ation d{'}un outil utilisant la combinaison de ces ressources. en situation de {``}mot sur le bout de la langue{''} l{'}utilisateur n{'}acc{\`e}de plus au mot qu{'}il veut dire ou {\'e}crire mais est capable d{'}en produire d{'}autres s{\'e}mantiquement associ{\'e}s. l{'}{\'e}valuation se base sur un corpus de 20 mots {``}sur le bout de la langue{''} pour lesquels on dispose de 50 groupes de 5 associations s{\'e}mantiques effectu{\'e}es par des utilisateurs. les r{\'e}sultats montrent que les ressources sont compl{\'e}mentaires et peu redondantes. de plus au moins une association propos{\'e}e parmi les 5 permettrait de retrouver le mot {``}sur le bout de la langue{''} dans 79{\%} des cas {\`a} condition de le s{\'e}lectionner parmi les 2500 mot potentiels. enfin les r{\'e}sultats montrent des disparit{\'e}s entre les utilisateurs ce qui permettrait de d{\'e}finir des profils d{'}utilisateur pour une am{\'e}lioration des performances.
610,raynal-2004-representation,"   Repr{\'e}sentation compositionnelle de la s{\'e}mantique de aussi"",
",l{'}objectif de notre travail est de d{\'e}gager une repr{\'e}sentation formelle compositionnelle de la contribution s{\'e}mantique de aussi lorsqu{'}il a une valeur additive. plusieurs probl{\`e}mes de compositionnalit{\'e} li{\'e}s surtout {\`a} la diversit{\'e} des arguments concern{\'e}s par l{'}adverbe vont se poser. nous proposons une alternative compositionnelle {\`a} la repr{\'e}sentation propos{\'e}e initialement en l-drt.
611,kaushik-etal-2021-cnlp,"   {CNLP}-{NITS} @ {L}ong{S}umm 2021: {T}ext{R}ank Variant for Generating Long Summaries"",
",the huge influx of published papers in the field of machine learning makes the task of summarization of scholarly documents vital not just to eliminate the redundancy but also to provide a complete and satisfying crux of the content. we participated in longsumm 2021: the $2^{nd}$ shared task on generating long summaries for scientific documents where the task is to generate long summaries for scientific papers provided by the organizers. this paper discusses our extractive summarization approach to solve the task. we used textrank algorithm with the bm25 score as a similarity function. even after being a graph-based ranking algorithm that does not require any learning textrank produced pretty decent results with minimal compute power and time. we attained $3^{rd}$ rank according to rouge-1 scores (0.5131 for f-measure and 0.5271 for recall) and performed decently as shown by the rouge-2 scores.
612,aggarwal-etal-2020-exploration,"   Exploration of Gender Differences in {COVID-19} Discourse on {R}eddit"",
",decades of research on differences in the language of men and women have established postulates about the nature of lexical topical and emotional preferences between the two genders along with their sociological underpinnings. using a novel dataset of male and female linguistic productions collected from the reddit discussion platform we further confirm existing assumptions about gender-linked affective distinctions and demonstrate that these distinctions are amplified in social media postings involving emotionally-charged discourse related to covid-19. our analysis also confirms considerable differences in topical preferences between male and female authors in pandemic-related discussions.
613,gritta-etal-2017-vancouver,"   {V}ancouver Welcomes You! Minimalist Location Metonymy Resolution"",
",named entities are frequently used in a metonymic manner. they serve as references to related entities such as people and organisations. accurate identification and interpretation of metonymy can be directly beneficial to various nlp applications such as named entity recognition and geographical parsing. until now metonymy resolution (mr) methods mainly relied on parsers taggers dictionaries external word lists and other handcrafted lexical resources. we show how a minimalist neural approach combined with a novel predicate window method can achieve competitive results on the semeval 2007 task on metonymy resolution. additionally we contribute with a new wikipedia-based mr dataset called relocar which is tailored towards locations as well as improving previous deficiencies in annotation guidelines.
614,li-etal-2017-ckip,"   {CKIP} at {IJCNLP}-2017 Task 2: Neural Valence-Arousal Prediction for Phrases"",
",ckip takes part in solving the dimensional sentiment analysis for chinese phrases (dsap) share task of ijcnlp 2017. this task calls for systems that can predict the valence and the arousal of chinese phrases which are real values between 1 and 9. to achieve this functions mapping chinese character sequences to real numbers are built by regression techniques. in addition the ckip phrase valence-arousal (va) predictor depends on knowledge of modifier words and head words. this includes the types of known modifier words va of head words and distributional semantics of both these words. the predictor took the second place out of 13 teams on phrase va prediction with 0.444 mae and 0.935 pcc on valence and 0.395 mae and 0.904 pcc on arousal.
615,ferro-etal-2019-scalable,"   Scalable Methods for Annotating Legal-Decision Corpora"",
",recent research has demonstrated that judicial and administrative decisions can be predicted by machine-learning models trained on prior decisions. however to have any practical application these predictions must be explainable which in turn requires modeling a rich set of features. such approaches face a roadblock if the knowledge engineering required to create these features is not scalable. we present an approach to developing a feature-rich corpus of administrative rulings about domain name disputes an approach which leverages a small amount of manual annotation and prototypical patterns present in the case documents to automatically extend feature labels to the entire corpus. to demonstrate the feasibility of this approach we report results from systems trained on this dataset.
616,zhou-choi-2018-exist,"   They Exist! Introducing Plural Mentions to Coreference Resolution and Entity Linking"",
",this paper analyzes arguably the most challenging yet under-explored aspect of resolution tasks such as coreference resolution and entity linking that is the resolution of plural mentions. unlike singular mentions each of which represents one entity plural mentions stand for multiple entities. to tackle this aspect we take the character identification corpus from the semeval 2018 shared task that consists of entity annotation for singular mentions and expand it by adding annotation for plural mentions. we then introduce a novel coreference resolution algorithm that selectively creates clusters to handle both singular and plural mentions and also a deep learning-based entity linking model that jointly handles both types of mentions through multi-task learning. adjusted evaluation metrics are proposed for these tasks as well to handle the uniqueness of plural mentions. our experiments show that the new coreference resolution and entity linking models significantly outperform traditional models designed only for singular mentions. to the best of our knowledge this is the first time that plural mentions are thoroughly analyzed for these two resolution tasks.
617,gimenez-perez-etal-2017-single,"   Single and Cross-domain Polarity Classification using String Kernels"",
",the polarity classification task aims at automatically identifying whether a subjective text is positive or negative. when the target domain is different from those where a model was trained we refer to a cross-domain setting. that setting usually implies the use of a domain adaptation method. in this work we study the single and cross-domain polarity classification tasks from the string kernels perspective. contrary to classical domain adaptation methods which employ texts from both domains to detect pivot features we do not use the target domain for training. our approach detects the lexical peculiarities that characterise the text polarity and maps them into a domain independent space by means of kernel discriminant analysis. experimental results show state-of-the-art performance in single and cross-domain polarity classification.
618,michon-etal-2020-integrating,"   Integrating Domain Terminology into Neural Machine Translation"",
",this paper extends existing work on terminology integration into neural machine translation a common industrial practice to dynamically adapt translation to a specific domain. our method based on the use of placeholders complemented with morphosyntactic annotation efficiently taps into the ability of the neural network to deal with symbolic knowledge to surpass the surface generalization shown by alternative techniques. we compare our approach to state-of-the-art systems and benchmark them through a well-defined evaluation framework focusing on actual application of terminology and not just on the overall performance. results indicate the suitability of our method in the use-case where terminology is used in a system trained on generic data only.
619,vandeghinste-etal-2008-evaluation,"   Evaluation of a Machine Translation System for Low Resource Languages: {METIS}-{II}"",
",in this paper we describe the metis-ii system and its evaluation on each of the language pairs: dutch german greek and spanish to english. the metis-ii system envisaged developing a data-driven approach in which no parallel corpus is required and in which no full parser or extensive rule sets are needed. we describe evalution on a development test set and on a test set coming from europarl and compare our results with systran. we also provide some further analysis researching the impact of the number and source of the reference translations and analysing the results according to test text type. the results are expectably lower for the metis system but not at an unatainable distance from a mature system like systran.
620,zhang-etal-2021-certified,"   Certified Robustness to Programmable Transformations in {LSTM}s"",
",deep neural networks for natural language processing are fragile in the face of adversarial examples{---}small input perturbations like synonym substitution or word duplication which cause a neural network to change its prediction. we present an approach to certifying the robustness of lstms (and extensions of lstms) and training models that can be efficiently certified. our approach can certify robustness to intractably large perturbation spaces defined programmatically in a language of string transformations. our evaluation shows that (1) our approach can train models that are more robust to combinations of string transformations than those produced using existing techniques; (2) our approach can show high certification accuracy of the resulting models.
621,haviv-etal-2021-latent,"   Can Latent Alignments Improve Autoregressive Machine Translation?"",
",latent alignment objectives such as ctc and axe significantly improve non-autoregressive machine translation models. can they improve autoregressive models as well? we explore the possibility of training autoregressive machine translation models with latent alignment objectives and observe that in practice this approach results in degenerate models. we provide a theoretical explanation for these empirical results and prove that latent alignment objectives are incompatible with teacher forcing.
622,cetoli-2020-exploring,"   Exploring the zero-shot limit of {F}ew{R}el"",
",this paper proposes a general purpose relation extractor that uses wikidata descriptions to represent the relation{'}s surface form. the results are tested on the fewrel 1.0 dataset which provides an excellent framework for training and evaluating the proposed zero-shot learning system in english. this relation extractor architecture exploits the implicit knowledge of a language model through a question-answering approach.
623,fortuna-etal-2021-cartography,"   Cartography of Natural Language Processing for Social Good ({NLP}4{SG}): Searching for Definitions, Statistics and White Spots"",
",the range of works that can be considered as developing nlp for social good (nlp4sg) is enormous. while many of them target the identification of hate speech or fake news there are others that address e.g. text simplification to alleviate consequences of dyslexia or coaching strategies to fight depression. however so far there is no clear picture of what areas are targeted by nlp4sg who are the actors which are the main scenarios and what are the topics that have been left aside. in order to obtain a clearer view in this respect we first propose a working definition of nlp4sg and identify some primary aspects that are crucial for nlp4sg including e.g. areas ethics privacy and bias. then we draw upon a corpus of around 50000 articles downloaded from the acl anthology. based on a list of keywords retrieved from the literature and revised in view of the task we select from this corpus articles that can be considered to be on nlp4sg according to our definition and analyze them in terms of trends along the time line etc. the result is a map of the current nlp4sg research and insights concerning the white spots on this map.
624,delli-bovi-raganato-2017-sew,"   Sew-Embed at {S}em{E}val-2017 Task 2: Language-Independent Concept Representations from a Semantically Enriched {W}ikipedia"",
",this paper describes sew-embed our language-independent approach to multilingual and cross-lingual semantic word similarity as part of the semeval-2017 task 2. we leverage the wikipedia-based concept representations developed by raganato et al. (2016) and propose an embedded augmentation of their explicit high-dimensional vectors which we obtain by plugging in an arbitrary word (or sense) embedding representation and computing a weighted average in the continuous vector space. we evaluate sew-embed with two different off-the-shelf embedding representations and report their performances across all monolingual and cross-lingual benchmarks available for the task. despite its simplicity especially compared with supervised or overly tuned approaches sew-embed achieves competitive results in the cross-lingual setting (3rd best result in the global ranking of subtask 2 score 0.56).
625,odijk-2010-clarin,"   The {CLARIN}-{NL} Project"",
",in this paper i present the clarin-nl project the dutch national project that aims to play a central role in the european clarin infrastructure not only for the preparatory phase but also for the implementation and exploitation phases. i argue that the way the clarin-nl project has been set-up can serve as an excellent example for other national clarin projects for the following reasons: (1) it is a mix between a programme and a project; (2) it offers opportunities to seriously test standards and protocols currently proposed by clarin thus providing evidence-based requirements and desiderata for the clarin infrastructure and ensuring compatibility of clarin with national data and tools; (3) it brings the intended users (humanities researchers) and the technology providers (infrastructure specialists and language and speech technology researchers) together in concrete cooperation projects with a central role for the users research questions thus ensuring that the infrastructure will provide functionality that is needed by its intended users.
626,jawahar-seddah-2019-contextualized,"   Contextualized Diachronic Word Representations"",
",diachronic word embeddings play a key role in capturing interesting patterns about how language evolves over time. most of the existing work focuses on studying corpora spanning across several decades which is understandably still not a possibility when working on social media-based user-generated content. in this work we address the problem of studying semantic changes in a large twitter corpus collected over five years a much shorter period than what is usually the norm in diachronic studies. we devise a novel attentional model based on bernoulli word embeddings that are conditioned on contextual extra-linguistic (social) features such as network spatial and socio-economic variables which are associated with twitter users as well as topic-based features. we posit that these social features provide an inductive bias that helps our model to overcome the narrow time-span regime problem. our extensive experiments reveal that our proposed model is able to capture subtle semantic shifts without being biased towards frequency cues and also works well when certain contextual features are absent. our model fits the data better than current state-of-the-art dynamic word embedding models and therefore is a promising tool to study diachronic semantic changes over small time periods.
627,levy-etal-2016-modeling,"   Modeling Extractive Sentence Intersection via Subtree Entailment"",
",sentence intersection captures the semantic overlap of two texts generalizing over paradigms such as textual entailment and semantic text similarity. despite its modeling power it has received little attention because it is difficult for non-experts to annotate. we analyze 200 pairs of similar sentences and identify several underlying properties of sentence intersection. we leverage these insights to design an algorithm that decomposes the sentence intersection task into several simpler annotation tasks facilitating the construction of a high quality dataset via crowdsourcing. we implement this approach and provide an annotated dataset of 1764 sentence intersections.
628,li-etal-2018-multi-head,"   Multi-Head Attention with Disagreement Regularization"",
",multi-head attention is appealing for the ability to jointly attend to information from different representation subspaces at different positions. in this work we introduce a disagreement regularization to explicitly encourage the diversity among multiple attention heads. specifically we propose three types of disagreement regularization which respectively encourage the subspace the attended positions and the output representation associated with each attention head to be different from other heads. experimental results on widely-used wmt14 english-german and wmt17 chinese-english translation tasks demonstrate the effectiveness and universality of the proposed approach.
629,maini-etal-2020-pool,"   Why and when should you pool? Analyzing Pooling in Recurrent Architectures"",
",pooling-based recurrent neural architectures consistently outperform their counterparts without pooling on sequence classification tasks. however the reasons for their enhanced performance are largely unexamined. in this work we examine three commonly used pooling techniques (mean-pooling max-pooling and attention and propose *max-attention* a novel variant that captures interactions among predictive tokens in a sentence. using novel experiments we demonstrate that pooling architectures substantially differ from their non-pooling equivalents in their learning ability and positional biases: (i) pooling facilitates better gradient flow than bilstms in initial training epochs and (ii) bilstms are biased towards tokens at the beginning and end of the input whereas pooling alleviates this bias. consequently we find that pooling yields large gains in low resource scenarios and instances when salient words lie towards the middle of the input. across several text classification tasks we find max-attention to frequently outperform other pooling techniques.
630,yan-etal-2020-unknown,"   Unknown Intent Detection Using {G}aussian Mixture Model with an Application to Zero-shot Intent Classification"",
",user intent classification plays a vital role in dialogue systems. since user intent may frequently change over time in many realistic scenarios unknown (new) intent detection has become an essential problem where the study has just begun. this paper proposes a semantic-enhanced gaussian mixture model (seg) for unknown intent detection. in particular we model utterance embeddings with a gaussian mixture distribution and inject dynamic class semantic information into gaussian means which enables learning more class-concentrated embeddings that help to facilitate downstream outlier detection. coupled with a density-based outlier detection algorithm seg achieves competitive results on three real task-oriented dialogue datasets in two languages for unknown intent detection. on top of that we propose to integrate seg as an unknown intent identifier into existing generalized zero-shot intent classification models to improve their performance. a case study on a state-of-the-art method recapsnet shows that seg can push the classification performance to a significantly higher level.
631,jauhiainen-etal-2021-comparing,"   Comparing Approaches to {D}ravidian Language Identification"",
",this paper describes the submissions by team hwr to the dravidian language identification (dli) shared task organized at vardial 2021 workshop. the dli training set includes 16674 youtube comments written in roman script containing code-mixed text with english and one of the three south dravidian languages: kannada malayalam and tamil. we submitted results generated using two models a naive bayes classifier with adaptive language models which has shown to obtain competitive performance in many language and dialect identification tasks and a transformer-based model which is widely regarded as the state-of-the-art in a number of nlp tasks. our first submission was sent in the closed submission track using only the training set provided by the shared task organisers whereas the second submission is considered to be open as it used a pretrained model trained with external data. our team attained shared second position in the shared task with the submission based on naive bayes. our results reinforce the idea that deep learning methods are not as competitive in language identification related tasks as they are in many other text classification tasks.
632,kobayashi-etal-2021-improving,"   Improving Neural {RST} Parsing Model with Silver Agreement Subtrees"",
",most of the previous rhetorical structure theory (rst) parsing methods are based on supervised learning such as neural networks that require an annotated corpus of sufficient size and quality. however the rst discourse treebank (rst-dt) the benchmark corpus for rst parsing in english is small due to the costly annotation of rst trees. the lack of large annotated training data causes poor performance especially in relation labeling. therefore we propose a method for improving neural rst parsing models by exploiting silver data i.e. automatically annotated data. we create large-scale silver data from an unlabeled corpus by using a state-of-the-art rst parser. to obtain high-quality silver data we extract agreement subtrees from rst trees for documents built using the rst parsers. we then pre-train a neural rst parser with the obtained silver data and fine-tune it on the rst-dt. experimental results show that our method achieved the best micro-f1 scores for nuclearity and relation at 75.0 and 63.2 respectively. furthermore we obtained a remarkable gain in the relation score 3.0 points against the previous state-of-the-art parser.
633,liu-etal-2020-towards-conversational,"   Towards Conversational Recommendation over Multi-Type Dialogs"",
",we focus on the study of conversational recommendation in the context of multi-type dialogs where the bots can proactively and naturally lead a conversation from a non-recommendation dialog (e.g. qa) to a recommendation dialog taking into account user{'}s interests and feedback. to facilitate the study of this task we create a human-to-human chinese dialog dataset durecdial (about 10k dialogs 156k utterances) where there are multiple sequential dialogs for a pair of a recommendation seeker (user) and a recommender (bot). in each dialog the recommender proactively leads a multi-type dialog to approach recommendation targets and then makes multiple recommendations with rich interaction behavior. this dataset allows us to systematically investigate different parts of the overall problem e.g. how to naturally lead a dialog how to interact with users for recommendation. finally we establish baseline results on durecdial for future studies.
634,li-etal-2020-explicit,"   Explicit Semantic Decomposition for Definition Generation"",
",definition generation which aims to automatically generate dictionary definitions for words has recently been proposed to assist the construction of dictionaries and help people understand unfamiliar texts. however previous works hardly consider explicitly modeling the {``}components{''} of definitions leading to under-specific generation results. in this paper we propose esd namely explicit semantic decomposition for definition generation which explicitly decomposes the meaning of words into semantic components and models them with discrete latent variables for definition generation. experimental results show that achieves top results on wordnet and oxford benchmarks outperforming strong previous baselines.
635,wullach-etal-2021-fight-fire,"   Fight Fire with Fire: Fine-tuning Hate Detectors using Large Samples of Generated Hate Speech"",
",automatic hate speech detection is hampered by the scarcity of labeled datasetd leading to poor generalization. we employ pretrained language models (lms) to alleviate this data bottleneck. we utilize the gpt lm for generating large amounts of synthetic hate speech sequences from available labeled examples and leverage the generated data in fine-tuning large pretrained lms on hate detection. an empirical study using the models of bert roberta and albert shows that this approach improves generalization significantly and consistently within and across data distributions. in fact we find that generating relevant labeled hate speech sequences is preferable to using out-of-domain and sometimes also within-domain human-labeled examples.
636,parikh-etal-2020-irlab-daiict,"   {IRL}ab{\_}{DAIICT} at {S}em{E}val-2020 Task 12: Machine Learning and Deep Learning Methods for Offensive Language Identification"",
",the paper describes systems that our team irlab{\_}daiict employed for shared task offenseval2020: multilingual offensive language identification in social media shared task. we conducted experiments on the english language dataset which contained weakly labelled data. there were three sub-tasks but we only participated in sub-tasks a and b. we employed machine learning techniques like logistic regression support vector machine random forest and deep learning techniques like convolutional neural network and bert. our best approach achieved a macrof1 score of 0.91 for sub-task a and 0.64 for sub-task b.
637,guo-etal-2019-kingsofts,"   Kingsoft{'}s Neural Machine Translation System for {WMT}19"",
",this paper describes the kingsoft ai lab{'}s submission to the wmt2019 news translation shared task. we participated in two language directions: english-chinese and chinese-english. for both language directions we trained several variants of transformer models using the provided parallel data enlarged with a large quantity of back-translated monolingual data. the best translation result was obtained with ensemble and reranking techniques. according to automatic metrics (bleu) our chinese-english system reached the second highest score and our english-chinese system reached the second highest score for this subtask.
638,beilharz-etal-2020-librivoxdeen,"   {L}ibri{V}ox{D}e{E}n: A Corpus for {G}erman-to-{E}nglish Speech Translation and {G}erman Speech Recognition"",
",we present a corpus of sentence-aligned triples of german audio german text and english translation based on german audio books. the speech translation data consist of 110 hours of audio material aligned to over 50k parallel sentences. an even larger dataset comprising 547 hours of german speech aligned to german text is available for speech recognition. the audio data is read speech and thus low in disfluencies. the quality of audio and sentence alignments has been checked by a manual evaluation showing that speech alignment quality is in general very high. the sentence alignment quality is comparable to well-used parallel translation data and can be adjusted by cutoffs on the automatic alignment score. to our knowledge this corpus is to date the largest resource for german speech recognition and for end-to-end german-to-english speech translation.
639,jain-etal-2021-tabpert,"   {T}ab{P}ert : An Effective Platform for Tabular Perturbation"",
",to grasp the true reasoning ability the natural language inference model should be evaluated on counterfactual data. tabpert facilitates this by generation of such counterfactual data for assessing model tabular reasoning issues. tabpert allows the user to update a table change the hypothesis change the labels and highlight rows that are important for hypothesis classification. tabpert also details the technique used to automatically produce the table as well as the strategies employed to generate the challenging hypothesis. these counterfactual tables and hypotheses as well as the metadata is then used to explore the existing model{'}s shortcomings methodically and quantitatively.
640,ein-dor-etal-2020-active,"   {A}ctive {L}earning for {BERT}: {A}n {E}mpirical {S}tudy"",
",real world scenarios present a challenge for text classification since labels are usually expensive and the data is often characterized by class imbalance. active learning (al) is a ubiquitous paradigm to cope with data scarcity. recently pre-trained nlp models and bert in particular are receiving massive attention due to their outstanding performance in various nlp tasks. however the use of al with deep pre-trained models has so far received little consideration. here we present a large-scale empirical study on active learning techniques for bert-based classification addressing a diverse set of al strategies and datasets. we focus on practical scenarios of binary text classification where the annotation budget is very small and the data is often skewed. our results demonstrate that al can boost bert performance especially in the most realistic scenario in which the initial set of labeled examples is created using keyword-based queries resulting in a biased sample of the minority class. we release our research framework aiming to facilitate future research along the lines explored here.
641,thurmair-etal-2012-large,"   Large Scale Lexical Analysis"",
",the following paper presents a lexical analysis component as implemented in the panacea project. the goal is to automatically extract lexicon entries from crawled corpora in an attempt to use corpus-based methods for high-quality linguistic text processing and to focus on the quality of data without neglecting quantitative aspects. lexical analysis has the task to assign linguistic information (like: part of speech inflectional class gender subcategorisation frame semantic properties etc.) to all parts of the input text. if tokens are ambiguous lexical analysis must provide all possible sets of annotation for later (syntactic) disambiguation be it tagging or full parsing. the paper presents an approach for assigning part-of-speech tags for german and english to large input corpora ({\textgreater} 50 mio tokens) providing a workflow which takes as input crawled corpora and provides pos-tagged lemmata ready for lexicon integration. tools include sentence splitting lexicon lookup decomposition and pos defaulting. evaluation shows that the overall error rate can be brought down to about 2{\%} if language resources are properly designed. the complete workflow is implemented as a sequence of web services integrated into the panacea platform.
642,shimorina-etal-2019-creating,"   Creating a Corpus for {R}ussian Data-to-Text Generation Using Neural Machine Translation and Post-Editing"",
",in this paper we propose an approach for semi-automatically creating a data-to-text (d2t) corpus for russian that can be used to learn a d2t natural language generation model. an error analysis of the output of an english-to-russian neural machine translation system shows that 80{\%} of the automatically translated sentences contain an error and that 53{\%} of all translation errors bear on named entities (ne). we therefore focus on named entities and introduce two post-editing techniques for correcting wrongly translated nes.
643,musil-2021-representations,"   Representations of Meaning in Neural Networks for {NLP}: a Thesis Proposal"",
",neural networks are the state-of-the-art method of machine learning for many problems in nlp. their success in machine translation and other nlp tasks is phenomenal but their interpretability is challenging. we want to find out how neural networks represent meaning. in order to do this we propose to examine the distribution of meaning in the vector space representation of words in neural networks trained for nlp tasks. furthermore we propose to consider various theories of meaning in the philosophy of language and to find a methodology that would enable us to connect these areas.
644,dai-etal-2021-syntax,"   Does syntax matter? A strong baseline for Aspect-based Sentiment Analysis with {R}o{BERT}a"",
",aspect-based sentiment analysis (absa) aiming at predicting the polarities for aspects is a fine-grained task in the field of sentiment analysis. previous work showed syntactic information e.g. dependency trees can effectively improve the absa performance. recently pre-trained models (ptms) also have shown their effectiveness on absa. therefore the question naturally arises whether ptms contain sufficient syntactic information for absa so that we can obtain a good absa model only based on ptms. in this paper we firstly compare the induced trees from ptms and the dependency parsing trees on several popular models for the absa task showing that the induced tree from fine-tuned roberta (ft-roberta) outperforms the parser-provided tree. the further analysis experiments reveal that the ft-roberta induced tree is more sentiment-word-oriented and could benefit the absa task. the experiments also show that the pure roberta-based model can outperform or approximate to the previous sota performances on six datasets across four languages since it implicitly incorporates the task-oriented syntactic information.
645,chen-etal-2021-improving-simultaneous,"   Improving Simultaneous Translation by Incorporating Pseudo-References with Fewer Reorderings"",
",simultaneous translation is vastly different from full-sentence translation in the sense that it starts translation before the source sentence ends with only a few words delay. however due to the lack of large-scale high-quality simultaneous translation datasets most such systems are still trained on conventional full-sentence bitexts. this is far from ideal for the simultaneous scenario due to the abundance of unnecessary long-distance reorderings in those bitexts. we propose a novel method that rewrites the target side of existing full-sentence corpora into simultaneous-style translation. experiments on zh$\rightarrow$en and ja$\rightarrow$en simultaneous translation show substantial improvements (up to +2.7 bleu) with the addition of these generated pseudo-references.
646,cohan-goharian-2016-revisiting,"   Revisiting Summarization Evaluation for Scientific Articles"",
",evaluation of text summarization approaches have been mostly based on metrics that measure similarities of system generated summaries with a set of human written gold-standard summaries. the most widely used metric in summarization evaluation has been the rouge family. rouge solely relies on lexical overlaps between the terms and phrases in the sentences; therefore in cases of terminology variations and paraphrasing rouge is not as effective. scientific article summarization is one such case that is different from general domain summarization (e.g. newswire data). we provide an extensive analysis of rouge{'}s effectiveness as an evaluation metric for scientific summarization; we show that contrary to the common belief rouge is not much reliable in evaluating scientific summaries. we furthermore show how different variants of rouge result in very different correlations with the manual pyramid scores. finally we propose an alternative metric for summarization evaluation which is based on the content relevance between a system generated summary and the corresponding human written summaries. we call our metric sera (summarization evaluation by relevance analysis). unlike rouge sera consistently achieves high correlations with manual scores which shows its effectiveness in evaluation of scientific article summarization.
647,chen-etal-2020-trying,"   What Are You Trying to Do? Semantic Typing of Event Processes"",
",this paper studies a new cognitively motivated semantic typing taskmulti-axis event process typing that given anevent process attempts to infer free-form typelabels describing (i) the type of action made bythe process and (ii) the type of object the pro-cess seeks to affect. this task is inspired bycomputational and cognitive studies of eventunderstanding which suggest that understand-ing processes of events is often directed by rec-ognizing the goals plans or intentions of theprotagonist(s). we develop a large dataset con-taining over 60k event processes featuring ul-tra fine-grained typing on both the action andobject type axes with very large (10{\^{}}3∼10{\^{}}4)label vocabularies. we then propose a hybridlearning frameworkp2gt which addressesthe challenging typing problem with indirectsupervision from glosses1and a joint learning-to-rank framework. as our experiments indi-catep2gtsupports identifying the intent ofprocesses as well as the fine semantic type ofthe affected object. it also demonstrates the ca-pability of handling few-shot cases and stronggeneralizability on out-of-domain processes.
648,phan-etal-2019-robust,"   Robust Representation Learning of Biomedical Names"",
",biomedical concepts are often mentioned in medical documents under different name variations (synonyms). this mismatch between surface forms is problematic resulting in difficulties pertaining to learning effective representations. consequently this has tremendous implications such as rendering downstream applications inefficacious and/or potentially unreliable. this paper proposes a new framework for learning robust representations of biomedical names and terms. the idea behind our approach is to consider and encode contextual meaning conceptual meaning and the similarity between synonyms during the representation learning process. via extensive experiments we show that our proposed method outperforms other baselines on a battery of retrieval similarity and relatedness benchmarks. moreover our proposed method is also able to compute meaningful representations for unseen names resulting in high practical utility in real-world applications.
649,emmery-etal-2017-simple,"   Simple Queries as Distant Labels for Predicting Gender on {T}witter"",
",the majority of research on extracting missing user attributes from social media profiles use costly hand-annotated labels for supervised learning. distantly supervised methods exist although these generally rely on knowledge gathered using external sources. this paper demonstrates the effectiveness of gathering distant labels for self-reported gender on twitter using simple queries. we confirm the reliability of this query heuristic by comparing with manual annotation. moreover using these labels for distant supervision we demonstrate competitive model performance on the same data as models trained on manual annotations. as such we offer a cheap extensible and fast alternative that can be employed beyond the task of gender classification.
650,ribeyre-etal-2016-accurate,"   Accurate Deep Syntactic Parsing of Graphs: The Case of {F}rench"",
",parsing predicate-argument structures in a deep syntax framework requires graphs to be predicted. argument structures represent a higher level of abstraction than the syntactic ones and are thus more difficult to predict even for highly accurate parsing models on surfacic syntax. in this paper we investigate deep syntax parsing using a french data set (ribeyre et al. 2014a). we demonstrate that the use of topologically different types of syntactic features such as dependencies tree fragments spines or syntactic paths brings a much needed context to the parser. our higher-order parsing model gaining thus up to 4 points establishes the state of the art for parsing french deep syntactic structures.
651,oprea-magdy-2020-isarcasm,"   i{S}arcasm: A Dataset of Intended Sarcasm"",
",we consider the distinction between intended and perceived sarcasm in the context of textual sarcasm detection. the former occurs when an utterance is sarcastic from the perspective of its author while the latter occurs when the utterance is interpreted as sarcastic by the audience. we show the limitations of previous labelling methods in capturing intended sarcasm and introduce the isarcasm dataset of tweets labeled for sarcasm directly by their authors. examining the state-of-the-art sarcasm detection models on our dataset showed low performance compared to previously studied datasets which indicates that these datasets might be biased or obvious and sarcasm could be a phenomenon under-studied computationally thus far. by providing the isarcasm dataset we aim to encourage future nlp research to develop methods for detecting sarcasm in text as intended by the authors of the text not as labeled under assumptions that we demonstrate to be sub-optimal.
652,dunn-2021-representations,"   Representations of Language Varieties Are Reliable Given Corpus Similarity Measures"",
",this paper measures similarity both within and between 84 language varieties across nine languages. these corpora are drawn from digital sources (the web and tweets) allowing us to evaluate whether such geo-referenced corpora are reliable for modelling linguistic variation. the basic idea is that if each source adequately represents a single underlying language variety then the similarity between these sources should be stable across all languages and countries. the paper shows that there is a consistent agreement between these sources using frequency-based corpus similarity measures. this provides further evidence that digital geo-referenced corpora consistently represent local language varieties.
653,obamuyide-etal-2021-bayesian,"   {B}ayesian Model-Agnostic Meta-Learning with Matrix-Valued Kernels for Quality Estimation"",
",most current quality estimation (qe) models for machine translation are trained and evaluated in a fully supervised setting requiring significant quantities of labelled training data. however obtaining labelled data can be both expensive and time-consuming. in addition the test data that a deployed qe model would be exposed to may differ from its training data in significant ways. in particular training samples are often labelled by one or a small set of annotators whose perceptions of translation quality and needs may differ substantially from those of end-users who will employ predictions in practice. thus it is desirable to be able to adapt qe models efficiently to new user data with limited supervision data. to address these challenges we propose a bayesian meta-learning approach for adapting qe models to the needs and preferences of each user with limited supervision. to enhance performance we further propose an extension to a state-of-the-art bayesian meta-learning approach which utilizes a matrix-valued kernel for bayesian meta-learning of quality estimation. experiments on data with varying number of users and language characteristics demonstrates that the proposed bayesian meta-learning approach delivers improved predictive performance in both limited and full supervision settings.
654,dowlagar-mamidi-2020-multilingual,"   Multilingual Pre-Trained Transformers and Convolutional {NN} Classification Models for Technical Domain Identification"",
",in this paper we present a transfer learning system to perform technical domain identification on multilingual text data. we have submitted two runs one uses the transformer model bert and the other uses xlm-roberta with the cnn model for text classification. these models allowed us to identify the domain of the given sentences for the icon 2020 shared task techdofication: technical domain identification. our system ranked the best for the subtasks 1d 1g for the given techdofication dataset.
655,goel-etal-2021-goodwill,"   Goodwill Hunting: Analyzing and Repurposing Off-the-Shelf Named Entity Linking Systems"",
",named entity linking (nel) or mapping {``}strings{''} to {``}things{''} in a knowledge base is a fundamental preprocessing step in systems that require knowledge of entities such as information extraction and question answering. in this work we lay out and investigate two challenges faced by individuals or organizations building nel systems. can they directly use an off-the-shelf system? if not how easily can such a system be repurposed for their use case? first we conduct a study of off-the-shelf commercial and academic nel systems. we find that most systems struggle to link rare entities with commercial solutions lagging their academic counterparts by 10{\%}+. second for a use case where the nel model is used in a sports question-answering (qa) system we investigate how to close the loop in our analysis by repurposing the best off-the-shelf model (bootleg) to correct sport-related errors. we show how tailoring a simple technique for patching models using weak labeling can provide a 25{\%} absolute improvement in accuracy of sport-related errors.
656,liu-etal-2020-diverse,"   Diverse, Controllable, and Keyphrase-Aware: A Corpus and Method for News Multi-Headline Generation"",
",news headline generation aims to produce a short sentence to attract readers to read the news. one news article often contains multiple keyphrases that are of interest to different users which can naturally have multiple reasonable headlines. however most existing methods focus on the single headline generation. in this paper we propose generating multiple headlines with keyphrases of user interests whose main idea is to generate multiple keyphrases of interest to users for the news first and then generate multiple keyphrase-relevant headlines. we propose a multi-source transformer decoder which takes three sources as inputs: (a) keyphrase (b) keyphrase-filtered article and (c) original article to generate keyphrase-relevant high-quality and diverse headlines. furthermore we propose a simple and effective method to mine the keyphrases of interest in the news article and build a first large-scale keyphrase-aware news headline corpus which contains over 180k aligned triples of {\textless}news article headline keyphrase{\textgreater}. extensive experimental comparisons on the real-world dataset show that the proposed method achieves state-of-the-art results in terms of quality and diversity.
657,rathinasamy-etal-2020-infosys,"   Infosys Machine Translation System for {WMT}20 Similar Language Translation Task"",
",this paper describes infosys{'}s submission to the wmt20 similar language translation shared task. we participated in indo-aryan language pair in the language direction hindi to marathi. our baseline system is byte-pair encoding based transformer model trained with the fairseq sequence modeling toolkit. our final system is an ensemble of two transformer models which ranked first in wmt20 evaluation. one model is designed to learn the nuances of translation of this low resource language pair by taking advantage of the fact that the source and target languages are same alphabet languages. the other model is the result of experimentation with the proportion of back-translated data to the parallel data to improve translation fluency.
658,zhang-etal-2021-extract-integrate,"   Extract, Integrate, Compete: Towards Verification Style Reading Comprehension"",
",in this paper we present a new verification style reading comprehension dataset named vgaokao from chinese language tests of gaokao. different from existing efforts the new dataset is originally designed for native speakers{'} evaluation thus requiring more advanced language understanding skills. to address the challenges in vgaokao we propose a novel extract-integrate-compete approach which iteratively selects complementary evidence with a novel query updating mechanism and adaptively distills supportive evidence followed by a pairwise competition to push models to learn the subtle difference among similar text pieces. experiments show that our methods outperform various baselines on vgaokao with retrieved complementary evidence while having the merits of efficiency and explainability. our dataset and code are released for further research.
659,rehm-etal-2021-european,"   {E}uropean Language Grid: A Joint Platform for the {E}uropean Language Technology Community"",
",europe is a multilingual society in which dozens of languages are spoken. the only option to enable and to benefit from multilingualism is through language technologies (lt) i.e. natural language processing and speech technologies. we describe the european language grid (elg) which is targeted to evolve into the primary platform and marketplace for lt in europe by providing one umbrella platform for the european lt landscape including research and industry enabling all stakeholders to upload share and distribute their services products and resources. at the end of our eu project which will establish a legal entity in 2022 the elg will provide access to approx. 1300 services for all european languages as well as thousands of data sets.
660,caroli-1993-types,"   Types of lexical co-occurrences: descriptive parameters"",
",in this article i will discuss different types of lexical co-occurrences and examine the requirements for representing them in a reusable lexical resource. i will focus the discussion on the delimitation of a limited set of descriptive parameters rather than on an exhaustive classification of idioms or multiword units. descriptive parameters will be derived from a detailed discussion of the problem of how to determine adequate translations for such units. criteria for determining translation equivalences between multiword units of two languages will be: the syntactic and the semantic structure as well as functional pragmatic and stylistic properties.
661,larsson-bernardy-2021-semantic,"   Semantic Classification and Learning Using a Linear Tranformation Model in a Probabilistic Type Theory with Records"",
",starting from an existing account of semantic classification and learning from interaction formulated in a probabilistic type theory with records encompassing bayesian inference and learning with a frequentist flavour we observe some problems with this account and provide an alternative account of classification learning that addresses the observed problems. the proposed account is also broadly bayesian in nature but instead uses a linear transformation model for classification and learning.
662,oliver-mikelenic-2020-resipc,"   {R}e{S}i{PC}: a Tool for Complex Searches in Parallel Corpora"",
",in this paper a tool specifically designed to allow for complex searches in large parallel corpora is presented. the formalism for the queries is very powerful as it uses standard regular expressions that allow for complex queries combining word forms lemmata and pos-tags. as queries are performed over pos-tags at least one of the languages in the parallel corpus should be pos-tagged. searches can be performed in one of the languages or in both languages at the same time. the program is able to pos-tag the corpora using the freeling analyzer through its python api. resipc is developed in python version 3 and it is distributed under a free license (gnu gpl). the tool can be used to provide data for contrastive linguistics research and an example of use in a spanish-croatian parallel corpus is presented. resipc is designed for queries in pos-tagged corpora but it can be easily adapted for querying corpora containing other kinds of information.
663,zhang-singh-2019-leveraging,"   Leveraging Structural and Semantic Correspondence for Attribute-Oriented Aspect Sentiment Discovery"",
",opinionated text often involves attributes such as authorship and location that influence the sentiments expressed for different aspects. we posit that structural and semantic correspondence is both prevalent in opinionated text especially when associated with attributes and crucial in accurately revealing its latent aspect and sentiment structure. however it is not recognized by existing approaches. we propose trait an unsupervised probabilistic model that discovers aspects and sentiments from text and associates them with different attributes. to this end trait infers and leverages structural and semantic correspondence using a markov random field. we show empirically that by incorporating attributes explicitly trait significantly outperforms state-of-the-art baselines both by generating attribute profiles that accord with our intuitions as shown via visualization and yielding topics of greater semantic cohesion.
664,saunders-etal-2020-using,"   Using Context in Neural Machine Translation Training Objectives"",
",we present neural machine translation (nmt) training using document-level metrics with batch-level documents. previous sequence-objective approaches to nmt training focus exclusively on sentence-level metrics like sentence bleu which do not correspond to the desired evaluation metric typically document bleu. meanwhile research into document-level nmt training focuses on data or model architecture rather than training procedure. we find that each of these lines of research has a clear space in it for the other and propose merging them with a scheme that allows a document-level evaluation metric to be used in the nmt training objective. we first sample pseudo-documents from sentence samples. we then approximate the expected document bleu gradient with monte carlo sampling for use as a cost function in minimum risk training (mrt). this two-level sampling procedure gives nmt performance gains over sequence mrt and maximum-likelihood training. we demonstrate that training is more robust for document-level metrics than with sequence metrics. we further demonstrate improvements on nmt with ter and grammatical error correction (gec) using gleu both metrics used at the document level for evaluations.
665,elazar-etal-2020-extraordinary,"   The Extraordinary Failure of Complement Coercion Crowdsourcing"",
",crowdsourcing has eased and scaled up the collection of linguistic annotation in recent years. in this work we follow known methodologies of collecting labeled data for the complement coercion phenomenon. these are constructions with an implied action {---} e.g. {``}i started a new book i bought last week{''} where the implied action is reading. we aim to collect annotated data for this phenomenon by reducing it to either of two known tasks: explicit completion and natural language inference. however in both cases crowdsourcing resulted in low agreement scores even though we followed the same methodologies as in previous work. why does the same process fail to yield high agreement scores? we specify our modeling schemes highlight the differences with previous work and provide some insights about the task and possible explanations for the failure. we conclude that specific phenomena require tailored solutions not only in specialized algorithms but also in data collection methods.
666,belainine-etal-2020-towards,"   Towards a Multi-Dataset for Complex Emotions Learning Based on Deep Neural Networks"",
",in sentiment analysis several researchers have used emoji and hashtags as specific forms of training and supervision. some emotions such as fear and disgust are underrepresented in the text of social media. others such as anticipation are absent. this research paper proposes a new dataset for complex emotion detection using a combination of several existing corpora in order to represent and interpret complex emotions based on the plutchik{'}s theory. our experiments and evaluations confirm that using transfer learning (tl) with a rich emotional corpus facilitates the detection of complex emotions in a four-dimensional space. in addition the incorporation of the rule on the reverse emotions in the model{'}s architecture brings a significant improvement in terms of precision recall and f-score.
667,sellam-etal-2020-bleurt,"   {BLEURT}: Learning Robust Metrics for Text Generation"",
",text generation has made significant advances in the last few years. yet evaluation metrics have lagged behind as the most popular choices (e.g. bleu and rouge) may correlate poorly with human judgment. we propose bleurt a learned evaluation metric for english based on bert. bleurt can model human judgment with a few thousand possibly biased training examples. a key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize. bleurt provides state-of-the-art results on the last three years of the wmt metrics shared task and the webnlg data set. in contrast to a vanilla bert-based approach it yields superior results even when the training data is scarce and out-of-distribution.
668,aggarwal-etal-2020-really,"   {``}Did you really mean what you said?{''} : Sarcasm Detection in {H}indi-{E}nglish Code-Mixed Data using Bilingual Word Embeddings"",
",with the increased use of social media platforms by people across the world many new interesting nlp problems have come into existence. one such being the detection of sarcasm in the social media texts. we present a corpus of tweets for training custom word embeddings and a hinglish dataset labelled for sarcasm detection. we propose a deep learning based approach to address the issue of sarcasm detection in hindi-english code mixed tweets using bilingual word embeddings derived from fasttext and word2vec approaches. we experimented with various deep learning models including cnns lstms bi-directional lstms (with and without attention). we were able to outperform all state-of-the-art performances with our deep learning models with attention based bi-directional lstms giving the best performance exhibiting an accuracy of 78.49{\%}.
669,su-yan-2017-cross,"   Cross-domain Semantic Parsing via Paraphrasing"",
",existing studies on semantic parsing mainly focus on the in-domain setting. we formulate cross-domain semantic parsing as a domain adaptation problem: train a semantic parser on some source domains and then adapt it to the target domain. due to the diversity of logical forms in different domains this problem presents unique and intriguing challenges. by converting logical forms into canonical utterances in natural language we reduce semantic parsing to paraphrasing and develop an attentive sequence-to-sequence paraphrase model that is general and flexible to adapt to different domains. we discover two problems small micro variance and large macro variance of pre-trained word embeddings that hinder their direct use in neural networks and propose standardization techniques as a remedy. on the popular overnight dataset which contains eight domains we show that both cross-domain training and standardized pre-trained word embeddings can bring significant improvement.
670,lester-etal-2020-constrained,"   Constrained Decoding for Computationally Efficient Named Entity Recognition Taggers"",
",current state-of-the-art models for named entity recognition (ner) are neural models with a conditional random field (crf) as the final layer. entities are represented as per-token labels with a special structure in order to decode them into spans. current work eschews prior knowledge of how the span encoding scheme works and relies on the crf learning which transitions are illegal and which are not to facilitate global coherence. we find that by constraining the output to suppress illegal transitions we can train a tagger with a cross-entropy loss twice as fast as a crf with differences in f1 that are statistically insignificant effectively eliminating the need for a crf. we analyze the dynamics of tag co-occurrence to explain when these constraints are most effective and provide open source implementations of our tagger in both pytorch and tensorflow.
671,cognetta-etal-2019-online,"   Online Infix Probability Computation for Probabilistic Finite Automata"",
",probabilistic finite automata (pfas) are com- mon statistical language model in natural lan- guage and speech processing. a typical task for pfas is to compute the probability of all strings that match a query pattern. an impor- tant special case of this problem is computing the probability of a string appearing as a pre- fix suffix or infix. these problems find use in many natural language processing tasks such word prediction and text error correction. recently we gave the first incremental algorithm to efficiently compute the infix probabilities of each prefix of a string (cognetta et al. 2018). we develop an asymptotic improvement of that algorithm and solve the open problem of computing the infix probabilities of pfas from streaming data which is crucial when process- ing queries online and is the ultimate goal of the incremental approach.
672,norlund-stenbom-2021-building,"   Building a {S}wedish Open-Domain Conversational Language Model"",
",we present on-going work of evaluating the to our knowledge first large generative language model trained to converse in swedish using data from the online discussion forum flashback. we conduct a human evaluation pilot study that indicates the model is often able to respond to conversations in both a human-like and informative manner on a diverse set of topics. while data from online forums can be useful to build conversational systems we reflect on the negative consequences that incautious application might have and the need for taking active measures to safeguard against them.
673,paul-etal-2018-mostly,"   A mostly unlexicalized model for recognizing textual entailment"",
",many approaches to automatically recognizing entailment relations have employed classifiers over hand engineered lexicalized features or deep learning models that implicitly capture lexicalization through word embeddings. this reliance on lexicalization may complicate the adaptation of these tools between domains. for example such a system trained in the news domain may learn that a sentence like {``}palestinians recognize texas as part of mexico{''} tends to be unsupported but this fact (and its corresponding lexicalized cues) have no value in say a scientific domain. to mitigate this dependence on lexicalized information in this paper we propose a model that reads two sentences from any given domain to determine entailment without using lexicalized features. instead our model relies on features that are either unlexicalized or are domain independent such as proportion of negated verbs antonyms or noun overlap. in its current implementation this model does not perform well on the fever dataset due to two reasons. first for the information retrieval portion of the task we used the baseline system provided since this was not the aim of our project. second this is work in progress and we still are in the process of identifying more features and gradually increasing the accuracy of our model. in the end we hope to build a generic end-to-end classifier which can be used in a domain outside the one in which it was trained with no or minimal re-training.
674,pal-zampieri-2020-neural,"   Neural Machine Translation for Similar Languages: The Case of {I}ndo-{A}ryan Languages"",
",in this paper we present the wipro-rit systems submitted to the similar language translation shared task at wmt 2020. the second edition of this shared task featured parallel data from pairs/groups of similar languages from three different language families: indo-aryan languages (hindi and marathi) romance languages (catalan portuguese and spanish) and south slavic languages (croatian serbian and slovene). we report the results obtained by our systems in translating from hindi to marathi and from marathi to hindi. wipro-rit achieved competitive performance ranking 1st in marathi to hindi and 2nd in hindi to marathi translation among 22 systems.
675,reuver-etal-2021-stance,"   Is Stance Detection Topic-Independent and Cross-topic Generalizable? - A Reproduction Study"",
",cross-topic stance detection is the task to automatically detect stances (pro against or neutral) on unseen topics. we successfully reproduce state-of-the-art cross-topic stance detection work (reimers et. al 2019) and systematically analyze its reproducibility. our attention then turns to the cross-topic aspect of this work and the specificity of topics in terms of vocabulary and socio-cultural context. we ask: to what extent is stance detection topic-independent and generalizable across topics? we compare the model{'}s performance on various unseen topics and find topic (e.g. abortion cloning) class (e.g. pro con) and their interaction affecting the model{'}s performance. we conclude that investigating performance on different topics and addressing topic-specific vocabulary and context is a future avenue for cross-topic stance detection. references nils reimers benjamin schiller tilman beck johannes daxenberger christian stab and iryna gurevych. 2019. classification and clustering of arguments with contextualized word embeddings. in proceedings of the 57th annual meeting of the association for computational linguistics pages 567{--}578 florence italy. association for computational linguistics.
676,yannakoudakis-etal-2017-neural,"   Neural Sequence-Labelling Models for Grammatical Error Correction"",
",we propose an approach to n-best list reranking using neural sequence-labelling models. we train a compositional model for error detection that calculates the probability of each token in a sentence being correct or incorrect utilising the full sentence as context. using the error detection model we then re-rank the n best hypotheses generated by statistical machine translation systems. our approach achieves state-of-the-art results on error correction for three different datasets and it has the additional advantage of only using a small set of easily computed features that require no linguistic input.
677,cercas-curry-etal-2021-convabuse,"   {C}onv{A}buse: Data, Analysis, and Benchmarks for Nuanced Abuse Detection in Conversational {AI}"",
",we present the first english corpus study on abusive language towards three conversational ai systems gathered {`}in the wild{'}: an open-domain social bot a rule-based chatbot and a task-based system. to account for the complexity of the task we take a more {`}nuanced{'} approach where our convai dataset reflects fine-grained notions of abuse as well as views from multiple expert annotators. we find that the distribution of abuse is vastly different compared to other commonly used datasets with more sexually tinted aggression towards the virtual persona of these systems. finally we report results from bench-marking existing models against this data. unsurprisingly we find that there is substantial room for improvement with f1 scores below 90{\%}.
678,l-homme-etal-2016-proposal,"   A Proposal for combining {``}general{''} and specialized frames"",
",the objectives of the work described in this paper are: 1. to list the differences between a general language resource (namely framenet) and a domain-specific resource; 2. to devise solutions to merge their contents in order to increase the coverage of the general resource. both resources are based on frame semantics (fillmore 1985; fillmore and baker 2010) and this raises specific challenges since the theoretical framework and the methodology derived from it provide for both a lexical description and a conceptual representation. we propose a series of strategies that handle both lexical and conceptual (frame) differences and implemented them in the specialized resource. we also show that most differences can be handled in a straightforward manner. however some more domain specific differences (such as frames defined exclusively for the specialized domain or relations between these frames) are likely to be much more difficult to take into account since some are domain-specific.
679,damaschk-etal-2019-multiclass,"   Multiclass Text Classification on Unbalanced, Sparse and Noisy Data"",
",this paper discusses methods to improve the performance of text classification on data that is difficult to classify due to a large number of unbalanced classes with noisy examples. a variety of features are tested in combination with three different neural-network-based methods with increasing complexity. the classifiers are applied to a songtext{--}artist dataset which is large unbalanced and noisy. we come to the conclusion that substantial improvement can be obtained by removing unbalancedness and sparsity from the data. this fulfils a classification task unsatisfactorily{---}however with contemporary methods it is a practical step towards fairly satisfactory results.
680,mao-etal-2018-end,"   End-to-End Reinforcement Learning for Automatic Taxonomy Induction"",
",we present a novel end-to-end reinforcement learning approach to automatic taxonomy induction from a set of terms. while prior methods treat the problem as a two-phase task (\textit{i.e.} detecting hypernymy pairs followed by organizing these pairs into a tree-structured hierarchy) we argue that such two-phase methods may suffer from error propagation and cannot effectively optimize metrics that capture the holistic structure of a taxonomy. in our approach the representations of term pairs are learned using multiple sources of information and used to determine \textit{which} term to select and \textit{where} to place it on the taxonomy via a policy network. all components are trained in an end-to-end manner with cumulative rewards measured by a holistic tree metric over the training taxonomies. experiments on two public datasets of different domains show that our approach outperforms prior state-of-the-art taxonomy induction methods up to 19.6{\%} on ancestor f1.
681,schwenk-douze-2017-learning,"   Learning Joint Multilingual Sentence Representations with Neural Machine Translation"",
",in this paper we use the framework of neural machine translation to learn joint sentence representations across six very different languages. our aim is that a representation which is independent of the language is likely to capture the underlying semantics. we define a new cross-lingual similarity measure compare up to 1.4m sentence representations and study the characteristics of close sentences. we provide experimental evidence that sentences that are close in embedding space are indeed semantically highly related but often have quite different structure and syntax. these relations also hold when comparing sentences in different languages.
682,wright-etal-2017-vectors,"   Vectors for Counterspeech on {T}witter"",
",a study of conversations on twitter found that some arguments between strangers led to favorable change in discourse and even in attitudes. the authors propose that such exchanges can be usefully distinguished according to whether individuals or groups take part on each side since the opportunity for a constructive exchange of views seems to vary accordingly.
683,feng-wan-2019-towards,"   Argument Mining: A Survey"",
",argument mining is the automatic identification and extraction of the structure of inference and reasoning expressed as arguments presented in natural language. understanding argumentative structure makes it possible to determine not only what positions people are adopting but also why they hold the opinions they do providing valuable insights in domains as diverse as financial market prediction and public relations. this survey explores the techniques that establish the foundations for argument mining provides a review of recent advances in argument mining techniques and discusses the challenges faced in automatically extracting a deeper understanding of reasoning expressed in language in general.
684,sims-bamman-2020-measuring,"   Measuring Information Propagation in Literary Social Networks"",
",we present the task of modeling information propagation in literature in which we seek to identify pieces of information passing from character a to character b to character c only given a description of their activity in text. we describe a new pipeline for measuring information propagation in this domain and publish a new dataset for speaker attribution enabling the evaluation of an important component of this pipeline on a wider range of literary texts than previously studied. using this pipeline we analyze the dynamics of information propagation in over 5000 works of fiction finding that information flows through characters that fill structural holes connecting different communities and that characters who are women are depicted as filling this role much more frequently than characters who are men.
685,zhou-li-2020-temporalteller,"   {T}emporal{T}eller at {S}em{E}val-2020 Task 1: Unsupervised Lexical Semantic Change Detection with Temporal Referencing"",
",this paper describes our temporalteller system for semeval task 1: unsupervised lexical semantic change detection. we develop a unified framework for the common semantic change detection pipelines including preprocessing learning word embeddings calculating vector distances and determining threshold. we also propose gamma quantile threshold to distinguish between changed and stable words. based on our system we conduct a comprehensive comparison among bert skip-gram temporal referencing and alignment-based methods. evaluation results show that skip-gram with temporal referencing achieves the best performance of 66.5{\%} classification accuracy and 51.8{\%} spearman{'}s ranking correlation.
686,hao-paul-2018-learning,"   Learning Multilingual Topics from Incomparable Corpora"",
",multilingual topic models enable crosslingual tasks by extracting consistent topics from multilingual corpora. most models require parallel or comparable training corpora which limits their ability to generalize. in this paper we first demystify the knowledge transfer mechanism behind multilingual topic models by defining an alternative but equivalent formulation. based on this analysis we then relax the assumption of training data required by most existing models creating a model that only requires a dictionary for training. experiments show that our new method effectively learns coherent multilingual topics from partially and fully incomparable corpora with limited amounts of dictionary resources.
687,khalifa-etal-2016-dalila,"   {DALILA}: The Dialectal {A}rabic Linguistic Learning Assistant"",
",dialectal arabic (da) poses serious challenges for natural language processing (nlp). the number and sophistication of tools and datasets in da are very limited in comparison to modern standard arabic (msa) and other languages. msa tools do not effectively model da which makes the direct use of msa nlp tools for handling dialects impractical. this is particularly a challenge for the creation of tools to support learning arabic as a living language on the web where authentic material can be found in both msa and da. in this paper we present the dialectal arabic linguistic learning assistant (dalila) a chrome extension that utilizes cutting-edge arabic dialect nlp research to assist learners and non-native speakers in understanding text written in either msa or da. dalila provides dialectal word analysis and english gloss corresponding to each word.
688,kocmi-etal-2018-cuni,"   {CUNI} Submissions in {WMT}18"",
",we participated in the wmt 2018 shared news translation task in three language pairs: english-estonian english-finnish and english-czech. our main focus was the low-resource language pair of estonian and english for which we utilized finnish parallel data in a simple method. we first train a {``}parent model{''} for the high-resource language pair followed by adaptation on the related low-resource language pair. this approach brings a substantial performance boost over the baseline system trained only on estonian-english parallel data. our systems are based on the transformer architecture. for the english to czech translation we have evaluated our last year models of hybrid phrase-based approach and neural machine translation mainly for comparison purposes.
689,li-etal-2018-joint,"   Joint Learning from Labeled and Unlabeled Data for Information Retrieval"",
",recently a significant number of studies have focused on neural information retrieval (ir) models. one category of works use unlabeled data to train general word embeddings based on term proximity which can be integrated into traditional ir models. the other category employs labeled data (e.g. click-through data) to train end-to-end neural ir models consisting of layers for target-specific representation learning. the latter idea accounts better for the ir task and is favored by recent research works which is the one we will follow in this paper. we hypothesize that general semantics learned from unlabeled data can complement task-specific representation learned from labeled data of limited quality and that a combination of the two is favorable. to this end we propose a learning framework which can benefit from both labeled and more abundant unlabeled data for representation learning in the context of ir. through a joint learning fashion in a single neural framework the learned representation is optimized to minimize both the supervised loss on query-document matching and the unsupervised loss on text reconstruction. standard retrieval experiments on trec collections indicate that the joint learning methodology leads to significant better performance of retrieval over several strong baselines for ir.
690,palogiannidi-etal-2016-affective,"   Affective Lexicon Creation for the {G}reek Language"",
",starting from the english affective lexicon anew (bradley and lang 1999a) we have created the first greek affective lexicon. it contains human ratings for the three continuous affective dimensions of valence arousal and dominance for 1034 words. the greek affective lexicon is compared with affective lexica in english spanish and portuguese. the lexicon is automatically expanded by selecting a small number of manually annotated words to bootstrap the process of estimating affective ratings of unknown words. we experimented with the parameters of the semantic-affective model in order to investigate their impact to its performance which reaches 85{\%} binary classification accuracy (positive vs. negative ratings). we share the greek affective lexicon that consists of 1034 words and the automatically expanded greek affective lexicon that contains 407k words.
691,zhang-kordoni-2008-robust,"   Robust Parsing with a Large {HPSG} Grammar"",
",in this paper we propose a partial parsing model which achieves robust parsing with a large hpsg grammar. constraint-based precision grammars like the hpsg grammar we are using for the experiments reported in this paper typically lack robustness especially when applied to real world texts. to maximally recover the linguistic knowledge from an unsuccessful parse a proper selection model must be used. also the efficiency challenges usually presented by the selection model must be answered. building on the work reported in (zhang et al. 2007) we further propose a new partial parsing model that splits the parsing process into two stages both of which use the bottom-up chart-based parsing algorithm. the algorithm is implemented and a preliminary experiment shows promising results.
692,nikolov-hahnloser-2019-large,"   Large-Scale Hierarchical Alignment for Data-driven Text Rewriting"",
",we propose a simple unsupervised method for extracting pseudo-parallel monolingual sentence pairs from comparable corpora representative of two different text styles such as news articles and scientific papers. our approach does not require a seed parallel corpus but instead relies solely on hierarchical search over pre-trained embeddings of documents and sentences. we demonstrate the effectiveness of our method through automatic and extrinsic evaluation on text simplification from the normal to the simple wikipedia. we show that pseudo-parallel sentences extracted with our method not only supplement existing parallel data but can even lead to competitive performance on their own.
693,chauhan-etal-2019-context,"   Context-aware Interactive Attention for Multi-modal Sentiment and Emotion Analysis"",
",in recent times multi-modal analysis has been an emerging and highly sought-after field at the intersection of natural language processing computer vision and speech processing. the prime objective of such studies is to leverage the diversified information (e.g. textual acoustic and visual) for learning a model. the effective interaction among these modalities often leads to a better system in terms of performance. in this paper we introduce a recurrent neural network based approach for the multi-modal sentiment and emotion analysis. the proposed model learns the inter-modal interaction among the participating modalities through an auto-encoder mechanism. we employ a context-aware attention module to exploit the correspondence among the neighboring utterances. we evaluate our proposed approach for five standard multi-modal affect analysis datasets. experimental results suggest the efficacy of the proposed model for both sentiment and emotion analysis over various existing state-of-the-art systems.
694,melamud-etal-2017-simple,"   A Simple Language Model based on {PMI} Matrix Approximations"",
",in this study we introduce a new approach for learning language models by training them to estimate word-context pointwise mutual information (pmi) and then deriving the desired conditional probabilities from pmi at test time. specifically we show that with minor modifications to word2vec{'}s algorithm we get principled language models that are closely related to the well-established noise contrastive estimation (nce) based language models. a compelling aspect of our approach is that our models are trained with the same simple negative sampling objective function that is commonly used in word2vec to learn word embeddings.
695,biteeva-lecocq-etal-2020-la,"   La phonotaxe du russe dans la typologie des langues : focus sur la palatalisation (This paper presents a phonotactic description of {R}ussian based on an analysis of 15,000 phonologically transcribed and syllabified lemmas)"",
",cet article pr{\'e}sente un travail de description phonotactique du russe bas{\'e} sur une analyse de 15 000 lemmes transcrits phonologiquement et syllab{\'e}s. un ensemble de donn{\'e}es quantitatives relatives aux structures syllabiques a {\'e}t{\'e} examin{\'e} dans une perspective typologique. {\`a} partir d{'}une analyse distributionnelle des segments consonantiques {\mbox{$\pm$}}pal des probabilit{\'e}s phonotactiques ont {\'e}t{\'e} estim{\'e}es. les r{\'e}sultats montrent que le russe suit globalement les tendances g{\'e}n{\'e}rales observ{\'e}es dans les langues de la base de donn{\'e}es g-ulsid (vall{\'e}e rousset {\&} rossato 2009) et mettent en {\'e}vidence des asym{\'e}tries de distribution des consonnes {\mbox{$\pm$}}pal {\`a} l{'}int{\'e}rieur de la syllabe. le fait que le syst{\`e}me consonantique du russe pr{\'e}sente une distinctivit{\'e} {\mbox{$\pm$}}pal {\'e}tendue {\`a} tous les lieux d{'}articulation semble contraindre les coccurrences entre consonne et voyelle d{'}une m{\^e}me syllabe pr{\'e}dites par la th{\'e}orie frame/content (macneilage 1998) et trouv{\'e}es dans de nombreuses langues.
696,liang-etal-2020-monolingual,"   Monolingual and Multilingual Reduction of Gender Bias in Contextualized Representations"",
",pretrained language models (plms) learn stereotypes held by humans and reflected in text from their training corpora including gender bias. when plms are used for downstream tasks such as picking candidates for a job people{'}s lives can be negatively affected by these learned stereotypes. prior work usually identifies a linear gender subspace and removes gender information by eliminating the subspace. following this line of work we propose to use densray an analytical method for obtaining interpretable dense subspaces. we show that densray performs on-par with prior approaches but provide arguments that it is more robust and provide indications that it preserves language model performance better. by applying densray to attention heads and layers of bert we show that gender information is spread across all attention heads and most of the layers. also we show that densray can obtain gender bias scores on both token and sentence levels. finally we demonstrate that we can remove bias multilingually e.g. from chinese using only english training data.
697,marion-etal-2021-structured,"   Structured Context and High-Coverage Grammar for Conversational Question Answering over Knowledge Graphs"",
",we tackle the problem of weakly-supervised conversational question answering over large knowledge graphs using a neural semantic parsing approach. we introduce a new logical form (lf) grammar that can model a wide range of queries on the graph while remaining sufficiently simple to generate supervision data efficiently. our transformer-based model takes a json-like structure as input allowing us to easily incorporate both knowledge graph and conversational contexts. this structured input is transformed to lists of embeddings and then fed to standard attention layers. we validate our approach both in terms of grammar coverage and lf execution accuracy on two publicly available datasets csqa and convquestions both grounded in wikidata. on csqa our approach increases the coverage from 80{\%} to 96.2{\%} and the lf execution accuracy from 70.6{\%} to 75.6{\%} with respect to previous state-of-the-art results. on convquestions we achieve competitive results with respect to the state-of-the-art.
698,rennes-jonsson-2021-synonym,"   Synonym Replacement based on a Study of Basic-level Nouns in {S}wedish Texts of Different Complexity"",
",basic-level terms have been described as the most important to human categorisation. they are the earliest emerging words in children{'}s language acquisition and seem to be more frequently occurring in language in general. in this article we explored the use of basic-level nouns in texts of different complexity and hypothesise that hypernyms with characteristics of basic-level words could be useful for the task of lexical simplification. we conducted two corpus studies using four different corpora two corpora of standard swedish and two corpora of simple swedish and explored whether corpora of simple texts contain a higher proportion of basic-level nouns than corpora of standard swedish. based on insights from the corpus studies we developed a novel algorithm for choosing the best synonym by rewarding high relative frequencies and monolexemity and restricting the climb in the word hierarchy not to suggest synonyms of a too high level of inclusiveness.
699,fischer-etal-2020-royal,"   The Royal Society Corpus 6.0: Providing 300+ Years of Scientific Writing for Humanistic Study"",
",we present a new extended version of the royal society corpus (rsc) a diachronic corpus of scientific english now covering 300+ years of scientific writing (1665--1996). the corpus comprises 47 837 texts primarily scientific articles and is based on publications of the royal society of london mainly its philosophical transactions and proceedings. the corpus has been built on the basis of the fair principles and is freely available under a creative commons license excluding copy-righted parts. we provide information on how the corpus can be found the file formats available for download as well as accessibility via a web-based corpus query platform. we show a number of analytic tools that we have implemented for better usability and provide an example of use of the corpus for linguistic analysis as well as examples of subsequent external uses of earlier releases. we place the rsc against the background of existing english diachronic/scientific corpora elaborating on its value for linguistic and humanistic study.
700,klimek-etal-2016-creating,"   Creating Linked Data Morphological Language Resources with {MM}o{O}n - The {H}ebrew Morpheme Inventory"",
",the development of standard models for describing general lexical resources has led to the emergence of numerous lexical datasets of various languages in the semantic web. however equivalent models covering the linguistic domain of morphology do not exist. as a result there are hardly any language resources of morphemic data available in rdf to date. this paper presents the creation of the hebrew morpheme inventory from a manually compiled tabular dataset comprising around 52.000 entries. it is an ongoing effort of representing the lexemes word-forms and morphologigal patterns together with their underlying relations based on the newly created multilingual morpheme ontology (mmoon). it will be shown how segmented hebrew language data can be granularly described in a linked data format thus serving as an exemplary case for creating morpheme inventories of any inflectional language with mmoon. the resulting dataset is described a) according to the structure of the underlying data format b) with respect to the hebrew language characteristic of building word-forms directly from roots c) by exemplifying how inflectional information is realized and d) with regard to its enrichment with external links to sense resources.
701,baly-etal-2018-integrating,"   Integrating Stance Detection and Fact Checking in a Unified Corpus"",
",a reasonable approach for fact checking a claim involves retrieving potentially relevant documents from different sources (e.g. news websites social media etc.) determining the stance of each document with respect to the claim and finally making a prediction about the claim{'}s factuality by aggregating the strength of the stances while taking the reliability of the source into account. moreover a fact checking system should be able to explain its decision by providing relevant extracts (rationales) from the documents. yet this setup is not directly supported by existing datasets which treat fact checking document retrieval source credibility stance detection and rationale extraction as independent tasks. in this paper we support the interdependencies between these tasks as annotations in the same corpus. we implement this setup on an arabic fact checking corpus the first of its kind.
702,uresova-etal-2018-synonymy,"   Synonymy in Bilingual Context: The {C}z{E}ng{C}lass Lexicon"",
",this paper describes czengclass a bilingual lexical resource being built to investigate verbal synonymy in bilingual context and to relate semantic roles common to one synonym class to verb arguments (verb valency). in addition the resource is linked to existing resources with the same of a similar aim: english and czech wordnet framenet propbank verbnet (semlink) and valency lexicons for czech and english (pdt-vallex vallex and engvallex). there are several goals of this work and resource: (a) to provide gold standard data for automatic experiments in the future (such as automatic discovery of synonym classes word sense disambiguation assignment of classes to occurrences of verbs in text coreferential linking of verb and event arguments in text etc.) (b) to build a core (bilingual) lexicon linked to existing resources for comparative studies and possibly for training automatic tools and (c) to enrich the annotation of a parallel treebank the prague czech english dependency treebank which so far contained valency annotation but has not linked synonymous senses of verbs together. the method used for extracting the synonym classes is a semi-automatic process with a substantial amount of manual work during filtering role assignment to classes and individual class members{'} arguments and linking to the external lexical resources. we present the first version with 200 classes (about 1800 verbs) and evaluate interannotator agreement using several metrics.
703,helcl-etal-2019-cuni,"   {CUNI} System for the {WMT}19 Robustness Task"",
",we present our submission to the wmt19 robustness task. our baseline system is the charles university (cuni) transformer system trained for the wmt18 shared task on news translation. quantitative results show that the cuni transformer system is already far more robust to noisy input than the lstm-based baseline provided by the task organizers. we further improved the performance of our model by fine-tuning on the in-domain noisy data without influencing the translation quality on the news domain.
704,schaden-jekosch-2006-casselberveetovallarga,"   {``}Casselberveetovallarga{''} and other Unpronounceable Places: The {C}ross{T}owns Corpus"",
",this paper presents a corpus of non-native speech that contains pronunciation variants of european city names from fivecountries spoken by speakers of four native languages. it was originally designed as a research tool for the study ofpronunciation errors by non-native speakers in the pronunciation of foreign city names. the corpus has now been released. followinga brief sketch of the research context in which this data collection was established the first part of this paper describes the contents and technical specifications of the corpus (design speakers language material recording conditions).compared to corpora of native speech non-native speech compilations raise a number of additional difficulties that requirespecific attention and methodology. therefore the second part of the paper aims to point out some of these general issuesfrom the perspective of the experience gained in our research. strategies to deal with these difficulties will be exploredalong with their specific benefits and shortfalls concluding that non-native speech corpora require a number of specificdesign guidelines which are often difficult to put into practice.
705,wang-etal-2020-answer-better,"   No Answer is Better Than Wrong Answer: A Reflection Model for Document Level Machine Reading Comprehension"",
",the natural questions (nq) benchmark set brings new challenges to machine reading comprehension: the answers are not only at different levels of granularity (long and short) but also of richer types (including no-answer yes/no single-span and multi-span). in this paper we target at this challenge and handle all answer types systematically. in particular we propose a novel approach called reflection net which leverages a two-step training procedure to identify the no-answer and wrong-answer cases. extensive experiments are conducted to verify the effectiveness of our approach. at the time of paper writing (may. 20 2020) our approach achieved the top 1 on both long and short answer leaderboard with f1 scores of 77.2 and 64.1 respectively.
706,ponomareva-etal-2017-automated,"   Automated Word Stress Detection in {R}ussian"",
",in this study we address the problem of automated word stress detection in russian using character level models and no part-speech-taggers. we use a simple bidirectional rnn with lstm nodes and achieve accuracy of 90{\%} or higher. we experiment with two training datasets and show that using the data from an annotated corpus is much more efficient than using only a dictionary since it allows to retain the context of the word and its morphological features.
707,chen-etal-2020-label,"   Label Representations in Modeling Classification as Text Generation"",
",several recent state-of-the-art transfer learning methods model classification tasks as text generation where labels are represented as strings for the model to generate. we investigate the effect that the choice of strings used to represent labels has on how effectively the model learns the task. for four standard text classification tasks we design a diverse set of possible string representations for labels ranging from canonical label definitions to random strings. we experiment with t5 on these tasks varying the label representations as well as the amount of training data. we find that in the low data setting label representation impacts task performance on some tasks with task-related labels being most effective but fails to have an impact on others. in the full data setting our results are largely negative: different label representations do not affect overall task performance.
708,toledo-ronen-etal-2018-learning,"   Learning Sentiment Composition from Sentiment Lexicons"",
",sentiment composition is a fundamental sentiment analysis problem. previous work relied on manual rules and manually-created lexical resources such as negator lists or learned a composition function from sentiment-annotated phrases or sentences. we propose a new approach for learning sentiment composition from a large unlabeled corpus which only requires a word-level sentiment lexicon for supervision. we automatically generate large sentiment lexicons of bigrams and unigrams from which we induce a set of lexicons for a variety of sentiment composition processes. the effectiveness of our approach is confirmed through manual annotation as well as sentiment classification experiments with both phrase-level and sentence-level benchmarks.
709,sojat-etal-2012-generation,"   Generation of Verbal Stems in Derivationally Rich Language"",
",the paper presents a procedure for generating prefixed verbs in croatian comprising combinations of one two or three prefixes. the result of this generation process is a pool of derivationally valid prefixed verbs although not necessarily occuring in corpora. the statistics of occurences of generated verbs in croatian national corpus has been calculated. further usage of such language resource with generated potential verbs is also suggested namely enrichment of croatian morphological lexicon croatian wordnet and crovallex.
710,kondratyuk-2019-cross,"   Cross-Lingual Lemmatization and Morphology Tagging with Two-Stage Multilingual {BERT} Fine-Tuning"",
",we present our charles-saarland system for the sigmorphon 2019 shared task on crosslinguality and context in morphology in task 2 morphological analysis and lemmatization in context. we leverage the multilingual bert model and apply several fine-tuning strategies introduced by udify demonstrating exceptional evaluation performance on morpho-syntactic tasks. our results show that fine-tuning multilingual bert on the concatenation of all available treebanks allows the model to learn cross-lingual information that is able to boost lemmatization and morphology tagging accuracy over fine-tuning it purely monolingually. unlike udify however we show that when paired with additional character-level and word-level lstm layers a second stage of fine-tuning on each treebank individually can improve evaluation even further. out of all submissions for this shared task our system achieves the highest average accuracy and f1 score in morphology tagging and places second in average lemmatization accuracy.
711,karamcheti-etal-2017-tale,"   A Tale of Two {DRAGGN}s: A Hybrid Approach for Interpreting Action-Oriented and Goal-Oriented Instructions"",
",robots operating alongside humans in diverse stochastic environments must be able to accurately interpret natural language commands. these instructions often fall into one of two categories: those that specify a goal condition or target state and those that specify explicit actions or how to perform a given task. recent approaches have used reward functions as a semantic representation of goal-based commands which allows for the use of a state-of-the-art planner to find a policy for the given task. however these reward functions cannot be directly used to represent action-oriented commands. we introduce a new hybrid approach the deep recurrent action-goal grounding network (draggn) for task grounding and execution that handles natural language from either category as input and generalizes to unseen environments. our robot-simulation results demonstrate that a system successfully interpreting both goal-oriented and action-oriented task specifications brings us closer to robust natural language understanding for human-robot interaction.
712,chen-etal-2018-feature,"   Feature Engineering for Second Language Acquisition Modeling"",
",knowledge tracing serves as a keystone in delivering personalized education. however few works attempted to model students{'} knowledge state in the setting of second language acquisition. the duolingo shared task on second language acquisition modeling provides students{'} trace data that we extensively analyze and engineer features from for the task of predicting whether a student will correctly solve a vocabulary exercise. our analyses of students{'} learning traces reveal that factors like exercise format and engagement impact their exercise performance to a large extent. overall we extracted 23 different features as input to a gradient tree boosting framework which resulted in an auc score of between 0.80 and 0.82 on the official test set.
713,pouran-ben-veyseh-etal-2020-graph,"   Graph Transformer Networks with Syntactic and Semantic Structures for Event Argument Extraction"",
",the goal of event argument extraction (eae) is to find the role of each entity mention for a given event trigger word. it has been shown in the previous works that the syntactic structures of the sentences are helpful for the deep learning models for eae. however a major problem in such prior works is that they fail to exploit the semantic structures of the sentences to induce effective representations for eae. consequently in this work we propose a novel model for eae that exploits both syntactic and semantic structures of the sentences with the graph transformer networks (gtns) to learn more effective sentence structures for eae. in addition we introduce a novel inductive bias based on information bottleneck to improve generalization of the eae models. extensive experiments are performed to demonstrate the benefits of the proposed model leading to state-of-the-art performance for eae on standard datasets.
714,li-etal-2020-exploring-role,"   Exploring the Role of Argument Structure in Online Debate Persuasion"",
",online debate forums provide users a platform to express their opinions on controversial topics while being exposed to opinions from diverse set of viewpoints. existing work in natural language processing (nlp) has shown that linguistic features extracted from the debate text and features encoding the characteristics of the audience are both critical in persuasion studies. in this paper we aim to further investigate the role of discourse structure of the arguments from online debates in their persuasiveness. in particular we use the factor graph model to obtain features for the argument structure of debates from an online debating platform and incorporate these features to an lstm-based model to predict the debater that makes the most convincing arguments. we find that incorporating argument structure features play an essential role in achieving the best predictive performance in assessing the persuasiveness of the arguments on online debates.
715,cinkova-etal-2012-database,"   A database of semantic clusters of verb usages"",
",we are presenting vps-30-en a small lexical resource that contains the following 30 english verbs: access ally arrive breathe claim cool crush cry deny enlarge enlist forge furnish hail halt part plough plug pour say smash smell steer submit swell tell throw trouble wake and yield. we have created and have been using vps-30-en to explore the interannotator agreement potential of the corpus pattern analysis. vps-30-en is a small snapshot of the pattern dictionary of english verbs (hanks and pustejovsky 2005) which we revised (both the entries and the annotated concordances) and enhanced with additional annotations. it is freely available at http://ufal.mff.cuni.cz/spr. in this paper we compare the annotation scheme of vps-30-en with the original pdev. we also describe the adjustments we have made and their motivation as well as the most pervasive causes of interannotator disagreements.
716,fontenelle-2006-les,"   Les nouveaux outils de correction linguistique de {M}icrosoft"",
",de nouveaux outils de correction linguistique sont disponibles pour le fran{\c{c}}ais depuis quelques mois. mis {\`a} la disposition des utilisateurs de microsoft office 2003 un nouveau correcteur orthographique et un nouveau correcteur grammatical permettent d{'}am{\'e}liorer le processus de r{\'e}daction de documents. en partant d{'}{\'e}valuations externes effectu{\'e}es r{\'e}cemment nous pr{\'e}sentons les diverses facettes de ces am{\'e}liorations et de ces outils en abordant la question de l{'}{\'e}valuation des outils de correction linguistique (qu{'}{\'e}valuer ? quels crit{\`e}res appliquer ? pourquoi d{\'e}velopper une nouvelle version ?). la r{\'e}forme de l{'}orthographe la f{\'e}minisation des noms de m{\'e}tier l{'}{\'e}volution de la langue figurent parmi les th{\`e}mes abord{\'e}s dans cet article.
717,prolo-2018-towards,"   Towards a Language for Natural Language Treebank Transductions"",
",this paper describes a transduction language suitable for natural language treebank transformations and motivates its application to tasks that have been used and described in the literature. the language which is the basis for a tree transduction tool allows for clean precise and concise description of what has been very confusingly ambiguously and incompletely textually described in the literature also allowing easy non-hard-coded implementation. we also aim at getting feedback from the nlp community to eventually converge to a de facto standard for such transduction language.
718,prasad-etal-2021-effectiveness,"   The Effectiveness of Intermediate-Task Training for Code-Switched Natural Language Understanding"",
",while recent benchmarks have spurred a lot of new work on improving the generalization of pretrained multilingual language models on multilingual tasks techniques to improve code-switched natural language understanding tasks have been far less explored. in this work we propose the use of \textit{bilingual intermediate pretraining} as a reliable technique to derive large and consistent performance gains using code-switched text on three different nlp tasks: natural language inference (nli) question answering (qa) and sentiment analysis (sa). we show consistent performance gains on four different code-switched language-pairs (hindi-english spanish-english tamil-english and malayalam-english) for sa and on hindi-english for nli and qa. we also present a code-switched masked language modeling (mlm) pretraining technique that consistently benefits sa compared to standard mlm pretraining using real code-switched text.
719,littell-etal-2014-morphological,"   Morphological parsing of {S}wahili using crowdsourced lexical resources"",
",we describe a morphological analyzer for the swahili language written in an extension of xfst/lexc intended for the easy declaration of morphophonological patterns and importation of lexical resources. our analyzer was supplemented extensively with data from the kamusi project (kamusi.org) a user-contributed multilingual dictionary. making use of this resource allowed us to achieve wide lexical coverage quickly but the heterogeneous nature of user-contributed content also poses some challenges when adapting it for use in an expert system.
720,ji-etal-2020-dilated,"   Dilated Convolutional Attention Network for Medical Code Assignment from Clinical Text"",
",medical code assignment which predicts medical codes from clinical texts is a fundamental task of intelligent medical information systems. the emergence of deep models in natural language processing has boosted the development of automatic assignment methods. however recent advanced neural architectures with flat convolutions or multi-channel feature concatenation ignore the sequential causal constraint within a text sequence and may not learn meaningful clinical text representations especially for lengthy clinical notes with long-term sequential dependency. this paper proposes a dilated convolutional attention network (dcan) integrating dilated convolutions residual connections and label attention for medical code assignment. it adopts dilated convolutions to capture complex medical patterns with a receptive field which increases exponentially with dilation size. experiments on a real-world clinical dataset empirically show that our model improves the state of the art.
721,artetxe-schwenk-2019-margin,"   Margin-based Parallel Corpus Mining with Multilingual Sentence Embeddings"",
",machine translation is highly sensitive to the size and quality of the training data which has led to an increasing interest in collecting and filtering large parallel corpora. in this paper we propose a new method for this task based on multilingual sentence embeddings. in contrast to previous approaches which rely on nearest neighbor retrieval with a hard threshold over cosine similarity our proposed method accounts for the scale inconsistencies of this measure considering the margin between a given sentence pair and its closest candidates instead. our experiments show large improvements over existing methods. we outperform the best published results on the bucc mining task and the un reconstruction task by more than 10 f1 and 30 precision points respectively. filtering the english-german paracrawl corpus with our approach we obtain 31.2 bleu points on newstest2014 an improvement of more than one point over the best official filtered version.
722,lee-etal-2020-reference,"   Reference and Document Aware Semantic Evaluation Methods for {K}orean Language Summarization"",
",text summarization refers to the process that generates a shorter form of text from the source document preserving salient information. many existing works for text summarization are generally evaluated by using recall-oriented understudy for gisting evaluation (rouge) scores. however as rouge scores are computed based on n-gram overlap they do not reflect semantic meaning correspondences between generated and reference summaries. because korean is an agglutinative language that combines various morphemes into a word that express several meanings rouge is not suitable for korean summarization. in this paper we propose evaluation metrics that reflect semantic meanings of a reference summary and the original document reference and document aware semantic score (rdass). we then propose a method for improving the correlation of the metrics with human judgment. evaluation results show that the correlation with human judgment is significantly higher for our evaluation metrics than for rouge scores.
723,ju-etal-2021-leveraging-information,"   Leveraging Information Bottleneck for Scientific Document Summarization"",
",this paper presents an unsupervised extractive approach to summarize scientific long documents based on the information bottleneck principle. inspired by previous work which uses the information bottleneck principle for sentence compression we extend it to document level summarization with two separate steps. in the first step we use signal(s) as queries to retrieve the key content from the source document. then a pre-trained language model conducts further sentence search and edit to return the final extracted summaries. importantly our work can be flexibly extended to a multi-view framework by different signals. automatic evaluation on three scientific document datasets verifies the effectiveness of the proposed framework. the further human evaluation suggests that the extracted summaries cover more content aspects than previous systems.
724,espla-gomis-etal-2020-bicleaner,"   Bicleaner at {WMT} 2020: {U}niversitat d{'}Alacant-Prompsit{'}s submission to the parallel corpus filtering shared task"",
",this paper describes the joint submission of universitat d{'}alacant and prompsit language engineering to the wmt 2020 shared task on parallel corpus filtering. our submission based on the free/open-source tool bicleaner enhances it with extremely randomised trees and lexical similarity features that account for the frequency of the words in the parallel sentences to determine if two sentences are parallel. to train this classifier we used the clean corpora provided for the task and synthetic noisy parallel sentences. in addition we re-score the output of bicleaner using character-level language models and n-gram saturation.
725,lamercerie-2018-analyse,"   Analyse formelle d{'}exigences en langue naturelle pour la conception de syst{\`e}mes cyber-physiques (Formal analysis of natural language requirements for the design of cyber-physical systems )"",
",cet article explore la construction de repr{\'e}sentations formelles d{'}{\'e}nonc{\'e}s en langue naturelle. le passage d{'}un langage naturel {\`a} une repr{\'e}sentation logique est r{\'e}alis{\'e} avec un formalisme grammatical reliant l{'}analyse syntaxique de l{'}{\'e}nonc{\'e} {\`a} une repr{\'e}sentation s{\'e}mantique. nous ciblons l{'}aspect comportemental des cahiers des charges pour les syst{\`e}mes cyber-physiques c{'}est-{\`a}-dire tout type de syst{\`e}mes dans lesquels des composants logiciels interagissent {\'e}troitement avec un environnement physique. dans ce cadre l{'}enjeu serait d{'}apporter une aide au concepteur. il s{'}agit de permettre de simuler et v{\'e}rifier par des m{\'e}thodes automatiques ou assist{\'e}es des cahiers des charges {``}syst{\`e}mes{''} exprim{\'e}s en langue naturelle. cet article pr{\'e}sente des solutions existantes qui pourraient {\^e}tre combin{\'e}es en vue de la r{\'e}solution de la probl{\'e}matique expos{\'e}e.
726,jongejan-2012-automatic,"   Automatic annotation of head velocity and acceleration in Anvil"",
",we describe an automatic face tracker plugin for the anvil annotation tool. the face tracker produces data for velocity and for acceleration in two dimensions. we compare annotations generated by the face tracking algorithm with independently made manual annotations for head movements. the annotations are a useful supplement to manual annotations and may help human annotators to quickly and reliably determine onset of head movements and to suggest which kind of head movement is taking place.
727,bennani-smires-etal-2018-simple,"   Simple Unsupervised Keyphrase Extraction using Sentence Embeddings"",
",keyphrase extraction is the task of automatically selecting a small set of phrases that best describe a given free text document. supervised keyphrase extraction requires large amounts of labeled training data and generalizes very poorly outside the domain of the training data. at the same time unsupervised systems have poor accuracy and often do not generalize well as they require the input document to belong to a larger corpus also given as input. addressing these drawbacks in this paper we tackle keyphrase extraction from single documents with embedrank: a novel unsupervised method that leverages sentence embeddings. embedrank achieves higher f-scores than graph-based state of the art systems on standard datasets and is suitable for real-time processing of large amounts of web data. with embedrank we also explicitly increase coverage and diversity among the selected keyphrases by introducing an embedding-based maximal marginal relevance (mmr) for new phrases. a user study including over 200 votes showed that although reducing the phrases{'} semantic overlap leads to no gains in f-score our high diversity selection is preferred by humans.
728,meunier-etal-2020-la,"   La mobilisation du tractus vocal est-elle variable selon les langues en parole spontan{\'e}e ? (Does vocal tract use depend on language characteristics in spontaneous speech?)"",
",l{'}objectif de ce travail est de quantifier les positions articulatoires th{\'e}oriques lors de la production de la parole spontan{\'e}e dans trois langues. chaque langue dispose d{'}un inventaire phonologique sp{\'e}cifique. mais ces sp{\'e}cificit{\'e}s ne sont pas repr{\'e}sent{\'e}es telles quelles en parole spontan{\'e}e dans laquelle les phon{\`e}mes n{'}ont pas tous la m{\^e}me fr{\'e}quence d{'}apparition. nous avons compar{\'e} trois langues (polonais fran{\c{c}}ais et anglais am{\'e}ricain) pr{\'e}sentant des diff{\'e}rences notables dans leur inventaire phonologique. des positions articulatoires ont {\'e}t{\'e} calcul{\'e}es sur la base des fr{\'e}quences des phon{\`e}mes dans chacune des trois langues dans des corpus de parole spontan{\'e}e. etonnamment les r{\'e}sultats tendent {\`a} montrer que les positions articulatoires majoritaires sont tr{\`e}s similaires dans les trois langues. il semble ainsi que l{'}usage de la parole spontan{\'e}e et donc la distribution des phon{\`e}mes dans les langues gomme les disparit{\'e}s des syst{\`e}mes phonologiques pour tendre vers une mobilisation articulatoire commune. des investigations plus approfondies devront v{\'e}rifier cette observation.
729,lim-etal-2020-annotating,"   Annotating and Analyzing Biased Sentences in News Articles using Crowdsourcing"",
",the spread of biased news and its consumption by the readers has become a considerable issue. researchers from multiple domains including social science and media studies have made efforts to mitigate this media bias issue. specifically various techniques ranging from natural language processing to machine learning have been used to help determine news bias automatically. however due to the lack of publicly available datasets in this field especially ones containing labels concerning bias on a fine-grained level (e.g. on sentence level) it is still challenging to develop methods for effectively identifying bias embedded in new articles. in this paper we propose a novel news bias dataset which facilitates the development and evaluation of approaches for detecting subtle bias in news articles and for understanding the characteristics of biased sentences. our dataset consists of 966 sentences from 46 english-language news articles covering 4 different events and contains labels concerning bias on the sentence level. for scalability reasons the labels were obtained based on crowd-sourcing. our dataset can be used for analyzing news bias as well as for developing and evaluating methods for news bias detection. it can also serve as resource for related researches including ones focusing on fake news detection.
730,raghavan-etal-2021-emrkbqa,"   emr{KBQA}: A Clinical Knowledge-Base Question Answering Dataset"",
",we present emrkbqa a dataset for answering physician questions from a structured patient record. it consists of questions logical forms and answers. the questions and logical forms are generated based on real-world physician questions and are slot-filled and answered from patients in the mimic-iii kb through a semi-automated process. this community-shared release consists of over 940000 question logical form and answer triplets with 389 types of questions and {\textasciitilde}7.5 paraphrases per question type. we perform experiments to validate the quality of the dataset and set benchmarks for question to logical form learning that helps answer questions on this dataset.
731,muhonen-purtonen-2012-rule,"   Rule-Based Detection of Clausal Coordinate Ellipsis"",
",with our experiment we show how we can detect and annotate clausal coordinate ellipsis with constraint grammar rules. we focus on such an elliptical structure in which there are two coordinated clauses and the latter one lacks a verb. for example the sentence this belongs to me and that to you demonstrates the ellipsis in question namely gapping. the constraint grammar rules are made for a finnish parsebank finntreebank. the finntreebank project is building a parsebank in the dependency syntactic framework in which verbs are central since other sentence elements depend on them. without correct detection of omitted verbs the syntactic analysis of the whole sentence fails. in the experiment we detect gapping based on morphology and linear order of the words without using syntactic or semantic information. the test corpus finnish wikipedia is morphologically analyzed but not disambiguated. even with an ambiguous morphological analysis the results show that 899{\%} of the detected sentences are elliptical making the rules accurate enough to be used in the creation of finntreebank. once we have a morphologically disambiguated corpus we can write more accurate rules and expect better results.
732,schmid-1997-parsing,"   Parsing by Successive Approximation"",
",it is proposed to parse feature structure-based grammars in several steps. each step is aimed to eliminate as many invalid analyses as possible as efficiently as possible. to this end the set of feature constraints is divided into three subsets a set of context-free constraints a set of filtering constraints and a set of structure-building constraints which are solved in that order. the best processing strategy differs: context-free constraints are solved efficiently with one of the well-known algorithms for context-free parsing. filtering constraints can be solved using unification algorithms for non-disjunctive feature structures whereas structure-building constraints require special techniques to represent feature structures with embedded disjunctions efficiently. a compilation method and an efficient processing strategy for filtering constraints are presented.
733,papageorgiou-etal-2006-adding,"   Adding multi-layer semantics to the {G}reek Dependency Treebank"",
",in this paper we give an overview of the approach adopted to add a layer of semantic information to the greek dependency treebank [gdt]. our ultimate goal is to come up with a large corpus reliably annotated with rich semantic structures. to this end a corpus has been compiled encompassing various data sources and domains. this collection has been preprocessed annotated and validated on the basis of dependency representation. taking into account multi-layered annotation schemes designed to provide deeper representations of structure and meaning we describe the methodology followed as regards the semantic layer we report on the annotation process and the problems faced and we conclude with comments on future work and exploitation of the resulting resource.
734,prakash-babu-eswari-2020-cia,"   {CIA}{\_}{NITT} at {WNUT}-2020 Task 2: Classification of {COVID}-19 Tweets Using Pre-trained Language Models"",
",this paper presents our models for wnut2020 shared task2. the shared task2 involves identification of covid-19 related informative tweets. we treat this as binary text clas-sification problem and experiment with pre-trained language models. our first model which is based on ct-bert achieves f1-scoreof 88.7{\%} and second model which is an ensemble of ct-bert roberta and svm achieves f1-score of 88.52{\%}.
735,eckart-etal-2012-influence,"   The Influence of Corpus Quality on Statistical Measurements on Language Resources"",
",the quality of statistical measurements on corpora is strongly related to a strict definition of the measuring process and to corpus quality. in the case of multiple result inspections an exact measurement of previously specified parameters ensures compatibility of the different measurements performed by different researchers on possibly different objects. hence the comparison of different values requires an exact description of the measuring process. to illustrate this correlation the influence of different definitions for the concepts ''''''``word'''''''' and ''''''``sentence'''''''' is shown for several properties of large text corpora. it is also shown that corpus pre-processing strongly influences corpus size and quality as well. as an example near duplicate sentences are identified as source of many statistical irregularities. the problem of strongly varying results especially holds for web corpora with a large set of pre-processing steps. here a well-defined and language independent pre-processing is indispensable for language comparison based on measured values. conversely irregularities found in such measurements are often a result of poor pre-processing and therefore such measurements can help to improve corpus quality.
736,alzantot-etal-2018-generating,"   Generating Natural Language Adversarial Examples"",
",deep neural networks (dnns) are vulnerable to adversarial examples perturbations to correctly classified examples which can cause the model to misclassify. in the image domain these perturbations can often be made virtually indistinguishable to human perception causing humans and state-of-the-art models to disagree. however in the natural language domain small perturbations are clearly perceptible and the replacement of a single word can drastically alter the semantics of the document. given these challenges we use a black-box population-based optimization algorithm to generate semantically and syntactically similar adversarial examples that fool well-trained sentiment analysis and textual entailment models with success rates of 97{\%} and 70{\%} respectively. we additionally demonstrate that 92.3{\%} of the successful sentiment analysis adversarial examples are classified to their original label by 20 human annotators and that the examples are perceptibly quite similar. finally we discuss an attempt to use adversarial training as a defense but fail to yield improvement demonstrating the strength and diversity of our adversarial examples. we hope our findings encourage researchers to pursue improving the robustness of dnns in the natural language domain.
737,cercas-curry-rieser-2019-crowd,"   A Crowd-based Evaluation of Abuse Response Strategies in Conversational Agents"",
",how should conversational agents respond to verbal abuse through the user? to answer this question we conduct a large-scale crowd-sourced evaluation of abuse response strategies employed by current state-of-the-art systems. our results show that some strategies such as {``}polite refusal{''} score highly across the board while for other strategies demographic factors such as age as well as the severity of the preceding abuse influence the user{'}s perception of which response is appropriate. in addition we find that most data-driven models lag behind rule-based or commercial systems in terms of their perceived appropriateness.
738,cano-bojar-2019-efficiency,"   Efficiency Metrics for Data-Driven Models: A Text Summarization Case Study"",
",using data-driven models for solving text summarization or similar tasks has become very common in the last years. yet most of the studies report basic accuracy scores only and nothing is known about the ability of the proposed models to improve when trained on more data. in this paper we define and propose three data efficiency metrics: data score efficiency data time deficiency and overall data efficiency. we also propose a simple scheme that uses those metrics and apply it for a more comprehensive evaluation of popular methods on text summarization and title generation tasks. for the latter task we process and release a huge collection of 35 million abstract-title pairs from scientific articles. our results reveal that among the tested models the transformer is the most efficient on both tasks.
739,mortensen-etal-2016-panphon,"   {P}an{P}hon: A Resource for Mapping {IPA} Segments to Articulatory Feature Vectors"",
",this paper contributes to a growing body of evidence that{---}when coupled with appropriate machine-learning techniques{--}linguistically motivated information-rich representations can outperform one-hot encodings of linguistic data. in particular we show that phonological features outperform character-based models. panphon is a database relating over 5000 ipa segments to 21 subsegmental articulatory features. we show that this database boosts performance in various ner-related tasks. phonologically aware neural crf models built on panphon features are able to perform better on monolingual spanish and turkish ner tasks that character-based models. they have also been shown to work well in transfer models (as between uzbek and turkish). panphon features also contribute measurably to orthography-to-ipa conversion tasks.
740,fu-etal-2020-design,"   Design Challenges in Low-resource Cross-lingual Entity Linking"",
",cross-lingual entity linking (xel) the problem of grounding mentions of entities in a foreign language text into an english knowledge base such as wikipedia has seen a lot of research in recent years with a range of promising techniques. however current techniques do not rise to the challenges introduced by text in low-resource languages (lrl) and surprisingly fail to generalize to text not taken from wikipedia on which they are usually trained. this paper provides a thorough analysis of low-resource xel techniques focusing on the key step of identifying candidate english wikipedia titles that correspond to a given foreign language mention. our analysis indicates that current methods are limited by their reliance on wikipedia{'}s interlanguage links and thus suffer when the foreign language{'}s wikipedia is small. we conclude that the lrl setting requires the use of outside-wikipedia cross-lingual resources and present a simple yet effective zero-shot xel system quel that utilizes search engines query logs. with experiments on 25 languages quel shows an average increase of 25{\%} in gold candidate recall and of 13{\%} in end-to-end linking accuracy over state-of-the-art baselines.
741,wang-etal-2021-exploring,"   Exploring the Importance of Source Text in Automatic Post-Editing for Context-Aware Machine Translation"",
",accurate translation requires document-level information which is ignored by sentence-level machine translation. recent work has demonstrated that document-level consistency can be improved with automatic post-editing (ape) using only target-language (tl) information. we study an extended ape model that additionally integrates source context. a human evaluation of fluency and adequacy in english{--}russian translation reveals that the model with access to source context significantly outperforms monolingual ape in terms of adequacy an effect largely ignored by automatic evaluation metrics. our results show that tl-only modelling increases fluency without improving adequacy demonstrating the need for conditioning on source text for automatic post-editing. they also highlight blind spots in automatic methods for targeted evaluation and demonstrate the need for human assessment to evaluate document-level translation quality reliably.
742,johnson-etal-2017-generating,"   Generating and Evaluating Summaries for Partial Email Threads: Conversational {B}ayesian Surprise and Silver Standards"",
",we define and motivate the problem of summarizing partial email threads. this problem introduces the challenge of generating reference summaries for partial threads when human annotation is only available for the threads as a whole particularly when the human-selected sentences are not uniformly distributed within the threads. we propose an oracular algorithm for generating these reference summaries with arbitrary length and we are making the resulting dataset publicly available. in addition we apply a recent unsupervised method based on bayesian surprise that incorporates background knowledge into partial thread summarization extend it with conversational features and modify the mechanism by which it handles redundancy. experiments with our method indicate improved performance over the baseline for shorter partial threads; and our results suggest that the potential benefits of background knowledge to partial thread summarization should be further investigated with larger datasets.
743,swanberg-etal-2018-alanis,"   {ALANIS} at {S}em{E}val-2018 Task 3: A Feature Engineering Approach to Irony Detection in {E}nglish Tweets"",
",this paper describes the alanis system that participated in task 3 of semeval-2018. we develop a system for detection of irony as well as the detection of three types of irony: verbal polar irony other verbal irony and situational irony. the system uses a logistic regression model in subtask a and a voted classifier system with manually developed features to identify ironic tweets. this model improves on a naive bayes baseline by about 8 percent on training set.
744,ravfogel-etal-2020-unsupervised,"   Unsupervised Distillation of Syntactic Information from Contextualized Word Representations"",
",contextualized word representations such as elmo and bert were shown to perform well on various semantic and syntactic task. in this work we tackle the task of unsupervised disentanglement between semantics and structure in neural language representations: we aim to learn a transformation of the contextualized vectors that discards the lexical semantics but keeps the structural information. to this end we automatically generate groups of sentences which are structurally similar but semantically different and use metric-learning approach to learn a transformation that emphasizes the structural component that is encoded in the vectors. we demonstrate that our transformation clusters vectors in space by structural properties rather than by lexical semantics. finally we demonstrate the utility of our distilled representations by showing that they outperform the original contextualized representations in a few-shot parsing setting.
745,pinter-etal-2019-character,"   Character Eyes: Seeing Language through Character-Level Taggers"",
",character-level models have been used extensively in recent years in nlp tasks as both supplements and replacements for closed-vocabulary token-level word representations. in one popular architecture character-level lstms are used to feed token representations into a sequence tagger predicting token-level annotations such as part-of-speech (pos) tags. in this work we examine the behavior of pos taggers across languages from the perspective of individual hidden units within the character lstm. we aggregate the behavior of these units into language-level metrics which quantify the challenges that taggers face on languages with different morphological properties and identify links between synthesis and affixation preference and emergent behavior of the hidden tagger layer. in a comparative experiment we show how modifying the balance between forward and backward hidden units affects model arrangement and performance in these types of languages.
746,hangya-etal-2018-two,"   Two Methods for Domain Adaptation of Bilingual Tasks: Delightfully Simple and Broadly Applicable"",
",bilingual tasks such as bilingual lexicon induction and cross-lingual classification are crucial for overcoming data sparsity in the target language. resources required for such tasks are often out-of-domain thus domain adaptation is an important problem here. we make two contributions. first we test a delightfully simple method for domain adaptation of bilingual word embeddings. we evaluate these embeddings on two bilingual tasks involving different domains: cross-lingual twitter sentiment classification and medical bilingual lexicon induction. second we tailor a broadly applicable semi-supervised classification method from computer vision to these tasks. we show that this method also helps in low-resource setups. using both methods together we achieve large improvements over our baselines by using only additional unlabeled data.
747,prasad-etal-2010-exploiting,"   Exploiting Scope for Shallow Discourse Parsing"",
",we present an approach to automatically identifying the arguments of discourse connectives based on data from the penn discourse treebank. of the two arguments of connectives called arg1 and arg2 we focus on arg1 which has proven more challenging to identify. our approach employs a sentence-based representation of arguments and distinguishes ''``intra-sentential connectives'''' which take both their arguments in the same sentence from ''``inter-sentential connectives'''' whose arguments are found in different sentences. the latter are further distinguished by paragraph position into ''``parainit'''' connectives which appear in a paragraph-initial sentence and ''``paranoninit'''' connectives which appear elsewhere. the paper focusses on predicting arg1 of inter-sentential paranoninit connectives presenting a set of scope-based filters that reduce the search space for arg1 from all the previous sentences in the paragraph to a subset of them. for cases where these filters do not uniquely identify arg1 coreference-based heuristics are employed. our analysis shows an absolute 3{\%} performance improvement over the high baseline of 83.3{\%} for identifying arg1 of inter-sentential paranoninit connectives.
748,touileb-etal-2020-gender,"   Gender and sentiment, critics and authors: a dataset of {N}orwegian book reviews"",
",gender bias in models and datasets is widely studied in nlp. the focus has usually been on analysing how females and males express themselves or how females and males are described. however a less studied aspect is the combination of these two perspectives how female and male describe the same or opposite gender. in this paper we present a new gender annotated sentiment dataset of critics reviewing the works of female and male authors. we investigate if this newly annotated dataset contains differences in how the works of male and female authors are critiqued in particular in terms of positive and negative sentiment. we also explore the differences in how this is done by male and female critics. we show that there are differences in how critics assess the works of authors of the same or opposite gender. for example male critics rate crime novels written by females and romantic and sentimental works written by males more negatively.
749,nikulasdottir-etal-2020-language,"   Language Technology Programme for {I}celandic 2019-2023"",
",in this paper we describe a new national language technology programme for icelandic. the programme which spans a period of five years aims at making icelandic usable in communication and interactions in the digital world by developing accessible open-source language resources and software. the research and development work within the programme is carried out by a consortium of universities institutions and private companies with a strong emphasis on cooperation between academia and industries. five core projects will be the main content of the programme: language resources speech recognition speech synthesis machine translation and spell and grammar checking. we also describe other national language technology programmes and give an overview over the history of language technology in iceland.
750,lacatusu-etal-2006-impact,"   Impact of Question Decomposition on the Quality of Answer Summaries"",
",generating answers to complex questions in the form of multi-document summaries requires access to question decomposition methods. in this paper we present three methods for decomposing complex questions and we evaluate their impact on the responsiveness of the answers they enable.
751,ren-etal-2020-knowledge,"   Knowledge Graph Embedding with Atrous Convolution and Residual Learning"",
",knowledge graph embedding is an important task and it will benefit lots of downstream applications. currently deep neural networks based methods achieve state-of-the-art performance. however most of these existing methods are very complex and need much time for training and inference. to address this issue we propose a simple but effective atrous convolution based knowledge graph embedding method. compared with existing state-of-the-art methods our method has following main characteristics. first it effectively increases feature interactions by using atrous convolutions. second to address the original information forgotten issue and vanishing/exploding gradient issue it uses the residual learning method. third it has simpler structure but much higher parameter efficiency. we evaluate our method on six benchmark datasets with different evaluation metrics. extensive experiments show that our model is very effective. on these diverse datasets it achieves better results than the compared state-of-the-art methods on most of evaluation metrics. the source codes of our model could be found at https://github.com/neukg/acre.
752,scherrer-rabus-2017-multi,"   Multi-source morphosyntactic tagging for spoken {R}usyn"",
",this paper deals with the development of morphosyntactic taggers for spoken varieties of the slavic minority language rusyn. as neither annotated corpora nor parallel corpora are electronically available for rusyn we propose to combine existing resources from the etymologically close slavic languages russian ukrainian slovak and polish and adapt them to rusyn. using marmot as tagging toolkit we show that a tagger trained on a balanced set of the four source languages outperforms single language taggers by about 9{\%} and that additional automatically induced morphosyntactic lexicons lead to further improvements. the best observed accuracies for rusyn are 82.4{\%} for part-of-speech tagging and 75.5{\%} for full morphological tagging.
753,kuhnert-antolik-2016-strategies,"   Strat{\'e}gies d{'}adaptation de la vitesse d{'}articulation lors de conversations spontan{\'e}es entre locuteurs natifs et non-natifs (Adaptation of articulation rate in spontaneous speech between native speakers and {L}2 learners)"",
",cet article examine la vitesse d{'}articulation dans un corpus de conversations spontan{\'e}es entre locuteurs natifs et non-natifs. l{'}objectif est d{'}{\'e}tudier (i) dans quelle mesure les locuteurs natifs adaptent dans leur l1 leur vitesse d{'}articulation aux apprenants l2 et (ii) dans quelle mesure les deux locuteurs en interaction ont tendance {\`a} rapprocher ou {\`a} dissocier leurs caract{\'e}ristiques temporelles au cours d{'}une conversation. les donn{\'e}es proviennent du corpus sitaf d{'}interactions tandem en anglais-fran{\c{c}}ais. a ce jour 10 sujets ont {\'e}t{\'e} analys{\'e}s chacun ayant {\'e}t{\'e} enregistr{\'e} dans trois conditions diff{\'e}rentes : en utilisant sa l1 avec un autre locuteur natif en utilisant sa l1 avec un apprenant l2 et en utilisant sa l2 avec un interlocuteur parlant sa propre l1. les r{\'e}sultats indiquent que les propri{\'e}t{\'e}s rythmiques de la l1 ont une nette influence sur les variations de la vitesse d{'}articulation des locuteurs non seulement lorsqu{'}ils interagissent dans leur l2 mais {\'e}galement dans leurs strat{\'e}gies d{'}adaptation lorsqu{'}ils interagissent avec des apprenants.
754,evang-2020-configurable,"   Configurable Dependency Tree Extraction from {CCG} Derivations"",
",we revisit the problem of extracting dependency structures from the derivation structures of combinatory categorial grammar (ccg). previous approaches are often restricted to a narrow subset of ccg or support only one flavor of dependency tree. our approach is more general and easily configurable so that multiple styles of dependency tree can be obtained. in an initial case study we show promising results for converting english german italian and dutch ccg derivations from the parallel meaning bank into (unlabeled) ud-style dependency trees.
755,kongkachandra-chamnongthai-2006-semantic,"   Semantic-Based Keyword Recovery Function for Keyword Extraction System"",
",the goal of implementing a keyword extraction system is to increase as near as 100{\%} of precision and recall. these values are affected by the amount of extracted keywords. there are two groups of errors happened i.e. false-rejected and false-accepted keywords. to improve the performance of the system false-rejected keywords should be recovered and the false-accepted keywords should be reduced. in this paper we enhance the conventional keyword extraction systems by attaching the keyword recovery function. this function recovers the previously false-rejected keywords by comparing their semantic information with the contents of each relevant document. the function is automated in three processes i.e. domain identification knowledge base generation and keyword determination. domain identification process identifies domain of interest by searching domains from domain knowledge base by using extracted keywords.the most general domains are selected and then used subsequently. to recover the false-rejected keywords we match them with keywords in the identified domain within the domain knowledge base rely on their semantics by keyword determination process. to semantically recover keywords definitions of false-reject keywords and domain knowledge base are previously represented in term of conceptual graph by knowledge base generator process. to evaluate the performance of the proposed function extractor kea and our keyword-database-mapping based keyword extractor are compared. the experiments were performed in two modes i.e. training and recovering. in training mode we use four glossaries from the internet and 60 articles from the summary sections of ieice transaction. while in the recovering mode 200 texts from three resources i.e. summary section of 15 chapters in a computer textbook and articles from ieice and acm transactions are used. the experimental results revealed that our proposed function improves the precision and recall rates of the conventional keyword extraction systems approximately 3-5{\%} of precision and 6-10{\%} of recall respectively.
756,molla-2020-overview,"   Overview of the 2020 {ALTA} Shared Task: Assess Human Behaviour"",
",the 2020 alta shared task is the 11th in stance of a series of shared tasks organised by alta since 2010. the task is to classify texts posted in social media according to human judgements expressed in them. the data used for this task is a subset of semeval 2018 ait disc which has been annotated by domain experts for this task. in this paper we introduce the task describe the data and present the results of participating systems.
757,joshi-etal-2019-pair2vec,"   pair2vec: Compositional Word-Pair Embeddings for Cross-Sentence Inference"",
",reasoning about implied relationships (e.g. paraphrastic common sense encyclopedic) between pairs of words is crucial for many cross-sentence inference problems. this paper proposes new methods for learning and using embeddings of word pairs that implicitly represent background knowledge about such relationships. our pairwise embeddings are computed as a compositional function of each word{'}s representation which is learned by maximizing the pointwise mutual information (pmi) with the contexts in which the the two words co-occur. we add these representations to the cross-sentence attention layer of existing inference models (e.g. bidaf for qa esim for nli) instead of extending or replacing existing word embeddings. experiments show a gain of 2.7{\%} on the recently released squad 2.0 and 1.3{\%} on multinli. our representations also aid in better generalization with gains of around 6-7{\%} on adversarial squad datasets and 8.8{\%} on the adversarial entailment test set by glockner et al. (2018).
758,jiang-etal-2018-identifying,"   Identifying Emergent Research Trends by Key Authors and Phrases"",
",identifying emergent research trends is a key issue for both primary researchers as well as secondary research managers. such processes can uncover the historical development of an area and yield insight on developing topics. we propose an embedded trend detection framework for this task which incorporates our bijunctive hypothesis that important phrases are written by important authors within a field and vice versa. by ranking both author and phrase information in a multigraph our method jointly determines key phrases and authoritative authors. we represent this intermediate output as phrasal embeddings and feed this to a recurrent neural network (rnn) to compute trend scores that identify research trends. over two large datasets of scientific articles we demonstrate that our approach successfully detects past trends from the field outperforming baselines based solely on text centrality or citation.
759,hopkins-etal-2019-semeval,"   {S}em{E}val-2019 Task 10: Math Question Answering"",
",we report on the semeval 2019 task on math question answering. we provided a question set derived from math sat practice exams including 2778 training questions and 1082 test questions. for a significant subset of these questions we also provided smt-lib logical form annotations and an interpreter that could solve these logical forms. systems were evaluated based on the percentage of correctly answered questions. the top system correctly answered 45{\%} of the test questions a considerable improvement over the 17{\%} random guessing baseline.
760,pilan-etal-2016-predicting,"   Predicting proficiency levels in learner writings by transferring a linguistic complexity model from expert-written coursebooks"",
",the lack of a sufficient amount of data tailored for a task is a well-recognized problem for many statistical nlp methods. in this paper we explore whether data sparsity can be successfully tackled when classifying language proficiency levels in the domain of learner-written output texts. we aim at overcoming data sparsity by incorporating knowledge in the trained model from another domain consisting of input texts written by teaching professionals for learners. we compare different domain adaptation techniques and find that a weighted combination of the two types of data performs best which can even rival systems based on considerably larger amounts of in-domain data. moreover we show that normalizing errors in learners{'} texts can substantially improve classification when level-annotated in-domain data is not available.
761,caines-etal-2020-grammatical,"   Grammatical error detection in transcriptions of spoken {E}nglish"",
",we describe the collection of transcription corrections and grammatical error annotations for the crowded corpus of spoken english monologues on business topics. the corpus recordings were crowdsourced from native speakers of english and learners of english with german as their first language. the new transcriptions and annotations are obtained from different crowdworkers: we analyse the 1108 new crowdworker submissions and propose that they can be used for automatic transcription post-editing and grammatical error correction for speech. to further explore the data we train grammatical error detection models with various configurations including pre-trained and contextual word representations as input additional features and auxiliary objectives and extra training data from written error-annotated corpora. we find that a model concatenating pre-trained and contextual word representations as input performs best and that additional information does not lead to further performance gains.
762,hedi-maaloul-keskes-2010-resume,"   R{\'e}sum{\'e} automatique de documents arabes bas{\'e} sur la technique {RST}"",
",dans cet article nous nous int{\'e}ressons au r{\'e}sum{\'e} automatique de textes arabes. nous commen{\c{c}}ons par pr{\'e}senter une {\'e}tude analytique r{\'e}alis{\'e}e sur un corpus de travail qui nous a permis de d{\'e}duire suite {\`a} des observations empiriques un ensemble de relations et de frames (r{\`e}gles ou patrons) rh{\'e}toriques; ensuite nous pr{\'e}sentons notre m{\'e}thode de production de r{\'e}sum{\'e}s pour les textes arabes. la m{\'e}thode que nous proposons se base sur la th{\'e}orie de la structure rh{\'e}torique (rst) (mann et al. 1988) et utilise des connaissances purement linguistiques. le principe de notre proposition s{'}appuie sur trois piliers. le premier pilier est le rep{\'e}rage des relations rh{\'e}toriques entres les diff{\'e}rentes unit{\'e}s minimales du texte dont l{'}une poss{\`e}de le statut de noyau {--} segment de texte primordial pour la coh{\'e}rence {--} et l{'}autre a le statut noyau ou satellite {--} segment optionnel. le deuxi{\`e}me pilier est le dressage et la simplification de l{'}arbre rst. le troisi{\`e}me pilier est la s{\'e}lection des phrases noyaux formant le r{\'e}sum{\'e} final qui tiennent en compte le type de relation rh{\'e}toriques choisi pour l{'}extrait.
763,nangia-etal-2020-crows,"   {C}row{S}-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models"",
",pretrained language models especially masked language models (mlms) have seen success across many nlp tasks. however there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on implicitly creating harm with biased representations. to measure some forms of social bias in language models against protected demographic groups in the us we introduce the crowdsourced stereotype pairs benchmark (crows-pairs). crows-pairs has 1508 examples that cover stereotypes dealing with nine types of bias like race religion and age. in crows-pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. the data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. we find that all three of the widely-used mlms we evaluate substantially favor sentences that express stereotypes in every category in crows-pairs. as work on building less biased models advances this dataset can be used as a benchmark to evaluate progress.
764,guillaume-perrier-2010-leopar,"   {LEOPAR}, un analyseur syntaxique pour les grammaires d{'}interaction"",
",nous pr{\'e}sentons ici l{'}analyseur syntaxique leopar bas{\'e} sur les grammaires d{'}interaction ainsi que d{'}autres outils utiles pour notre cha{\^\i}ne de traitement syntaxique.
765,charbonnier-wartena-2018-using,"   Using Word Embeddings for Unsupervised Acronym Disambiguation"",
",scientific papers from all disciplines contain many abbreviations and acronyms. in many cases these acronyms are ambiguous. we present a method to choose the contextual correct definition of an acronym that does not require training for each acronym and thus can be applied to a large number of different acronyms with only few instances. we constructed a set of 19954 examples of 4365 ambiguous acronyms from image captions in scientific papers along with their contextually correct definition from different domains. we learn word embeddings for all words in the corpus and compare the averaged context vector of the words in the expansion of an acronym with the weighted average vector of the words in the context of the acronym. we show that this method clearly outperforms (classical) cosine similarity. furthermore we show that word embeddings learned from a 1 billion word corpus of scientific texts outperform word embeddings learned on much large general corpora.
766,garland-etal-2020-countering,"   Countering hate on social media: Large scale classification of hate and counter speech"",
",hateful rhetoric is plaguing online discourse fostering extreme societal movements and possibly giving rise to real-world violence. a potential solution to this growing global problem is citizen-generated counter speech where citizens actively engage with hate speech to restore civil non-polarized discourse. however its actual effectiveness in curbing the spread of hatred is unknown and hard to quantify. one major obstacle to researching this question is a lack of large labeled data sets for training automated classifiers to identify counter speech. here we use a unique situation in germany where self-labeling groups engaged in organized online hate and counter speech. we use an ensemble learning algorithm which pairs a variety of paragraph embeddings with regularized logistic regression functions to classify both hate and counter speech in a corpus of millions of relevant tweets from these two groups. our pipeline achieves macro f1 scores on out of sample balanced test sets ranging from 0.76 to 0.97{---}accuracy in line and even exceeding the state of the art. we then use the classifier to discover hate and counter speech in more than 135000 fully-resolved twitter conversations occurring from 2013 to 2018 and study their frequency and interaction. altogether our results highlight the potential of automated methods to evaluate the impact of coordinated counter speech in stabilizing conversations on social media.
767,yu-etal-2020-imsurreal,"   {IMS}ur{R}eal Too: {IMS} in the Surface Realization Shared Task 2020"",
",we introduce the ims contribution to the surface realization shared task 2020. the new system achieves substantial improvement over the state-of-the-art system from last year mainly due to a better token representation and a better linearizer as well as a simple ensembling approach. we also experiment with data augmentation which brings some additional performance gain. the system is available at https://github.com/eggplantelf/imsurreal.
768,pergola-etal-2021-boosting,"   Boosting Low-Resource Biomedical {QA} via Entity-Aware Masking Strategies"",
",biomedical question-answering (qa) has gained increased attention for its capability to provide users with high-quality information from a vast scientific literature. although an increasing number of biomedical qa datasets has been recently made available those resources are still rather limited and expensive to produce; thus transfer learning via pre-trained language models (lms) has been shown as a promising approach to leverage existing general-purpose knowledge. however fine-tuning these large models can be costly and time consuming and often yields limited benefits when adapting to specific themes of specialised domains such as the covid-19 literature. therefore to bootstrap further their domain adaptation we propose a simple yet unexplored approach which we call biomedical entity-aware masking (bem) strategy encouraging masked language models to learn entity-centric knowledge based on the pivotal entities characterizing the domain at hand and employ those entities to drive the lm fine-tuning. the resulting strategy is a downstream process applicable to a wide variety of masked lms not requiring additional memory or components in the neural architectures. experimental results show performance on par with the state-of-the-art models on several biomedical qa datasets.
769,persing-ng-2020-unsupervised,"   Unsupervised Argumentation Mining in Student Essays"",
",state-of-the-art systems for argumentation mining are supervised thus relying on training data containing manually annotated argument components and the relationships between them. to eliminate the reliance on annotated data we present a novel approach to unsupervised argument mining. the key idea is to bootstrap from a small set of argument components automatically identified using simple heuristics in combination with reliable contextual cues. results on a stab and gurevych{'}s corpus of 402 essays show that our unsupervised approach rivals two supervised baselines in performance and achieves 73.5-83.7{\%} of the performance of a state-of-the-art neural approach.
770,wilson-etal-2020-embedding,"   Embedding Structured Dictionary Entries"",
",previous work has shown how to effectively use external resources such as dictionaries to improve english-language word embeddings either by manipulating the training process or by applying post-hoc adjustments to the embedding space. we experiment with a multi-task learning approach for explicitly incorporating the structured elements of dictionary entries such as user-assigned tags and usage examples when learning embeddings for dictionary headwords. our work generalizes several existing models for learning word embeddings from dictionaries. however we find that the most effective representations overall are learned by simply training with a skip-gram objective over the concatenated text of all entries in the dictionary giving no particular focus to the structure of the entries.
771,srivastava-singh-2021-hinge,"   {H}in{GE}: A Dataset for Generation and Evaluation of Code-Mixed {H}inglish Text"",
",text generation is a highly active area of research in the computational linguistic community. the evaluation of the generated text is a challenging task and multiple theories and metrics have been proposed over the years. unfortunately text generation and evaluation are relatively understudied due to the scarcity of high-quality resources in code-mixed languages where the words and phrases from multiple languages are mixed in a single utterance of text and speech. to address this challenge we present a corpus (hinge) for a widely popular code-mixed language hinglish (code-mixing of hindi and english languages). hinge has hinglish sentences generated by humans as well as two rule-based algorithms corresponding to the parallel hindi-english sentences. in addition we demonstrate the in- efficacy of widely-used evaluation metrics on the code-mixed data. the hinge dataset will facilitate the progress of natural language generation research in code-mixed languages.
772,druskat-etal-2016-corpus,"   corpus-tools.org: An Interoperable Generic Software Tool Set for Multi-layer Linguistic Corpora"",
",this paper introduces an open source interoperable generic software tool set catering for the entire workflow of creation migration annotation query and analysis of multi-layer linguistic corpora. it consists of four components: salt a graph-based meta model and api for linguistic data the common data model for the rest of the tool set; pepper a conversion tool and platform for linguistic data that can be used to convert many different linguistic formats into each other; atomic an extensible platform-independent multi-layer desktop annotation software for linguistic corpora; annis a search and visualization architecture for multi-layer linguistic corpora with many different visualizations and a powerful native query language. the set was designed to solve the following issues in a multi-layer corpus workflow: lossless data transition between tools through a common data model generic enough to allow for a potentially unlimited number of different types of annotation conversion capabilities for different linguistic formats to cater for the processing of data from different sources and/or with existing annotations a high level of extensibility to enhance the sustainability of the whole tool set analysis capabilities encompassing corpus and annotation query alongside multi-faceted visualizations of all annotation layers.
773,conley-kalita-2020-language,"   Language Model Metrics and {P}rocrustes Analysis for Improved Vector Transformation of {NLP} Embeddings"",
",artificial neural networks are mathematical models at their core. this truism presents some fundamental difficulty when networks are tasked with natural language processing. a key problem lies in measuring the similarity or distance among vectors in nlp embedding space since the mathematical concept of distance does not always agree with the linguistic concept. we suggest that the best way to measure linguistic distance among vectors is by employing the language model (lm) that created them. we introduce language model distance (lmd) for measuring accuracy of vector transformations based on the distributional hypothesis ( lmd accuracy ). we show the efficacy of this metric by applying it to a simple neural network learning the procrustes algorithm for bilingual word mapping.
774,rozenknop-silaghi-2001-algorithme,"   Algorithme de d{\'e}codage de treillis selon le crit{\`e}re du co{\^u}t moyen pour la reconnaissance de la parole"",
",les mod{\`e}les de langage stochastiques utilis{\'e}s pour la reconnaissance de la parole continue ainsi que dans certains syst{\`e}mes de traitement automatique de la langue favorisent pour la plupart l{'}interpr{\'e}tation d{'}un signal par les phrases les plus courtes possibles celles-ci {\'e}tant par construction bien souvent affect{\'e}es des co{\^u}ts les plus bas. cet article expose un algorithme permettant de r{\'e}pondre {\`a} ce probl{\`e}me en rempla{\c{c}}ant le co{\^u}t habituel affect{\'e} par le mod{\`e}le de langage par sa moyenne sur la longueur de la phrase consid{\'e}r{\'e}e. cet algorithme est tr{\`e}s g{\'e}n{\'e}ral et peut {\^e}tre adapt{\'e} ais{\'e}ment {\`a} de nombreux mod{\`e}les de langage y compris sur des t{\^a}ches d{'}analyse syntaxique.
775,alam-anastasopoulos-2020-fine,"   Fine-Tuning {MT} systems for Robustness to Second-Language Speaker Variations"",
",the performance of neural machine translation (nmt) systems only trained on a single language variant degrades when confronted with even slightly different language variations. with this work we build upon previous work to explore how to mitigate this issue. we show that fine-tuning using naturally occurring noise along with pseudo-references (i.e. {``}corrected{''} non-native inputs translated using the baseline nmt system) is a promising solution towards systems robust to such type of input variations. we focus on four translation pairs from english to spanish italian french and portuguese with our system achieving improvements of up to 3.1 bleu points compared to the baselines establishing a new state-of-the-art on the jfleg-es dataset. all datasets and code are publicly available here: https://github.com/mahfuzibnalam/finetuning{\_}for{\_}robustness .
776,fothergill-etal-2016-evaluating,"   Evaluating a Topic Modelling Approach to Measuring Corpus Similarity"",
",web corpora are often constructed automatically and their contents are therefore often not well understood. one technique for assessing the composition of such a web corpus is to empirically measure its similarity to a reference corpus whose composition is known. in this paper we evaluate a number of measures of corpus similarity including a method based on topic modelling which has not been previously evaluated for this task. to evaluate these methods we use known-similarity corpora that have been previously used for this purpose as well as a number of newly-constructed known-similarity corpora targeting differences in genre topic time and region. our findings indicate that overall the topic modelling approach did not improve on a chi-square method that had previously been found to work well for measuring corpus similarity.
777,espana-bonet-barron-cedeno-2017-lump,"   Lump at {S}em{E}val-2017 Task 1: Towards an Interlingua Semantic Similarity"",
",this is the lump team participation at semeval 2017 task 1 on semantic textual similarity. our supervised model relies on features which are multilingual or interlingual in nature. we include lexical similarities cross-language explicit semantic analysis internal representations of multilingual neural networks and interlingual word embeddings. our representations allow to use large datasets in language pairs with many instances to better classify instances in smaller language pairs avoiding the necessity of translating into a single language. hence we can deal with all the languages in the task: arabic english spanish and turkish.
778,kimura-etal-2021-towards-language,"   Towards a Language Model for Temporal Commonsense Reasoning"",
",temporal commonsense reasoning is a challenging task as it requires temporal knowledge usually not explicit in text. in this work we propose an ensemble model for temporal commonsense reasoning. our model relies on pre-trained contextual representations from transformer-based language models (i.e. bert) and on a variety of training methods for enhancing model generalization: 1) multi-step fine-tuning using carefully selected auxiliary tasks and datasets and 2) a specifically designed temporal masked language model task aimed to capture temporal commonsense knowledge. our model greatly outperforms the standard fine-tuning approach and strong baselines on the mc-taco dataset.
779,ye-etal-2020-safer,"   {SAFER}: A Structure-free Approach for Certified Robustness to Adversarial Word Substitutions"",
",state-of-the-art nlp models can often be fooled by human-unaware transformations such as synonymous word substitution. for security reasons it is of critical importance to develop models with certified robustness that can provably guarantee that the prediction is can not be altered by any possible synonymous word substitution. in this work we propose a certified robust method based on a new randomized smoothing technique which constructs a stochastic ensemble by applying random word substitutions on the input sentences and leverage the statistical properties of the ensemble to provably certify the robustness. our method is simple and structure-free in that it only requires the black-box queries of the model outputs and hence can be applied to any pre-trained models (such as bert) and any types of models (world-level or subword-level). our method significantly outperforms recent state-of-the-art methods for certified robustness on both imdb and amazon text classification tasks. to the best of our knowledge we are the first work to achieve certified robustness on large systems such as bert with practically meaningful certified accuracy.
780,acs-etal-2021-subword,"   Subword Pooling Makes a Difference"",
",contextual word-representations became a standard in modern natural language processing systems. these models use subword tokenization to handle large vocabularies and unknown words. word-level usage of such systems requires a way of pooling multiple subwords that correspond to a single word. in this paper we investigate how the choice of subword pooling affects the downstream performance on three tasks: morphological probing pos tagging and ner in 9 typologically diverse languages. we compare these in two massively multilingual models mbert and xlm-roberta. for morphological tasks the widely used {`}choose the first subword{'} is the worst strategy and the best results are obtained by using attention over the subwords. for pos tagging both of these strategies perform poorly and the best choice is to use a small lstm over the subwords. the same strategy works best for ner and we show that mbert is better than xlm-roberta in all 9 languages. we publicly release all code data and the full result tables at https://github.com/juditacs/subword-choice .
781,marie-etal-2019-nicts,"   {NICT}{'}s Unsupervised Neural and Statistical Machine Translation Systems for the {WMT}19 News Translation Task"",
",this paper presents the nict{'}s participation in the wmt19 unsupervised news translation task. we participated in the unsupervised translation direction: german-czech. our primary submission to the task is the result of a simple combination of our unsupervised neural and statistical machine translation systems. our system is ranked first for the german-to-czech translation task using only the data provided by the organizers ({``}constraint{'}{''}) according to both bleu-cased and human evaluation. we also performed contrastive experiments with other language pairs namely english-gujarati and english-kazakh to better assess the effectiveness of unsupervised machine translation in for distant language pairs and in truly low-resource conditions.
782,hadjadj-filhol-2016-description,"   Description de la juxtaposition en Langue des Signes Fran{\c{c}}aise {\`a} partir d{'}une grammaire r{\'e}cursive (The present communication tackles formal grammar developpement of {F}rench {S}ign {L}anguage ({LSF}))"",
",la pr{\'e}sente communication s{'}inscrit dans le cadre du d{\'e}veloppement d{'}une grammaire formelle pour la langue des signes fran{\c{c}}aise (lsf). g{\'e}n{\'e}rer automatiquement des {\'e}nonc{\'e}s en lsf implique la d{\'e}finition de certaines r{\`e}gles de production pour synchroniser les diff{\'e}rents articulateurs du corps signes mouvements etc. cet article pr{\'e}sente dans sa premi{\`e}re partie notre m{\'e}thodologie pour d{\'e}finir des r{\`e}gles de production {\`a} partir d{'}une {\'e}tude de corpus. dans la deuxi{\`e}me partie nous pr{\'e}senterons notre {\'e}tude qui portera sur deux r{\`e}gles de production pour juxtaposer quelques types de structures en lsf. nous finissons par une discussion sur la nature et l{'}apport de notre d{\'e}marche par rapport aux approches existantes.
783,tang-etal-2021-continuous,"   Continuous Language Generative Flow"",
",recent years have witnessed various types of generative models for natural language generation (nlg) especially rnns or transformer based sequence-to-sequence models as well as variational autoencoder (vae) and generative adversarial network (gan) based models. however flow-based generative models which achieve strong performance in image generation due to their invertibility and exact density estimation properties have been less explored for nlg. in this paper we propose a flow-based language generation model by adapting previous flow generative models to language generation via continuous input embeddings adapted affine coupling structures and a novel architecture for autoregressive text generation. we also apply our framework to sequence-to-sequence generation including text- and video-based question generation (qg) and neural machine translation (nmt) and data augmentation for question answering (qa). we use our language flow model to provide extra input features for qg and nmt which achieves improvements over the strong qg baselines on squad and tvqa and nmt baseline on wmt16. we also augment qa data with new context by injecting noise to the latent features of the language flow and show this augmentation leads to a large performance improvement from strong baselines on squad and tvqa.
784,xie-etal-2008-extracting,"   From Extracting to Abstracting: Generating Quasi-abstractive Summaries"",
",in this paper we investigate quasi-abstractive summaries a new type of machine-generated summaries that do not use whole sentences but only fragments from the source. quasi-abstractive summaries aim at bridging the gap between human-written abstracts and extractive summaries. we present an approach that learns how to identify sets of sentences where each set contains fragments that can be used to produce one sentence in the abstract; and then uses these sets to produce the abstract itself. our experiments show very promising results. importantly we obtain our best results when the summary generation is anchored by the most salient noun phrases predicted from the text to be summarized.
785,li-chen-2020-multimodal,"   Multimodal Sentiment Analysis with Multi-perspective Fusion Network Focusing on Sense Attentive Language"",
",multimodal sentiment analysis aims to learn a joint representation of multiple features. as demonstrated by previous studies it is shown that the language modality may contain more semantic information than that of other modalities. based on this observation we propose a multi-perspective fusion network(mpfn) focusing on sense attentive language for multimodal sentiment analysis. different from previous studies we use the language modality as the main part of the final joint representation and propose a multi-stage and uni-stage fusion strategy to get the fusion representation of the multiple modalities to assist the final language-dominated multimodal representation. in our model a sense-level attention network is proposed to dynamically learn the word representation which is guided by the fusion of the multiple modalities. as in turn the learned language representation can also help the multi-stage and uni-stage fusion of the different modalities. in this way the model can jointly learn a well integrated final representation focusing on the language and the interactions between the multiple modalities both on multi-stage and uni-stage. several experiments are carried on the cmu-mosi the cmu-mosei and the youtube public datasets. the experiments show that our model performs better or competitive results compared with the baseline models.
786,pagel-roesiger-2018-towards,"   Towards Bridging Resolution in {G}erman: Data Analysis and Rule-based Experiments"",
",bridging resolution is the task of recognising bridging anaphors and linking them to their antecedents. while there is some work on bridging resolution for english there is only little work for german. we present two datasets which contain bridging annotations namely dirndl and grain and compare the performance of a rule-based system with a simple baseline approach on these two corpora. the performance for full bridging resolution ranges between an f1 score of 13.6{\%} for dirndl and 11.8{\%} for grain. an analysis using oracle lists suggests that the system could to a certain extent benefit from ranking and re-ranking antecedent candidates. furthermore we investigate the importance of single features and show that the features used in our work seem promising for future bridging resolution approaches.
787,veronis-2003-cartographie,"   Cartographie lexicale pour la recherche d{''}information"",
",nous d{\'e}crivons un algorithme hyperlex de d{\'e}termination automatique des diff{\'e}rents usages d{'}un mot dans une base textuelle sans utilisation d{'}un dictionnaire. cet algorithme bas{\'e} sur la d{\'e}tection des composantes de forte densit{\'e} du graphe des cooccurrences de mots permet contrairement aux m{\'e}thodes pr{\'e}c{\'e}demment propos{\'e}es (vecteurs de mots) d{'}isoler des usages tr{\`e}s peu fr{\'e}quents. il est associ{\'e} {\`a} une technique de repr{\'e}sentation graphique permettant {\`a} l{'}utilisateur de naviguer de fa{\c{c}}on visuelle {\`a} travers le lexique et d{'}explorer les diff{\'e}rentes th{\'e}matiques correspondant aux usages discrimin{\'e}s.
788,mitzalis-etal-2021-bertgen,"   {BERTG}en: Multi-task Generation through {BERT}"",
",we present bertgen a novel generative decoder-only model which extends bert by fusing multimodal and multilingual pre-trained models vl-bert and m-bert respectively. bertgen is auto-regressively trained for language generation tasks namely image captioning machine translation and multimodal machine translation under a multi-task setting. with a comprehensive set of evaluations we show that bertgen outperforms many strong baselines across the tasks explored. we also show bertgen{'}s ability for zero-shot language generation where it exhibits competitive performance to supervised counterparts. finally we conduct ablation studies which demonstrate that bertgen substantially benefits from multi-tasking and effectively transfers relevant inductive biases from the pre-trained models.
789,deleris-etal-2018-decision,"   Decision Conversations Decoded"",
",we describe the vision and current version of a natural language processing system aimed at group decision making facilitation. borrowing from the scientific field of decision analysis its essential role is to identify alternatives and criteria associated with a given decision to keep track of who proposed them and of the expressed sentiment towards them. based on this information the system can help identify agreement and dissent or recommend an alternative. overall it seeks to help a group reach a decision in a natural yet auditable fashion.
790,li-wang-2017-building,"   Building Large {C}hinese Corpus for Spoken Dialogue Research in Specific Domains"",
",corpus is a valuable resource for information retrieval and data-driven natural language processing systemsespecially for spoken dialogue research in specific domains. howeverthere is little non-english corpora particular for ones in chinese. spoken by the nation with the largest population in the world chinese become increasingly prevalent and popular among millions of people worldwide. in this paper we build a large-scale and high-quality chinese corpus called csdc (chinese spoken dialogue corpus). it contains five domains and more than 140 thousand dialogues in all. each sentence in this corpus is annotated with slot information additionally compared to other corpora. to our best knowledge this is the largest chinese spoken dialogue corpus as well as the first one with slot information. with this corpus we proposed a method and did a well-designed experiment. the indicative result is reported at last.
791,hellrich-etal-2019-modeling,"   Modeling Word Emotion in Historical Language: Quantity Beats Supposed Stability in Seed Word Selection"",
",to understand historical texts we must be aware that language{---}including the emotional connotation attached to words{---}changes over time. in this paper we aim at estimating the emotion which is associated with a given word in former language stages of english and german. emotion is represented following the popular valence-arousal-dominance (vad) annotation scheme. while being more expressive than polarity alone existing word emotion induction methods are typically not suited for addressing it. to overcome this limitation we present adaptations of two popular algorithms to vad. to measure their effectiveness in diachronic settings we present the first gold standard for historical word emotions which was created by scholars with proficiency in the respective language stages and covers both english and german. in contrast to claims in previous work our findings indicate that hand-selecting small sets of seed words with supposedly stable emotional meaning is actually harm- rather than helpful.
792,coltekin-rama-2018-tubingen,"   {S}em{E}val 2018 Task 2: Multilingual Emoji Prediction"",
",this paper describes our participation in the semeval-2018 task multilingual emoji prediction. we participated in both english and spanish subtasks experimenting with support vector machines (svms) and recurrent neural networks. our svm classifier obtained the top rank in both subtasks with macro-averaged f1-measures of 35.99{\%} for english and 22.36{\%} for spanish data sets. similar to a few earlier attempts the results with neural networks were not on par with linear svms.
793,kumar-etal-2021-machine,"   Machine Translation into Low-resource Language Varieties"",
",state-of-the-art machine translation (mt) systems are typically trained to generate {``}standard{''} target language; however many languages have multiple varieties (regional varieties dialects sociolects non-native varieties) that are different from the standard language. such varieties are often low-resource and hence do not benefit from contemporary nlp solutions mt included. we propose a general framework to rapidly adapt mt systems to generate language varieties that are close to but different from the standard target language using no parallel (source{--}variety) data. this also includes adaptation of mt systems to low-resource typologically-related target languages. we experiment with adapting an english{--}russian mt system to generate ukrainian and belarusian an english{--}norwegian bokm{\aa}l system to generate nynorsk and an english{--}arabic system to generate four arabic dialects obtaining significant improvements over competitive baselines.
794,ruder-sil-2021-multi,"   Multi-Domain Multilingual Question Answering"",
",question answering (qa) is one of the most challenging and impactful tasks in natural language processing. most research in qa however has focused on the open-domain or monolingual setting while most real-world applications deal with specific domains or languages. in this tutorial we attempt to bridge this gap. firstly we introduce standard benchmarks in multi-domain and multilingual qa. in both scenarios we discuss state-of-the-art approaches that achieve impressive performance ranging from zero-shot transfer learning to out-of-the-box training with open-domain qa systems. finally we will present open research problems that this new research agenda poses such as multi-task learning cross-lingual transfer learning domain adaptation and training large scale pre-trained multilingual language models.
795,martindale-2012-statistical,"   Can Statistical Post-Editing with a Small Parallel Corpus Save a Weak {MT} Engine?"",
",statistical post-editing has been shown in several studies to increase bleu score for rule-based mt systems. however previous studies have relied solely on bleu and have not conducted further study to determine whether those gains indicated an increase in quality or in score alone. in this work we conduct a human evaluation of statistical post-edited output from a weak rule-based mt system comparing the results with the output of the original rule-based system and a phrase-based statistical mt system trained on the same data. we show that for this weak rule-based system despite significant bleu score increases human evaluators prefer the output of the original system. while this is not a generally conclusive condemnation of statistical post-editing this result does cast doubt on the efficacy of statistical post-editing for weak mt systems and on the reliability of bleu score for comparison between weak rule-based and hybrid systems built from them.
796,apresjan-etal-2006-syntactically,"   A Syntactically and Semantically Tagged Corpus of {R}ussian: State of the Art and Prospects"",
",we describe a project aimed at creating a deeply annotated corpus of russian texts. the annotation consists of comprehensive morphological marking syntactic tagging in the form of a complete dependency tree and semantic tagging within a restricted semantic dictionary. syntactic tagging is using about 80 dependency relations. the syntactically annotated corpus counts more than 28000 sentences and makes an autonomous part of the russian national corpus (www.ruscorpora.ru). semantic tagging is based on an inventory of semantic features (descriptors) and a dictionary comprising about 3000 entries with a set of tags assigned to each lexeme and its argument slots. the set of descriptors assigned to words has been designed in such a way as to construct a linguistically relevant classification for the whole russian vocabulary. this classification serves for discovering laws according to which the elements of various lexical and semantic classes interact in the texts. the inventory of semantic descriptors consists of two parts object descriptors (about 90 items in total) and predicate descriptors (about a hundred). a set of semantic roles is thoroughly elaborated and contains about 50 roles.
797,reeder-white-2003-granularity,"   Granularity in {MT} evaluation"",
",this paper looks at granularity issues in machine translation evaluation. we start with work by (white 2001) who examined the correlation between intelligibility and fidelity at the document level. his work showed that intelligibility and fidelity do not correlate well at the document level. these dissimilarities lead to our investigation of evaluation granularity. in particular we revisit the intelligibility and fidelity relationship at the corpus level. we expect these to support certain assumptions in both evaluations as well as indicate issues germane to future evaluations.
798,wang-etal-2018-translating,"   Translating a Math Word Problem to a Expression Tree"",
",sequence-to-sequence (seq2seq) models have been successfully applied to automatic math word problem solving. despite its simplicity a drawback still remains: a math word problem can be correctly solved by more than one equations. this non-deterministic transduction harms the performance of maximum likelihood estimation. in this paper by considering the uniqueness of expression tree we propose an equation normalization method to normalize the duplicated equations. moreover we analyze the performance of three popular seq2seq models on the math word problem solving. we find that each model has its own specialty in solving problems consequently an ensemble model is then proposed to combine their advantages. experiments on dataset math23k show that the ensemble model with equation normalization significantly outperforms the previous state-of-the-art methods.
799,gregoire-langlais-2018-extracting,"   Extracting Parallel Sentences with Bidirectional Recurrent Neural Networks to Improve Machine Translation"",
",parallel sentence extraction is a task addressing the data sparsity problem found in multilingual natural language processing applications. we propose a bidirectional recurrent neural network based approach to extract parallel sentences from collections of multilingual texts. our experiments with noisy parallel corpora show that we can achieve promising results against a competitive baseline by removing the need of specific feature engineering or additional external resources. to justify the utility of our approach we extract sentence pairs from wikipedia articles to train machine translation systems and show significant improvements in translation performance.
800,todirascu-etal-2012-french,"   {F}rench and {G}erman Corpora for Audience-based Text Type Classification"",
",this paper presents some of the results of the classyn project which investigated the classification of text according to audience-related text types. we describe the design principles and the properties of the french and german linguistically annotated corpora that we have created. we report on tools used to collect the data and on the quality of the syntactic annotation. the classyn corpora comprise two text collections to investigate general text types difference between scientific and popular science text on the two domains of medical and computer science.
801,sarsfield-tayyar-madabushi-2020-uob,"   {U}o{B} at {S}em{E}val-2020 Task 1: Automatic Identification of Novel Word Senses"",
",much as the social landscape in which languages are spoken shifts language too evolves to suit the needs of its users. lexical semantic change analysis is a burgeoning field of semantic analysis which aims to trace changes in the meanings of words over time. this paper presents an approach to lexical semantic change detection based on bayesian word sense induction suitable for novel word sense identification. this approach is used for a submission to semeval-2020 task 1 which shows the approach to be capable of the semeval task. the same approach is also applied to a corpus gleaned from 15 years of twitter data the results of which are then used to identify words which may be instances of slang.
802,chesley-salmon-alt-2006-automatic,"   Automatic extraction of subcategorization frames for {F}rench"",
",this paper describes the automatic extraction of french subcategorization frames from corpora. the subcategorization frames have been acquired via visl a dependency-based parser (bick 2003) whose verb lexicon is currently incomplete with respect to subcategorization frames. therefore we have implemented binomial hypothesis testing as a post-parsing filtering step. on a test set of 104 frequent verbs we achieve lower bounds on type precision at 86.8{\%} and on token recall at 54.3{\%}. these results show that contra (korhonen et al. 2000) binomial hypothesis testing can be robust for determining subcategorization frames given corpus data. additionally we estimate that our extracted subcategorization frames account for 85.4{\%} of all frames in french corpora. we conclude that using a language resource such as the visl parser with a currently unevaluated (and potentially high) error rate can yield robust results in conjunction with probabilistic filtering of the resource output.
803,schulz-etal-2018-stochastic,"   A Stochastic Decoder for Neural Machine Translation"",
",the process of translation is ambiguous in that there are typically many valid translations for a given sentence. this gives rise to significant variation in parallel corpora however most current models of machine translation do not account for this variation instead treating the problem as a deterministic process. to this end we present a deep generative model of machine translation which incorporates a chain of latent variables in order to account for local lexical and syntactic variation in parallel corpora. we provide an in-depth analysis of the pitfalls encountered in variational inference for training deep generative models. experiments on several different language pairs demonstrate that the model consistently improves over strong baselines.
804,gupta-etal-2021-product-review,"   Product Review Translation using Phrase Replacement and Attention Guided Noise Augmentation"",
",product reviews provide valuable feedback of the customers and however and they are available today only in english on most of the e-commerce platforms. the nature of reviews provided by customers in any multilingual country poses unique challenges for machine translation such as code-mixing and ungrammatical sentences and presence of colloquial terms and lack of e-commerce parallel corpus etc. given that 44{\%} of indian population speaks and operates in hindi language and we address the above challenges by presenting an english{--}to{--}hindi neural machine translation (nmt) system to translate the product reviews available on e-commerce websites by creating an in-domain parallel corpora and handling various types of noise in reviews via two data augmentation techniques and viz. (i). a novel phrase augmentation technique (phrrep) where the syntactic noun phrases in sentences are replaced by the other noun phrases carrying different meanings but in similar context; and (ii). a novel attention guided noise augmentation (attnnoise) technique to make our nmt model robust towards various noise. evaluation shows that using the proposed augmentation techniques we achieve a 6.67 bleu score improvement over the baseline model. in order to show that our proposed approach is not language-specific and we also perform experiments for two other language pairs and viz. en-fr (mtnt18 corpus) and en-de (iwslt17) that yield the improvements of 2.55 and 0.91 bleu points and respectively and over the baselines.
805,boratko-etal-2018-systematic,"   A Systematic Classification of Knowledge, Reasoning, and Context within the {ARC} Dataset"",
",the recent work of clark et al. (2018) introduces the ai2 reasoning challenge (arc) and the associated arc dataset that partitions open domain complex science questions into easy and challenge sets. that paper includes an analysis of 100 questions with respect to the types of knowledge and reasoning required to answer them; however it does not include clear definitions of these types nor does it offer information about the quality of the labels. we propose a comprehensive set of definitions of knowledge and reasoning types necessary for answering the questions in the arc dataset. using ten annotators and a sophisticated annotation interface we analyze the distribution of labels across the challenge set and statistics related to them. additionally we demonstrate that although naive information retrieval methods return sentences that are irrelevant to answering the query sufficient supporting text is often present in the (arc) corpus. evaluating with human-selected relevant sentences improves the performance of a neural machine comprehension model by 42 points.
806,kshirsagar-etal-2017-detecting,"   Detecting and Explaining Crisis"",
",individuals on social media may reveal themselves to be in various states of crisis (e.g. suicide self-harm abuse or eating disorders). detecting crisis from social media text automatically and accurately can have profound consequences. however detecting a general state of crisis without explaining why has limited applications. an explanation in this context is a coherent concise subset of the text that rationalizes the crisis detection. we explore several methods to detect and explain crisis using a combination of neural and non-neural techniques. we evaluate these techniques on a unique data set obtained from koko an anonymous emotional support network available through various messaging applications. we annotate a small subset of the samples labeled with crisis with corresponding explanations. our best technique significantly outperforms the baseline for detection and explanation.
807,prudhommeaux-etal-2017-vector,"   Vector space models for evaluating semantic fluency in autism"",
",a common test administered during neurological examination is the semantic fluency test in which the patient must list as many examples of a given semantic category as possible under timed conditions. poor performance is associated with neurological conditions characterized by impairments in executive function such as dementia schizophrenia and autism spectrum disorder (asd). methods for analyzing semantic fluency responses at the level of detail necessary to uncover these differences have typically relied on subjective manual annotation. in this paper we explore automated approaches for scoring semantic fluency responses that leverage ontological resources and distributional semantic models to characterize the semantic fluency responses produced by young children with and without asd. using these methods we find significant differences in the semantic fluency responses of children with asd demonstrating the utility of using objective methods for clinical language analysis.
808,gao-etal-2021-hierarchical,"   Hierarchical Character Tagger for Short Text Spelling Error Correction"",
",state-of-the-art approaches to spelling error correction problem include transformer-based seq2seq models which require large training sets and suffer from slow inference time; and sequence labeling models based on transformer encoders like bert which involve token-level label space and therefore a large pre-defined vocabulary dictionary. in this paper we present a hierarchical character tagger model or hctagger for short text spelling error correction. we use a pre-trained language model at the character level as a text encoder and then predict character-level edits to transform the original text into its error-free form with a much smaller label space. for decoding we propose a hierarchical multi-task approach to alleviate the issue of long-tail label distribution without introducing extra model parameters. experiments on two public misspelling correction datasets demonstrate that hctagger is an accurate and much faster approach than many existing models.
809,beltagy-etal-2021-overview,"   Overview of the Second Workshop on Scholarly Document Processing"",
",with the ever-increasing pace of research and high volume of scholarly communication scholars face a daunting task. not only must they keep up with the growing literature in their own and related fields scholars increasingly also need to rebut pseudo-science and disinformation. these needs have motivated an increasing focus on computational methods for enhancing search summarization and analysis of scholarly documents. however the various strands of research on scholarly document processing remain fragmented. to reach out to the broader nlp and ai/ml community pool distributed efforts in this area and enable shared access to published research we held the 2nd workshop on scholarly document processing (sdp) at naacl 2021 as a virtual event (https://sdproc.org/2021/). the sdp workshop consisted of a research track three invited talks and three shared tasks (longsumm 2021 sciver and 3c). the program was geared towards the application of nlp information retrieval and data mining for scholarly documents with an emphasis on identifying and providing solutions to open challenges.
810,bowman-dahl-2021-will,"   What Will it Take to Fix Benchmarking in Natural Language Understanding?"",
",evaluation for many natural language understanding (nlu) tasks is broken: unreliable and biased systems score so highly on standard benchmarks that there is little room for researchers who develop better systems to demonstrate their improvements. the recent trend to abandon iid benchmarks in favor of adversarially-constructed out-of-distribution test sets ensures that current models will perform poorly but ultimately only obscures the abilities that we want our benchmarks to measure. in this position paper we lay out four criteria that we argue nlu benchmarks should meet. we argue most current benchmarks fail at these criteria and that adversarial data collection does not meaningfully address the causes of these failures. instead restoring a healthy evaluation ecosystem will require significant progress in the design of benchmark datasets the reliability with which they are annotated their size and the ways they handle social bias.
811,yasunaga-etal-2021-qa,"   {QA}-{GNN}: Reasoning with Language Models and Knowledge Graphs for Question Answering"",
",the problem of answering questions using knowledge from pre-trained language models (lms) and knowledge graphs (kgs) presents two challenges: given a qa context (question and answer choice) methods need to (i) identify relevant knowledge from large kgs and (ii) perform joint reasoning over the qa context and kg. here we propose a new model qa-gnn which addresses the above challenges through two key innovations: (i) relevance scoring where we use lms to estimate the importance of kg nodes relative to the given qa context and (ii) joint reasoning where we connect the qa context and kg to form a joint graph and mutually update their representations through graph-based message passing. we evaluate qa-gnn on the commonsenseqa and openbookqa datasets and show its improvement over existing lm and lm+kg models as well as its capability to perform interpretable and structured reasoning e.g. correctly handling negation in questions.
812,dahlmann-etal-2017-neural,"   Neural Machine Translation Leveraging Phrase-based Models in a Hybrid Search"",
",in this paper we introduce a hybrid search for attention-based neural machine translation (nmt). a target phrase learned with statistical mt models extends a hypothesis in the nmt beam search when the attention of the nmt model focuses on the source words translated by this phrase. phrases added in this way are scored with the nmt model but also with smt features including phrase-level translation probabilities and a target language model. experimental results on german-to-english news domain and english-to-russian e-commerce domain translation tasks show that using phrase-based models in nmt search improves mt quality by up to 2.3{\%} bleu absolute as compared to a strong nmt baseline.
813,andreou-petitjean-2017-describing,"   Describing derivational polysemy with {XMG}"",
",in this paper we model and test the monosemy and polysemy approaches to derivational multiplicity of meaning using frame semantics and xmg. in order to illustrate our claims and proposals we use data from deverbal nominalizations with the suffix -al on verbs of change of possession (e.g. rental disbursal). in our xmg implementation we show that the underspecified meaning of affixes cannot always be reduced to a single unitary meaning and that the polysemy approach to multiplicity of meaning is more judicious compared to the monosemy approach. we also introduce constraints on the potential referents of derivatives. these constraints have the form of type constraints and specify which arguments in the frame of the verbal base are compatible with the referential argument of the derivative. the introduction of type constraints rules out certain readings because frame unification only succeeds if types are compatible.
814,hutin-etal-2020-lenition,"   L{\'e}nition et fortition des occlusives en coda finale dans deux langues romanes : le fran{\c{c}}ais et le roumain (Lenition and fortition of word-final stops in two {R}omance languages: {F}rench and {R}omanian)"",
",l{'}exploration automatis{\'e}e de grands corpus permet d{'}analyser plus finement la relation entre motifs de variation phon{\'e}tique synchronique et changements diachroniques : les erreurs dans les transcriptions automatiques sont riches d{'}enseignements sur la variation contextuelle en parole continue et sur les possibles mutations syst{\'e}miques sur le point d{'}appara{\^\i}tre. d{\`e}s lors il est int{\'e}ressant de se pencher sur des ph{\'e}nom{\`e}nes phonologiques largement attest{\'e}s dans les langues en diachronie comme en synchronie pour {\'e}tablir leur {\'e}mergence ou non dans des langues qui n{'}y sont pas encore sujettes. la pr{\'e}sente {\'e}tude propose donc d{'}utiliser l{'}alignement forc{\'e} avec variantes de prononciation pour observer les alternances de voisement en coda finale de mot dans deux langues romanes : le fran{\c{c}}ais et le roumain. il sera mis en {\'e}vidence notamment que voisement et d{\'e}voisement non-canoniques des codas fran{\c{c}}aises comme roumaines ne sont pas le fruit du hasard mais bien des instances de d{\'e}voisement final et d{'}assimilation r{\'e}gressive de trait laryngal qu{'}il s{'}agisse de voisement ou de non-voisement.
815,spliethover-wachsmuth-2020-argument,"   Argument from Old Man{'}s View: Assessing Social Bias in Argumentation"",
",social bias in language - towards genders ethnicities ages and other social groups - poses a problem with ethical impact for many nlp applications. recent research has shown that machine learning models trained on respective data may not only adopt but even amplify the bias. so far however little attention has been paid to bias in computational argumentation. in this paper we study the existence of social biases in large english debate portals. in particular we train word embedding models on portal-specific corpora and systematically evaluate their bias using weat an existing metric to measure bias in word embeddings. in a word co-occurrence analysis we then investigate causes of bias. the results suggest that all tested debate corpora contain unbalanced and biased data mostly in favor of male people with european-american names. our empirical insights contribute towards an understanding of bias in argumentative data sources.
816,hamed-etal-2020-arzen,"   {A}rz{E}n: A Speech Corpus for Code-switched {E}gyptian {A}rabic-{E}nglish"",
",in this paper we present our arzen corpus an egyptian arabic-english code-switching (cs) spontaneous speech corpus. the corpus is collected through informal interviews with 38 egyptian bilingual university students and employees held in a soundproof room. a total of 12 hours are recorded transcribed validated and sentence segmented. the corpus is mainly designed to be used in automatic speech recognition (asr) systems however it also provides a useful resource for analyzing the cs phenomenon from linguistic sociological and psychological perspectives. in this paper we first discuss the cs phenomenon in egypt and the factors that gave rise to the current language. we then provide a detailed description on how the corpus was collected giving an overview on the participants involved. we also present statistics on the cs involved in the corpus as well as a summary to the effort exerted in the corpus development in terms of number of hours required for transcription validation segmentation and speaker annotation. finally we discuss some factors contributing to the complexity of the corpus as well as arabic-english cs behaviour that could pose potential challenges to asr systems.
817,zsibrita-etal-2013-magyarlanc,"   Token and Type Constraints for Cross-Lingual Part-of-Speech Tagging"",
",we consider the construction of part-of-speech taggers for resource-poor languages. recently manually constructed tag dictionaries from wiktionary and dictionaries projected via bitext have been used as type constraints to overcome the scarcity of annotated data in this setting. in this paper we show that additional token constraints can be projected from a resource-rich source language to a resource-poor target language via word-aligned bitext. we present several models to this end; in particular a partially observed conditional random field model where coupled token and type constraints provide a partial signal for training. averaged across eight previously studied indo-european languages our model achieves a 25{\%} relative error reduction over the prior state of the art. we further present successful results on seven additional languages from different families empirically demonstrating the applicability of coupled token and type constraints across a diverse set of languages.
818,kallel-jaoua-etal-2008-integration,"   Int{\'e}gration d{'}une {\'e}tape de pr{\'e}-filtrage et d{'}une fonction multiobjectif en vue d{'}am{\'e}liorer le syst{\`e}me {E}xtra{N}ews de r{\'e}sum{\'e} de documents multiples"",
",dans cet article nous pr{\'e}sentons les am{\'e}liorations que nous avons apport{\'e}es au syst{\`e}me extranews de r{\'e}sum{\'e} automatique de documents multiples. ce syst{\`e}me se base sur l{'}utilisation d{'}un algorithme g{\'e}n{\'e}tique qui permet de combiner les phrases des documents sources pour former les extraits qui seront crois{\'e}s et mut{\'e}s pour g{\'e}n{\'e}rer de nouveaux extraits. la multiplicit{\'e} des crit{\`e}res de s{\'e}lection d{'}extraits nous a inspir{\'e} une premi{\`e}re am{\'e}lioration qui consiste {\`a} utiliser une technique d{'}optimisation multi-objectif en vue d{'}{\'e}valuer ces extraits. la deuxi{\`e}me am{\'e}lioration consiste {\`a} int{\'e}grer une {\'e}tape de pr{\'e}-filtrage de phrases qui a pour objectif la r{\'e}duction du nombre des phrases des textes sources en entr{\'e}e. une {\'e}valuation des am{\'e}liorations apport{\'e}es {\`a} notre syst{\`e}me est r{\'e}alis{\'e}e sur les corpus de duc{'}04 et duc{'}07.
819,miller-etal-2010-improving,"   Improving Personal Name Search in the {TIGR} System"",
",this paper describes the development and evaluation of enhancements to the specialized information retrieval capabilities of a multimodal reporting system. the system enables collection and dissemination of information through a distributed data architecture by allowing users to input free text documents which are indexed for subsequent search and retrieval by other users. this unstructured data entry method is essential for users of this system but it requires an intelligent support system for processing queries against the data. the system known as tigr (tactical ground reporting) allows keyword searching and geospatial filtering of results but lacked the ability to efficiently index and search person names and perform approximate name matching. to improve tigrs ability to provide accurate comprehensive results for queries on person names we iteratively updated existing entity extraction and name matching technologies to better align with the tigr use case. we evaluated each version of the entity extraction and name matching components to find the optimal configuration for the tigr context and combined those pieces into a named entity extraction indexing and search module that integrates with the current tigr system. by comparing system-level evaluations of the original and updated tigr search processes we show that our enhancements to personal name search significantly improved the performance of the overall information retrieval capabilities of the tigr system.
820,rotari-etal-2017-wild,"   Wild Devs{'} at {S}em{E}val-2017 Task 2: Using Neural Networks to Discover Word Similarity"",
",this paper presents wild devs{'} participation in the semeval-2017 task 2 {``}multi-lingual and cross-lingual semantic word similarity{''} which tries to automatically measure the semantic similarity between two words. the system was build using neural networks having as input a collection of word pairs whereas the output consists of a list of scores from 0 to 4 corresponding to the degree of similarity between the word pairs.
821,khalifa-etal-2016-yamama,"   {YAMAMA}: Yet Another Multi-Dialect {A}rabic Morphological Analyzer"",
",in this paper we present yamama a multi-dialect arabic morphological analyzer and disambiguator. our system is almost five times faster than the state-of-art madamira system with a slightly lower quality. in addition to speed yamama outputs a rich representation which allows for a wider spectrum of use. in this regard yamama transcends other systems such as farasa which is faster but provides specific outputs catering to specific applications.
822,jia-etal-2020-entity,"   Entity Enhanced {BERT} Pre-training for {C}hinese {NER}"",
",character-level bert pre-trained in chinese suffers a limitation of lacking lexicon information which shows effectiveness for chinese ner. to integrate the lexicon into pre-trained lms for chinese ner we investigate a semi-supervised entity enhanced bert pre-training method. in particular we first extract an entity lexicon from the relevant raw text using a new-word discovery method. we then integrate the entity information into bert using char-entity-transformer which augments the self-attention using a combination of character and entity representations. in addition an entity classification task helps inject the entity information into model parameters in pre-training. the pre-trained models are used for ner fine-tuning. experiments on a news dataset and two datasets annotated by ourselves for ner in long-text show that our method is highly effective and achieves the best results.
823,popovic-2019-evaluating,"   Evaluating Conjunction Disambiguation on {E}nglish-to-{G}erman and {F}rench-to-{G}erman {WMT} 2019 Translation Hypotheses"",
",we present a test set for evaluating an mt system{'}s capability to translate ambiguous conjunctions depending on the sentence structure. we concentrate on the english conjunction {``}but{''} and its french equivalent {``}mais{''} which can be translated into two different german conjunctions. we evaluate all english-to-german and french-to-german submissions to the wmt 2019 shared translation task. the evaluation is done mainly automatically with additional fast manual inspection of unclear cases. all systems almost perfectly recognise the target conjunction {``}aber{''} whereas accuracies for the other target conjunction {``}sondern{''} range from 78{\%} to 97{\%} and the errors are mostly caused by replacing it with the alternative conjunction {``}aber{''}. the best performing system for both language pairs is a multilingual transformer {``}tartunlp{''} system trained on all wmt 2019 language pairs which use the latin script indicating that the multilingual approach is beneficial for conjunction disambiguation. as for other system features such as using synthetic back-translated data context-aware hybrid etc. no particular (dis)advantages can be observed. qualitative manual inspection of translation hypotheses shown that highly ranked systems generally produce translations with high adequacy and fluency meaning that these systems are not only capable of capturing the right conjunction whereas the rest of the translation hypothesis is poor. on the other hand the low ranked systems generally exhibit lower fluency and poor adequacy.
824,vettigli-sorgente-2021-empna,"   {E}mp{N}a at {WASSA} 2021: A Lightweight Model for the Prediction of Empathy, Distress and Emotions from Reactions to News Stories"",
",this paper describes our submission for the wassa 2021 shared task regarding the prediction of empathy distress and emotions from news stories. the solution is based on combining the frequency of words lexicon-based information demographics of the annotators and personality of the annotators into a linear model. the prediction of empathy and distress is performed using linear regression while the prediction of emotions is performed using logistic regression. both tasks are performed using the same features. our models rank 4th for the prediction of emotions and 2nd for the prediction of empathy and distress. these results are particularly interesting when considered that the computational requirements of the solution are minimal.
825,hernandez-andreas-2021-low,"   The Low-Dimensional Linear Geometry of Contextualized Word Representations"",
",black-box probing models can reliably extract linguistic features like tense number and syntactic role from pretrained word representations. however the manner in which these features are encoded in representations remains poorly understood. we present a systematic study of the linear geometry of contextualized word representations in elmo and bert. we show that a variety of linguistic features (including structured dependency relationships) are encoded in low-dimensional subspaces. we then refine this geometric picture showing that there are hierarchical relations between the subspaces encoding general linguistic categories and more specific ones and that low-dimensional feature encodings are distributed rather than aligned to individual neurons. finally we demonstrate that these linear subspaces are causally related to model behavior and can be used to perform fine-grained manipulation of bert{'}s output distribution.
826,wu-yarowsky-2021-pronunciations,"   On Pronunciations in {W}iktionary: Extraction and Experiments on Multilingual Syllabification and Stress Prediction"",
",we constructed parsers for five non-english editions of wiktionary which combined with pronunciations from the english edition comprises over 5.3 million ipa pronunciations the largest pronunciation lexicon of its kind. this dataset is a unique comparable corpus of ipa pronunciations annotated from multiple sources. we analyze the dataset noting the presence of machine-generated pronunciations. we develop a novel visualization method to quantify syllabification. we experiment on the new combined task of multilingual ipa syllabification and stress prediction finding that training a massively multilingual neural sequence-to-sequence model with copy attention can improve performance on both high- and low-resource languages and multi-task training on stress prediction helps with syllabification.
827,marie-fujita-2019-unsupervised-joint,"   Unsupervised Joint Training of Bilingual Word Embeddings"",
",state-of-the-art methods for unsupervised bilingual word embeddings (bwe) train a mapping function that maps pre-trained monolingual word embeddings into a bilingual space. despite its remarkable results unsupervised mapping is also well-known to be limited by the original dissimilarity between the word embedding spaces to be mapped. in this work we propose a new approach that trains unsupervised bwe jointly on synthetic parallel data generated through unsupervised machine translation. we demonstrate that existing algorithms that jointly train bwe are very robust to noisy training data and show that unsupervised bwe jointly trained significantly outperform unsupervised mapped bwe in several cross-lingual nlp tasks.
828,abate-etal-2020-large,"   Large Vocabulary Read Speech Corpora for Four {E}thiopian Languages: {A}mharic, {T}igrigna, {O}romo, and {W}olaytta"",
",automatic speech recognition (asr) is one of the most important technologies to help people live a better life in the 21st century. however its development requires a big speech corpus for a language. the development of such a corpus is expensive especially for under-resourced ethiopian languages. to address this problem we have developed four medium-sized (longer than 22 hours each) speech corpora for four ethiopian languages: amharic tigrigna oromo and wolaytta. in a way of checking the usability of the corpora and deliver a baseline asr for each language. in this paper we present the corpora and the baseline asr systems for each language. the word error rates (wers) we achieved show that the corpora are usable for further investigation and we recommend the collection of text corpora to train strong language models for oromo and wolaytta compared to others.
829,gyanendro-singh-etal-2020-sentiment,"   Sentiment Analysis of Tweets using Heterogeneous Multi-layer Network Representation and Embedding"",
",sentiment classification on tweets often needs to deal with the problems of under-specificity noise and multilingual content. this study proposes a heterogeneous multi-layer network-based representation of tweets to generate multiple representations of a tweet and address the above issues. the generated representations are further ensembled and classified using a neural-based early fusion approach. further we propose a centrality aware random-walk for node embedding and tweet representations suitable for the multi-layer network. from various experimental analysis it is evident that the proposed method can address the problem of under-specificity noisy text and multilingual content present in a tweet and provides better classification performance than the text-based counterparts. further the proposed centrality aware based random walk provides better representations than unbiased and other biased counterparts.
830,jagannatha-yu-2020-calibrating,"   Calibrating Structured Output Predictors for Natural Language Processing"",
",we address the problem of calibrating prediction confidence for output entities of interest in natural language processing (nlp) applications. it is important that nlp applications such as named entity recognition and question answering produce calibrated confidence scores for their predictions especially if the applications are to be deployed in a safety-critical domain such as healthcare. however the output space of such structured prediction models are often too large to directly adapt binary or multi-class calibration methods. in this study we propose a general calibration scheme for output entities of interest in neural network based structured prediction models. our proposed method can be used with any binary class calibration scheme and a neural network model. additionally we show that our calibration method can also be used as an uncertainty-aware entity-specific decoding step to improve the performance of the underlying model at no additional training cost or data requirements. we show that our method outperforms current calibration techniques for named entity recognition part-of-speech tagging and question answering systems. we also observe an improvement in model performance from our decoding step across several tasks and benchmark datasets. our method improves the calibration and model performance on out-of-domain test scenarios as well.
831,dumitrache-etal-2018-crowdsourcing,"   Crowdsourcing Semantic Label Propagation in Relation Classification"",
",distant supervision is a popular method for performing relation extraction from text that is known to produce noisy labels. most progress in relation extraction and classification has been made with crowdsourced corrections to distant-supervised labels and there is evidence that indicates still more would be better. in this paper we explore the problem of propagating human annotation signals gathered for open-domain relation classification through the crowdtruth methodology for crowdsourcing that captures ambiguity in annotations by measuring inter-annotator disagreement. our approach propagates annotations to sentences that are similar in a low dimensional embedding space expanding the number of labels by two orders of magnitude. our experiments show significant improvement in a sentence-level multi-class relation classifier.
832,renner-etal-2021-end,"   An End-to-End Approach for Full Bridging Resolution"",
",in this article we describe our submission to the codi-crac 2021 shared task on anaphora resolution in dialogues {--} track br (gold). we demonstrate the performance of an end-to-end transformer-based higher-order coreference model finetuned for the task of full bridging. we find that while our approach is not effective at modeling the complexities of the task it performs well on bridging resolution suggesting a need for investigations into a robust anaphor identification model for future improvements.
833,emerson-2020-goals,"   What are the Goals of Distributional Semantics?"",
",distributional semantic models have become a mainstay in nlp providing useful features for downstream tasks. however assessing long-term progress requires explicit long-term goals. in this paper i take a broad linguistic perspective looking at how well current models can deal with various semantic challenges. given stark differences between models proposed in different subfields a broad perspective is needed to see how we could integrate them. i conclude that while linguistic insights can guide the design of model architectures future progress will require balancing the often conflicting demands of linguistic expressiveness and computational tractability.
834,sawhney-etal-2021-hypmix,"   {H}yp{M}ix: Hyperbolic Interpolative Data Augmentation"",
",interpolation-based regularisation methods for data augmentation have proven to be effective for various tasks and modalities. these methods involve performing mathematical operations over the raw input samples or their latent states representations - vectors that often possess complex hierarchical geometries. however these operations are performed in the euclidean space simplifying these representations which may lead to distorted and noisy interpolations. we propose hypmix a novel model- data- and modality-agnostic interpolative data augmentation technique operating in the hyperbolic space which captures the complex geometry of input and hidden state hierarchies better than its contemporaries. we evaluate hypmix on benchmark and low resource datasets across speech text and vision modalities showing that hypmix consistently outperforms state-of-the-art data augmentation techniques. in addition we demonstrate the use of hypmix in semi-supervised settings. we further probe into the adversarial robustness and qualitative inferences we draw from hypmix that elucidate the efficacy of the riemannian hyperbolic manifolds for interpolation-based data augmentation.
835,xue-kulick-2003-automatic,"   Automatic predicate argument structure analysis of the {P}enn {C}hinese Treebank"",
",recent work in machine translation and information extraction has demonstrated the utility of a level that represents the predicate-argument structure. it would be especially useful for machine translation to have two such proposition banks one for each language under consideration. a proposition bank for english has been developed over the last few years and we describe here our development of a tool for facilitating the development of a chinese proposition bank. we also discuss some issues specific to the chinese treebank that complicate the matter of mapping syntactic representation to a predicate-argument level and report on some preliminary evaluation of the accuracy of the semantic tagging tool.
836,banerjee-etal-2021-neural,"   Neural Machine Translation in Low-Resource Setting: a Case Study in {E}nglish-{M}arathi Pair"",
",in this paper and we explore different techniques of overcoming the challenges of low-resource in neural machine translation (nmt) and specifically focusing on the case of english-marathi nmt. nmt systems require a large amount of parallel corpora to obtain good quality translations. we try to mitigate the low-resource problem by augmenting parallel corpora or by using transfer learning. techniques such as phrase table injection (pti) and back-translation and mixing of language corpora are used for enhancing the parallel data; whereas pivoting and multilingual embeddings are used to leverage transfer learning. for pivoting and hindi comes in as assisting language for english-marathi translation. compared to baseline transformer model and a significant improvement trend in bleu score is observed across various techniques. we have done extensive manual and automatic and qualitative evaluation of our systems. since the trend in machine translation (mt) today is post-editing and measuring of human effort reduction (her) and we have given our preliminary observations on translation edit rate (ter) vs. bleu score study and where ter is regarded as a measure of her.
837,shen-etal-2019-progressive,"   A Progressive Model to Enable Continual Learning for Semantic Slot Filling"",
",semantic slot filling is one of the major tasks in spoken language understanding (slu). after a slot filling model is trained on precollected data it is crucial to continually improve the model after deployment to learn users{'} new expressions. as the data amount grows it becomes infeasible to either store such huge data and repeatedly retrain the model on all data or fine tune the model only on new data without forgetting old expressions. in this paper we introduce a novel progressive slot filling model progmodel. progmodel consists of a novel context gate that transfers previously learned knowledge to a small size expanded component; and meanwhile enables this new component to be fast trained to learn from new data. as such progmodel learns the new knowledge by only using new data at each time and meanwhile preserves the previously learned expressions. our experiments show that progmodel needs much less training time and smaller model size to outperform various model fine tuning competitors by up to 4.24{\%} and 3.03{\%} on two benchmark datasets.
838,santosh-etal-2020-detecting,"   Detecting Emerging Symptoms of {COVID}-19 using Context-based {T}witter Embeddings"",
",in this paper we present an iterative graph-based approach for the detection of symptoms of covid-19 the pathology of which seems to be evolving. more generally the method can be applied to finding context-specific words and texts (e.g. symptom mentions) in large imbalanced corpora (e.g. all tweets mentioning }{\#}covid-19). given the novelty of covid-19 we also test if the proposed approach generalizes to the problem of detecting adverse drug reaction (adr). we find that the approach applied to twitter data can detect symptom mentions substantially before to their being reported by the centers for disease control (cdc).
839,ji-etal-2021-hierarchical,"   Hierarchical Context-aware Network for Dense Video Event Captioning"",
",dense video event captioning aims to generate a sequence of descriptive captions for each event in a long untrimmed video. video-level context provides important information and facilities the model to generate consistent and less redundant captions between events. in this paper we introduce a novel hierarchical context-aware network for dense video event captioning (hcn) to capture context from various aspects. in detail the model leverages local and global context with different mechanisms to jointly learn to generate coherent captions. the local context module performs full interaction between neighbor frames and the global context module selectively attends to previous or future events. according to our extensive experiment on both youcook2 and activitynet captioning datasets the video-level hcn model outperforms the event-level context-agnostic model by a large margin. the code is available at https://github.com/kirkguo/hcn.
840,kurita-etal-2019-measuring,"   Measuring Bias in Contextualized Word Representations"",
",contextual word embeddings such as bert have achieved state of the art performance in numerous nlp tasks. since they are optimized to capture the statistical properties of training data they tend to pick up on and amplify social stereotypes present in the data as well. in this study we (1) propose a template-based method to quantify bias in bert; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study evaluating gender bias in a downstream task of gender pronoun resolution. although our case study focuses on gender bias the proposed technique is generalizable to unveiling other biases including in multiclass settings such as racial and religious biases.
841,peng-etal-2020-corpus,"   A Corpus of Adpositional Supersenses for {M}andarin {C}hinese"",
",adpositions are frequent markers of semantic relations but they are highly ambiguous and vary significantly from language to language. moreover there is a dearth of annotated corpora for investigating the cross-linguistic variation of adposition semantics or for building multilingual disambiguation systems. this paper presents a corpus in which all adpositions have been semantically annotated in mandarin chinese; to the best of our knowledge this is the first chinese corpus to be broadly annotated with adposition semantics. our approach adapts a framework that defined a general set of supersenses according to ostensibly language-independent semantic criteria though its development focused primarily on english prepositions (schneider et al. 2018). we find that the supersense categories are well-suited to chinese adpositions despite syntactic differences from english. on a mandarin translation of the little prince we achieve high inter-annotator agreement and analyze semantic correspondences of adposition tokens in bitext.
842,rasooli-etal-2021-wikily,"   {``}Wikily{''} Supervised Neural Translation Tailored to Cross-Lingual Tasks"",
",we present a simple but effective approach for leveraging wikipedia for neural machine translation as well as cross-lingual tasks of image captioning and dependency parsing without using any direct supervision from external parallel data or supervised models in the target language. we show that first sentences and titles of linked wikipedia pages as well as cross-lingual image captions are strong signals for a seed parallel data to extract bilingual dictionaries and cross-lingual word embeddings for mining parallel text from wikipedia. our final model achieves high bleu scores that are close to or sometimes higher than strong \textit{supervised} baselines in low-resource languages; e.g. supervised bleu of 4.0 versus 12.1 from our model in english-to-kazakh. moreover we tailor our \textit{wikily} translation models to unsupervised image captioning and cross-lingual dependency parser transfer. in image captioning we train a multi-tasking machine translation and image captioning pipeline for arabic and english from which the arabic training data is a \textit{wikily} translation of the english captioning data. our captioning results on arabic are slightly \textit{better} than that of its supervised model. in dependency parsing we translate a large amount of monolingual text and use it as an artificial training data in an \textit{annotation projection} framework. we show that our model outperforms recent work on cross-lingual transfer of dependency parsers.
843,saharia-etal-2020-non,"   Non-Autoregressive Machine Translation with Latent Alignments"",
",this paper presents two strong methods ctc and imputer for non-autoregressive machine translation that model latent alignments with dynamic programming. we revisit ctc for machine translation and demonstrate that a simple ctc model can achieve state-of-the-art for single-step non-autoregressive machine translation contrary to what prior work indicates. in addition we adapt the imputer model for non-autoregressive machine translation and demonstrate that imputer with just 4 generation steps can match the performance of an autoregressive transformer baseline. our latent alignment models are simpler than many existing non-autoregressive translation baselines; for example we do not require target length prediction or re-scoring with an autoregressive model. on the competitive wmt{'}14 en$\rightarrow$de task our ctc model achieves 25.7 bleu with a single generation step while imputer achieves 27.5 bleu with 2 generation steps and 28.0 bleu with 4 generation steps. this compares favourably to the autoregressive transformer baseline at 27.8 bleu.
844,li-etal-2020-composing,"   Composing Elementary Discourse Units in Abstractive Summarization"",
",in this paper we argue that elementary discourse unit (edu) is a more appropriate textual unit of content selection than the sentence unit in abstractive summarization. to well handle the problem of composing edus into an informative and fluent summary we propose a novel summarization method that first designs an edu selection model to extract and group informative edus and then an edu fusion model to fuse the edus in each group into one sentence. we also design the reinforcement learning mechanism to use edu fusion results to reward the edu selection action boosting the final summarization performance. experiments on cnn/daily mail have demonstrated the effectiveness of our model.
845,danlos-sagot-2010-ponctuations,"   Ponctuations fortes abusives"",
",certaines ponctuations fortes sont « abusivement » utilis{\'e}es {\`a} la place de ponctuations faibles d{\'e}bouchant sur des phrases graphiques qui ne sont pas des phrases grammaticales. cet article pr{\'e}sente une {\'e}tude sur corpus de ce ph{\'e}nom{\`e}ne et une {\'e}bauche d{'}outil pour rep{\'e}rer automatiquement les ponctuations fortes abusives.
846,liu-etal-2021-uor,"   {U}o{R} at {S}em{E}val-2021 Task 7: Utilizing Pre-trained {D}istil{BERT} Model and Multi-scale {CNN} for Humor Detection"",
",humour detection is an interesting but difficult task in nlp. because humorous might not be obvious in text it can be embedded into context hide behind the literal meaning and require prior knowledge to understand. we explored different shallow and deep methods to create a humour detection classifier for task 7-1a. models like logistic regression lstm mlp cnn were used and pre-trained models like distilbert were introduced to generate accurate vector representation for textual data. we focused on applying multi-scale strategy on modelling and compared different models. our best model is the distilbert+multiscale cnn it used different sizes of cnn kernel to get multiple scales of features which achieved 93.7{\%} f1-score and 92.1{\%} accuracy on the test set.
847,lee-etal-2019-manipulating,"   Manipulating the Difficulty of {C}-Tests"",
",we propose two novel manipulation strategies for increasing and decreasing the difficulty of c-tests automatically. this is a crucial step towards generating learner-adaptive exercises for self-directed language learning and preparing language assessment tests. to reach the desired difficulty level we manipulate the size and the distribution of gaps based on absolute and relative gap difficulty predictions. we evaluate our approach in corpus-based experiments and in a user study with 60 participants. we find that both strategies are able to generate c-tests with the desired difficulty level.
848,leng-etal-2021-fastcorrect-2,"   {F}ast{C}orrect 2: Fast Error Correction on Multiple Candidates for Automatic Speech Recognition"",
",error correction is widely used in automatic speech recognition (asr) to post-process the generated sentence and can further reduce the word error rate (wer). although multiple candidates are generated by an asr system through beam search current error correction approaches can only correct one sentence at a time failing to leverage the voting effect from multiple candidates to better detect and correct error tokens. in this work we propose fastcorrect 2 an error correction model that takes multiple asr candidates as input for better correction accuracy. fastcorrect 2 adopts non-autoregressive generation for fast inference which consists of an encoder that processes multiple source sentences and a decoder that generates the target sentence in parallel from the adjusted source sentence where the adjustment is based on the predicted duration of each source token. however there are some issues when handling multiple source sentences. first it is non-trivial to leverage the voting effect from multiple source sentences since they usually vary in length. thus we propose a novel alignment algorithm to maximize the degree of token alignment among multiple sentences in terms of token and pronunciation similarity. second the decoder can only take one adjusted source sentence as input while there are multiple source sentences. thus we develop a candidate predictor to detect the most suitable candidate for the decoder. experiments on our inhouse dataset and aishell-1 show that fastcorrect 2 can further reduce the wer over the previous correction model with single candidate by 3.2{\%} and 2.6{\%} demonstrating the effectiveness of leveraging multiple candidates in asr error correction. fastcorrect 2 achieves better performance than the cascaded re-scoring and correction pipeline and can serve as a unified post-processing module for asr.
849,dan-etal-2020-locally,"   A Locally Linear Procedure for Word Translation"",
",learning a mapping between word embeddings of two languages given a dictionary is an important problem with several applications. a common mapping approach is using an orthogonal matrix. the orthogonal procrustes analysis (pa) algorithm can be applied to find the optimal orthogonal matrix. this solution restricts the expressiveness of the translation model which may result in sub-optimal translations. we propose a natural extension of the pa algorithm that uses multiple orthogonal translation matrices to model the mapping and derive an algorithm to learn these multiple matrices. we achieve better performance in a bilingual word translation task and a cross-lingual word similarity task compared to the single matrix baseline. we also show how multiple matrices can model multiple senses of a word.
850,tsai-etal-2021-mining,"   Mining Commonsense and Domain Knowledge from Math Word Problems"",
",current neural math solvers learn to incorporate commonsense or domain knowledge by utilizing pre-specified constants or formulas. however as these constants and formulas are mainly human-specified the generalizability of the solvers is limited. in this paper we propose to explicitly retrieve the required knowledge from math problemdatasets. in this way we can determinedly characterize the required knowledge andimprove the explainability of solvers. our two algorithms take the problem text andthe solution equations as input. then they try to deduce the required commonsense and domain knowledge by integrating information from both parts. we construct two math datasets and show the effectiveness of our algorithms that they can retrieve the required knowledge for problem-solving.
851,rakshit-2019-joint,"   Joint Inference on Bilingual Parse Trees for {PP}-attachment Disambiguation"",
",prepositional phrase (pp) attachment is a classical problem in nlp for languages like english which suffer from structural ambiguity. in this work we solve this problem with the help of another language free from such ambiguities using the parse tree of the parallel sentence in the other language and word alignments. we formulate an optimization framework that encourages agreement between the parse trees for two languages and solve it using a novel dual decomposition (dd) based algorithm. experiments on the english-hindi language pair show promising improvements over the baseline.
852,pei-li-2018-s2spmn,"   {S}2{SPMN}: A Simple and Effective Framework for Response Generation with Relevant Information"",
",how to generate relevant and informative responses is one of the core topics in response generation area. following the task formulation of machine translation previous works mainly consider response generation task as a mapping from a source sentence to a target sentence. to realize this mapping existing works tend to design intuitive but complex models. however the relevant information existed in large dialogue corpus is mainly overlooked. in this paper we propose sequence to sequence with prototype memory network (s2spmn) to exploit the relevant information provided by the large dialogue corpus to enhance response generation. specifically we devise two simple approaches in s2spmn to select the relevant information (named prototypes) from the dialogue corpus. these prototypes are then saved into prototype memory network (pmn). furthermore a hierarchical attention mechanism is devised to extract the semantic information from the pmn to assist the response generation process. empirical studies reveal the advantage of our model over several classical and strong baselines.
853,zagar-robnik-sikonja-2021-unsupervised,"   Unsupervised Approach to Multilingual User Comments Summarization"",
",user commenting is a valuable feature of many news outlets enabling them a contact with readers and enabling readers to express their opinion provide different viewpoints and even complementary information. yet large volumes of user comments are hard to filter let alone read and extract relevant information. the research on the summarization of user comments is still in its infancy and human-created summarization datasets are scarce especially for less-resourced languages. to address this issue we propose an unsupervised approach to user comments summarization which uses a modern multilingual representation of sentences together with standard extractive summarization techniques. our comparison of different sentence representation approaches coupled with different summarization approaches shows that the most successful combinations are the same in news and comment summarization. the empirical results and presented visualisation show usefulness of the proposed methodology for several languages.
854,ni-mcauley-2018-personalized,"   Personalized Review Generation By Expanding Phrases and Attending on Aspect-Aware Representations"",
",in this paper we focus on the problem of building assistive systems that can help users to write reviews. we cast this problem using an encoder-decoder framework that generates personalized reviews by expanding short phrases (e.g. review summaries product titles) provided as input to the system. we incorporate aspect-level information via an aspect encoder that learns aspect-aware user and item representations. an attention fusion layer is applied to control generation by attending on the outputs of multiple encoders. experimental results show that our model successfully learns representations capable of generating coherent and diverse reviews. in addition the learned aspect-aware representations discover those aspects that users are more inclined to discuss and bias the generated text toward their personalized aspect preferences.
855,cumbicus-pineda-etal-2021-syntax,"   A Syntax-Aware Edit-based System for Text Simplification"",
",edit-based text simplification systems have attained much attention in recent years due to their ability to produce simplification solutions that are interpretable as well as requiring less training examples compared to traditional seq2seq systems. edit-based systems learn edit operations at a word level but it is well known that many of the operations performed when simplifying text are of a syntactic nature. in this paper we propose to add syntactic information into a well known edit-based system. we extend the system with a graph convolutional network module that mimics the dependency structure of the sentence thus giving the model an explicit representation of syntax. we perform a series of experiments in english spanish and italian and report improvements of the state of the art in four out of five datasets. further analysis shows that syntactic information is always beneficial and suggest that syntax is more helpful in complex sentences.
856,park-etal-2020-suicidal,"   Suicidal Risk Detection for Military Personnel"",
",we analyze social media for detecting the suicidal risk of military personnel which is especially crucial for countries with compulsory military service such as the republic of korea. from a widely-used korean social q{\&}a site we collect posts containing military-relevant content written by active-duty military personnel. we then annotate the posts with two groups of experts: military experts and mental health experts. our dataset includes 2791 posts with 13955 corresponding expert annotations of suicidal risk levels and this dataset is available to researchers who consent to research ethics agreement. using various fine-tuned state-of-the-art language models we predict the level of suicide risk reaching .88 f1 score for classifying the risks.
857,lee-etal-2021-discriminative,"   Discriminative Reranking for Neural Machine Translation"",
",reranking models enable the integration of rich features to select a better output hypothesis within an n-best list or lattice. these models have a long history in nlp and we revisit discriminative reranking for modern neural machine translation models by training a large transformer architecture. this takes as input both the source sentence as well as a list of hypotheses to output a ranked list. the reranker is trained to predict the observed distribution of a desired metric e.g. bleu over the n-best list. since such a discriminator contains hundreds of millions of parameters we improve its generalization using pre-training and data augmentation techniques. experiments on four wmt directions show that our discriminative reranking approach is effective and complementary to existing generative reranking approaches yielding improvements of up to 4 bleu over the beam search output.
858,sagot-2005-les,"   Les M{\'e}ta-{RCG}: description et mise en oeuvre"",
",nous pr{\'e}sentons dans cet article un nouveau formalisme linguistique qui repose sur les grammaires {\`a} concat{\'e}nation d{'}intervalles (rcg) appel{\'e} m{\'e}ta-rcg. nous exposons tout d{'}abord pourquoi la non-lin{\'e}arit{\'e} permet une repr{\'e}sentation ad{\'e}quate des ph{\'e}nom{\`e}nes linguistiques et en particulier de l{'}interaction entre les diff{\'e}rents niveaux de description. puis nous pr{\'e}sentons les m{\'e}ta-rcg et les concepts linguistiques suppl{\'e}mentaires qu{'}elles mettent en oeuvre tout en restant convertibles en rcg classiques. nous montrons que les analyses classiques (constituants d{\'e}pendances topologie s{\'e}mantique pr{\'e}dicat-arguments) peuvent {\^e}tre obtenues par projection partielle d{'}une analyse m{\'e}ta-rcg compl{\`e}te. enfin nous d{\'e}crivons la grammaire du fran{\c{c}}ais que nous d{\'e}veloppons dans ce nouveau formalisme et l{'}analyseur efficace qui en d{\'e}coule. nous illustrons alors la notion de projection partielle sur un exemple.
859,bhardwaj-etal-2019-carb,"   {C}a{RB}: A Crowdsourced Benchmark for Open {IE}"",
",open information extraction (open ie) systems have been traditionally evaluated via manual annotation. recently an automated evaluator with a benchmark dataset (oie2016) was released {--} it scores open ie systems automatically by matching system predictions with predictions in the benchmark dataset. unfortunately our analysis reveals that its data is rather noisy and the tuple matching in the evaluator has issues making the results of automated comparisons less trustworthy. we contribute carb an improved dataset and framework for testing open ie systems. to the best of our knowledge carb is the first crowdsourced open ie dataset and it also makes substantive changes in the matching code and metrics. nlp experts annotate carb{'}s dataset to be more accurate than oie2016. moreover we find that on one pair of open ie systems carb framework provides contradictory results to oie2016. human assessment verifies that carb{'}s ranking of the two systems is the accurate ranking. we release the carb framework along with its crowdsourced dataset.
860,fohr-mella-2012-coalt,"   {C}o{ALT}: A Software for Comparing Automatic Labelling Tools"",
",speech-text alignment tools are frequently used in speech technology and research. in this paper we propose a gpl software coalt (comparing automatic labelling tools) for comparing two automatic labellers or two speech-text alignment tools ranking them and displaying statistics about their differences. the main feature of coalt is that a user can define its own criteria for evaluating and comparing the speech-text alignment tools since the required quality for labelling depends on the targeted application. beyond ranking our tool provides useful statistics for each labeller and above all about their differences and can emphasize the drawbacks and advantages of each labeller. we have applied our software for the french and english languages but it can be used for another language by simply defining the list of the phonetic symbols and optionally a set of phonetic rules. in this paper we present the usage of the software for comparing two automatic labellers on the corpus timit. moreover as automatic labelling tools are configurable (number of gmms phonetic lexicon acoustic parameterisation) we then present how coalt allows to determine the best parameters for our automatic labelling tool.
861,winterstein-etal-2020-cantomap,"   {C}anto{M}ap: a {H}ong {K}ong {C}antonese {M}ap{T}ask Corpus"",
",this work reports on the construction of a corpus of connected spoken hong kong cantonese. the corpus aims at providing an additional resource for the study of modern (hong kong) cantonese and also involves several controlled elicitation tasks which will serve different projects related to the phonology and semantics of cantonese. the word-segmented corpus offers recordings phonemic transcription and chinese characters transcription. the corpus contains a total of 768 minutes of recordings and transcripts of forty speakers. all the audio material has been aligned at utterance level with the transcriptions using the elan transcription and annotation tool. the controlled elicitation task was based on the design of hcrc maptask corpus (anderson et al. 1991) in which participants had to communicate using solely verbal means as eye contact was restricted. in this paper we outline the design of the maps and their landmarks and the basic segmentation principles of the data and various transcription conventions we adopted. we also compare the contents of cantomap to those of comparable cantonese corpora.
862,wang-etal-2018-toward,"   Toward Fast and Accurate Neural Discourse Segmentation"",
",discourse segmentation which segments texts into elementary discourse units is a fundamental step in discourse analysis. previous discourse segmenters rely on complicated hand-crafted features and are not practical in actual use. in this paper we propose an end-to-end neural segmenter based on bilstm-crf framework. to improve its accuracy we address the problem of data insufficiency by transferring a word representation model that is trained on a large corpus. we also propose a restricted self-attention mechanism in order to capture useful information within a neighborhood. experiments on the rst-dt corpus show that our model is significantly faster than previous methods while achieving new state-of-the-art performance.
863,huang-etal-2020-simultaneous,"   Simultaneous Translation"",
",simultaneous translation which performs translation concurrently with the source speech is widely useful in many scenarios such as international conferences negotiations press releases legal proceedings and medicine. this problem has long been considered one of the hardest problems in ai and one of its holy grails. recently with rapid improvements in machine translation speech recognition and speech synthesis there has been exciting progress towards simultaneous translation. this tutorial will focus on the design and evaluation of policies for simultaneous translation to leave attendees with a deep technical understanding of the history the recent advances and the remaining challenges in this field.
864,alabdulkarim-alhindi-2019-spider,"   Spider-{J}erusalem at {S}em{E}val-2019 Task 4: Hyperpartisan News Detection"",
",this paper describes our system for detecting hyperpartisan news articles which was submitted for the shared task in semeval 2019 on hyperpartisan news detection. we developed a support vector machine (svm) model that uses tf-idf of tokens language inquiry and word count (liwc) features and structural features such as number of paragraphs and hyperlink count in an article. the model was trained on 645 articles from two classes: mainstream and hyperpartisan. our system was ranked seventeenth out of forty two participating teams in the binary classification task with an accuracy score of 0.742 on the blind test set (the accuracy of the top ranked system was 0.822). we provide a detailed description of our preprocessing steps discussion of our experiments using different combinations of features and analysis of our results and prediction errors.
865,b-etal-2021-overview,"   An Overview of Fairness in Data {--} Illuminating the Bias in Data Pipeline"",
",data in general encodes human biases by default; being aware of this is a good start and the research around how to handle it is ongoing. the term {`}bias{'} is extensively used in various contexts in nlp systems. in our research the focus is specific to biases such as gender racism religion demographic and other intersectional views on biases that prevail in text processing systems responsible for systematically discriminating specific population which is not ethical in nlp. these biases exacerbate the lack of equality diversity and inclusion of specific population while utilizing the nlp applications. the tools and technology at the intermediate level utilize biased data and transfer or amplify this bias to the downstream applications. however it is not enough to be colourblind gender-neutral alone when designing a unbiased technology {--} instead we should take a conscious effort by designing a unified framework to measure and benchmark the bias. in this paper we recommend six measures and one augment measure based on the observations of the bias in data annotations text representations and debiasing techniques.
866,chen-etal-2017-automatically,"   Automatically Labeled Data Generation for Large Scale Event Extraction"",
",modern models of event extraction for tasks like ace are based on supervised learning of events from small hand-labeled data. however hand-labeled training data is expensive to produce in low coverage of event types and limited in size which makes supervised methods hard to extract large scale of events for knowledge base population. to solve the data labeling problem we propose to automatically label training data for event extraction via world knowledge and linguistic knowledge which can detect key arguments and trigger words for each event type and employ them to label events in texts automatically. the experimental results show that the quality of our large scale automatically labeled data is competitive with elaborately human-labeled data. and our automatically labeled data can incorporate with human-labeled data then improve the performance of models learned from these data.
867,zerva-ananiadou-2018-paths,"   Paths for uncertainty: Exploring the intricacies of uncertainty identification for news"",
",currently news articles are produced shared and consumed at an extremely rapid rate. although their quantity is increasing at the same time their quality and trustworthiness is becoming fuzzier. hence it is important not only to automate information extraction but also to quantify the certainty of this information. automated identification of certainty has been studied both in the scientific and newswire domains but performance is considerably higher in tasks focusing on scientific text. we compare the differences in the definition and expression of uncertainty between a scientific domain i.e. biomedicine and newswire. we delve into the different aspects that affect the certainty of an extracted event in a news article and examine whether they can be easily identified by techniques already validated in the biomedical domain. finally we present a comparison of the syntactic and lexical differences between the the expression of certainty in the biomedical and newswire domains using two annotated corpora.
868,popat-etal-2018-declare,"   {D}e{C}lar{E}: Debunking Fake News and False Claims using Evidence-Aware Deep Learning"",
",misinformation such as fake news is one of the big challenges of our society. research on automated fact-checking has proposed methods based on supervised learning but these approaches do not consider external evidence apart from labeled training instances. recent approaches counter this deficit by considering external sources related to a claim. however these methods require substantial feature modeling and rich lexicons. this paper overcomes these limitations of prior work with an end-to-end model for evidence-aware credibility assessment of arbitrary textual claims without any human intervention. it presents a neural network model that judiciously aggregates signals from external evidence articles the language of these articles and the trustworthiness of their sources. it also derives informative features for generating user-comprehensible explanations that makes the neural network predictions transparent to the end-user. experiments with four datasets and ablation studies show the strength of our method.
869,xu-etal-2021-better,"   Better Feature Integration for Named Entity Recognition"",
",it has been shown that named entity recognition (ner) could benefit from incorporating the long-distance structured information captured by dependency trees. we believe this is because both types of features - the contextual information captured by the linear sequences and the structured information captured by the dependency trees may complement each other. however existing approaches largely focused on stacking the lstm and graph neural networks such as graph convolutional networks (gcns) for building improved ner models where the exact interaction mechanism between the two types of features is not very clear and the performance gain does not appear to be significant. in this work we propose a simple and robust solution to incorporate both types of features with our synergized-lstm (syn-lstm) which clearly captures how the two types of features interact. we conduct extensive experiments on several standard datasets across four languages. the results demonstrate that the proposed model achieves better performance than previous approaches while requiring fewer parameters. our further analysis demonstrates that our model can capture longer dependencies compared with strong baselines.
870,chen-etal-2021-zero,"   Zero-Shot Cross-Lingual Transfer of Neural Machine Translation with Multilingual Pretrained Encoders"",
",previous work mainly focuses on improving cross-lingual transfer for nlu tasks with a multilingual pretrained encoder (mpe) or improving the performance on supervised machine translation with bert. however it is under-explored that whether the mpe can help to facilitate the cross-lingual transferability of nmt model. in this paper we focus on a zero-shot cross-lingual transfer task in nmt. in this task the nmt model is trained with parallel dataset of only one language pair and an off-the-shelf mpe then it is directly tested on zero-shot language pairs. we propose sixt a simple yet effective model for this task. sixt leverages the mpe with a two-stage training schedule and gets further improvement with a position disentangled encoder and a capacity-enhanced decoder. using this method sixt significantly outperforms mbart a pretrained multilingual encoder-decoder model explicitly designed for nmt with an average improvement of 7.1 bleu on zero-shot any-to-english test sets across 14 source languages. furthermore with much less training computation cost and training data our model achieves better performance on 15 any-to-english test sets than criss and m2m-100 two strong multilingual nmt baselines.
871,barrow-etal-2020-joint,"   A Joint Model for Document Segmentation and Segment Labeling"",
",text segmentation aims to uncover latent structure by dividing text from a document into coherent sections. where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately we show that the tasks contain complementary information and are best addressed jointly. we introduce segment pooling lstm (s-lstm) which is capable of jointly segmenting a document and labeling segments. in support of joint training we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments. we show that s-lstm reduces segmentation error by 30{\%} on average while also improving segment labeling.
872,da-cunha-etal-2017-artext,"   The ar{T}ext prototype: An automatic system for writing specialized texts"",
",this article describes an automatic system for writing specialized texts in spanish. the artext prototype is a free online text editor that includes different types of linguistic information. it is designed for a variety of end users and domains including specialists and university students working in the fields of medicine and tourism and laypersons writing to the public administration. artext provides guidance on how to structure a text prompts users to include all necessary contents in each section and detects lexical and discourse problems in the text.
873,wang-etal-2020-multi-domain-named,"   Multi-Domain Named Entity Recognition with Genre-Aware and Agnostic Inference"",
",named entity recognition is a key component of many text processing pipelines and it is thus essential for this component to be robust to different types of input. however domain transfer of ner models with data from multiple genres has not been widely studied. to this end we conduct ner experiments in three predictive setups on data from: a) multiple domains; b) multiple domains where the genre label is unknown at inference time; c) domains not encountered in training. we introduce a new architecture tailored to this task by using shared and private domain parameters and multi-task learning. this consistently outperforms all other baseline and competitive methods on all three experimental setups with differences ranging between +1.95 to +3.11 average f1 across multiple genres when compared to standard approaches. these results illustrate the challenges that need to be taken into account when building real-world nlp applications that are robust to various types of text and the methods that can help at least partially alleviate these issues.
874,chuan-an-etal-2020-chinese,"   {C}hinese Discourse Parsing: Model and Evaluation"",
",chinese discourse parsing which aims to identify the hierarchical relationships of chinese elementary discourse units has not yet a consistent evaluation metric. although parseval is commonly used variations of evaluation differ from three aspects: micro vs. macro f1 scores binary vs. multiway ground truth and left-heavy vs. right-heavy binarization. in this paper we first propose a neural network model that unifies a pre-trained transformer and cky-like algorithm and then compare it with the previous models with different evaluation scenarios. the experimental results show that our model outperforms the previous systems. we conclude that (1) the pre-trained context embedding provides effective solutions to deal with implicit semantics in chinese texts and (2) using multiway ground truth is helpful since different binarization approaches lead to significant differences in performance.
875,meng-rumshisky-2018-triad,"   Triad-based Neural Network for Coreference Resolution"",
",we propose a triad-based neural network system that generates affinity scores between entity mentions for coreference resolution. the system simultaneously accepts three mentions as input taking mutual dependency and logical constraints of all three mentions into account and thus makes more accurate predictions than the traditional pairwise approach. depending on system choices the affinity scores can be further used in clustering or mention ranking. our experiments show that a standard hierarchical clustering using the scores produces state-of-art results with muc and b 3 metrics on the english portion of conll 2012 shared task. the model does not rely on many handcrafted features and is easy to train and use. the triads can also be easily extended to polyads of higher orders. to our knowledge this is the first neural network system to model mutual dependency of more than two members at mention level.
876,marchand-etal-2009-analyse,"   Analyse en d{\'e}pendances {\`a} l{'}aide des grammaires d{'}interaction"",
",cet article propose une m{\'e}thode pour extraire une analyse en d{\'e}pendances d{'}un {\'e}nonc{\'e} {\`a} partir de son analyse en constituants avec les grammaires d{'}interaction. les grammaires d{'}interaction sont un formalisme grammatical qui exprime l{'}interaction entre les mots {\`a} l{'}aide d{'}un syst{\`e}me de polarit{\'e}s. le m{\'e}canisme de composition syntaxique est r{\'e}gi par la saturation des polarit{\'e}s. les interactions s{'}effectuent entre les constituants mais les grammaires {\'e}tant lexicalis{\'e}es ces interactions peuvent se traduire sur les mots. la saturation des polarit{\'e}s lors de l{'}analyse syntaxique d{'}un {\'e}nonc{\'e} permet d{'}extraire des relations de d{\'e}pendances entre les mots chaque d{\'e}pendance {\'e}tant r{\'e}alis{\'e}e par une saturation. les structures de d{\'e}pendances ainsi obtenues peuvent {\^e}tre vues comme un raffinement de l{'}analyse habituellement effectu{\'e}e sous forme d{'}arbre de d{\'e}pendance. plus g{\'e}n{\'e}ralement ce travail apporte un {\'e}clairage nouveau sur les liens entre analyse en constituants et analyse en d{\'e}pendances.
877,zeldes-etal-2020-exhaustive,"   Exhaustive Entity Recognition for {C}optic: Challenges and Solutions"",
",entity recognition provides semantic access to ancient materials in the digital humanities: it exposes people and places of interest in texts that cannot be read exhaustively facilitates linking resources and can provide a window into text contents even for texts with no translations. in this paper we present entity recognition for coptic the language of hellenistic era egypt. we evaluate nlp approaches to the task and lay out difficulties in applying them to a low-resource morphologically complex language. we present solutions for named and non-named nested entity recognition and semi-automatic entity linking to wikipedia relying on robust dependency parsing feature-based crf models and hand-crafted knowledge base resources enabling high accuracy ner with orders of magnitude less data than those used for high resource languages. the results suggest avenues for research on other languages in similar settings.
878,caussade-etal-2016-disfluences,"   Disfluences dans le vieillissement « normal » et la maladie d{'}{A}lzheimer : indices segmentaux, suprasegmentaux et gestuels (Disfluencies in {``}normal{''} aging and {A}lzheimer{'}s disease: segmental, suprasegmental and gestural markers)"",
",l{'}objectif de cette {\'e}tude est d{'}analyser et comparer les productions langagi{\`e}res dans leur multimodalit{\'e} de 10 personnes atteintes de la maladie d{'}alzheimer (ma) appari{\'e}es {\`a} 10 contr{\^o}les. diff{\'e}rentes mesures aux niveaux segmental et suprasegmental {--} erreurs pauses et allongements vocaliques {--} ont {\'e}t{\'e} r{\'e}alis{\'e}es dans une t{\^a}che de r{\'e}p{\'e}tition avec ou sans gestes impos{\'e}s pour caract{\'e}riser une disfluence typique de la ma puis observ{\'e}es en lien avec les gestes manuels produits. les r{\'e}sultats montrent la diminution significative de la fluence chez les personnes atteintes de la ma avec davantage d{'}erreurs produites au niveau lexical par le groupe patient et au niveau phon{\'e}tique par les patients au stade mod{\'e}r{\'e} de la maladie ainsi que de nombreuses pauses silencieuses pr{\'e}c{\'e}dant ou suivant souvent les erreurs produites au niveau segmental. de plus dans la t{\^a}che avec gestes impos{\'e}s la r{\'e}p{\'e}tition de ceux-ci a impact{\'e} la fluence des groupes contr{\^o}le et patient avec une augmentation significative des disfluences au niveau suprasegmental et des erreurs phon{\'e}tiques au niveau segmental.
879,opitz-etal-2018-induction,"   Induction of a Large-Scale Knowledge Graph from the {R}egesta {I}mperii"",
",we induce and visualize a knowledge graph over the regesta imperii (ri) an important large-scale resource for medieval history research. the ri comprise more than 150000 digitized abstracts of medieval charters issued by the roman-german kings and popes distributed over many european locations and a time span of more than 700 years. our goal is to provide a resource for historians to visualize and query the ri possibly aiding medieval history research. the resulting medieval graph and visualization tools are shared publicly.
880,liang-etal-2021-towards,"   Towards Making the Most of Dialogue Characteristics for Neural Chat Translation"",
",neural chat translation (nct) aims to translate conversational text between speakers of different languages. despite the promising performance of sentence-level and context-aware neural machine translation models there still remain limitations in current nct models because the inherent dialogue characteristics of chat such as dialogue coherence and speaker personality are neglected. in this paper we propose to promote the chat translation by introducing the modeling of dialogue characteristics into the nct model. to this end we design four auxiliary tasks including monolingual response generation cross-lingual response generation next utterance discrimination and speaker identification. together with the main chat translation task we optimize the enhanced nct model through the training objectives of all these tasks. by this means the nct model can be enhanced by capturing the inherent dialogue characteristics thus generating more coherent and speaker-relevant translations. comprehensive experiments on four language directions (english{\textless}-{\textgreater}german and english{\textless}-{\textgreater}chinese) verify the effectiveness and superiority of the proposed approach.
881,libovicky-etal-2020-language,"   On the Language Neutrality of Pre-trained Multilingual Representations"",
",multilingual contextual embeddings such as multilingual bert and xlm-roberta have proved useful for many multi-lingual tasks. previous work probed the cross-linguality of the representations indirectly using zero-shot transfer learning on morphological and syntactic tasks. we instead investigate the language-neutrality of multilingual contextual embeddings directly and with respect to lexical semantics. our results show that contextual embeddings are more language-neutral and in general more informative than aligned static word-type embeddings which are explicitly trained for language neutrality. contextual embeddings are still only moderately language-neutral by default so we propose two simple methods for achieving stronger language neutrality: first by unsupervised centering of the representation for each language and second by fitting an explicit projection on small parallel data. besides we show how to reach state-of-the-art accuracy on language identification and match the performance of statistical methods for word alignment of parallel sentences without using parallel data.
882,gonzalez-agirre-etal-2012-proposal,"   A proposal for improving {W}ord{N}et Domains"",
",wordnet domains (wnd) is a lexical resource where synsets have been semi-automatically annotated with one or more domain labels from a set of 165 hierarchically organized domains. the uses of wnd include the power to reduce the polysemy degree of the words grouping those senses that belong to the same domain. but the semi-automatic method used to develop this resource was far from being perfect. by cross-checking the content of the multilingual central repository (mcr) it is possible to find some errors and inconsistencies. many are very subtle. others however leave no doubt. moreover it is very difficult to quantify the number of errors in the original version of wnd. this paper presents a novel semi-automatic method to propagate domain information through the mcr. we also compare both labellings (the original and the new one) allowing us to detect anomalies in the original wnd labels.
883,bao-etal-2021-g,"   {G}-Transformer for Document-Level Machine Translation"",
",document-level mt models are still far from satisfactory. existing work extend translation unit from single sentence to multiple sentences. however study shows that when we further enlarge the translation unit to a whole document supervised training of transformer can fail. in this paper we find such failure is not caused by overfitting but by sticking around local minima during training. our analysis shows that the increased complexity of target-to-source attention is a reason for the failure. as a solution we propose g-transformer introducing locality assumption as an inductive bias into transformer reducing the hypothesis space of the attention from target to source. experiments show that g-transformer converges faster and more stably than transformer achieving new state-of-the-art bleu scores for both nonpretraining and pre-training settings on three benchmark datasets.
884,mboning-tchiaze-nouvel-2020-nlu,"   {NLU}-Co at {S}em{E}val-2020 Task 5: {NLU}/{SVM} Based Model Apply Tocharacterise and Extract Counterfactual Items on Raw Data"",
",in this article we try to solve the problem of classification of counterfactual statements and extraction of antecedents/consequences in raw data by mobilizing on one hand support vector machine (svms) and on the other hand natural language understanding (nlu) infrastructures available on the market for conversational agents. our experiments allowed us to test different pipelines of two known platforms (snips nlu and rasa nlu). the results obtained show that a rasa nlu pipeline built with a well-preprocessed dataset and tuned algorithms allows to model accurately the structure of a counterfactual event in order to facilitate the identification and the extraction of its components.
885,etcheverry-wonsever-2019-unraveling,"   Unraveling Antonym{'}s Word Vectors through a {S}iamese-like Network"",
",discriminating antonyms and synonyms is an important nlp task that has the difficulty that both antonyms and synonyms contains similar distributional information. consequently pairs of antonyms and synonyms may have similar word vectors. we present an approach to unravel antonymy and synonymy from word vectors based on a siamese network inspired approach. the model consists of a two-phase training of the same base network: a pre-training phase according to a siamese model supervised by synonyms and a training phase on antonyms through a siamese-like model that supports the antitransitivity present in antonymy. the approach makes use of the claim that the antonyms in common of a word tend to be synonyms. we show that our approach outperforms distributional and pattern-based approaches relaying on a simple feed forward network as base network of the training phases.
886,li-etal-2018-dimsim,"   {DIMSIM}: An Accurate {C}hinese Phonetic Similarity Algorithm Based on Learned High Dimensional Encoding"",
",phonetic similarity algorithms identify words and phrases with similar pronunciation which are used in many natural language processing tasks. however existing approaches are designed mainly for indo-european languages and fail to capture the unique properties of chinese pronunciation. in this paper we propose a high dimensional encoded phonetic similarity algorithm for chinese dimsim. the encodings are learned from annotated data to separately map initial and final phonemes into n-dimensional coordinates. pinyin phonetic similarities are then calculated by aggregating the similarities of initial final and tone. dimsim demonstrates a 7.5x improvement on mean reciprocal rank over the state-of-the-art phonetic similarity approaches.
887,grundkiewicz-junczys-dowmunt-2019-minimally,"   Minimally-Augmented Grammatical Error Correction"",
",there has been an increased interest in low-resource approaches to automatic grammatical error correction. we introduce minimally-augmented grammatical error correction (magec) that does not require any error-labelled data. our unsupervised approach is based on a simple but effective synthetic error generation method based on confusion sets from inverted spell-checkers. in low-resource settings we outperform the current state-of-the-art results for german and russian gec tasks by a large margin without using any real error-annotated training data. when combined with labelled data our method can serve as an efficient pre-training technique
888,wang-etal-2017-transductive,"   Transductive Non-linear Learning for {C}hinese Hypernym Prediction"",
",finding the correct hypernyms for entities is essential for taxonomy learning fine-grained entity categorization query understanding etc. due to the flexibility of the chinese language it is challenging to identify hypernyms in chinese accurately. rather than extracting hypernyms from texts in this paper we present a transductive learning approach to establish mappings from entities to hypernyms in the embedding space directly. it combines linear and non-linear embedding projection models with the capacity of encoding arbitrary language-specific rules. experiments on real-world datasets illustrate that our approach outperforms previous methods for chinese hypernym prediction.
889,duan-etal-2017-question,"   Question Generation for Question Answering"",
",this paper presents how to generate questions from given passages using neural networks where large scale qa pairs are automatically crawled and processed from community-qa website and used as training data. the contribution of the paper is 2-fold: first two types of question generation approaches are proposed one is a retrieval-based method using convolution neural network (cnn) the other is a generation-based method using recurrent neural network (rnn); second we show how to leverage the generated questions to improve existing question answering systems. we evaluate our question generation method for the answer sentence selection task on three benchmark datasets including squad ms marco and wikiqa. experimental results show that by using generated questions as an extra signal significant qa improvement can be achieved.
890,luo-etal-2021-newsclippings,"   {N}ews{CLIP}pings: {A}utomatic {G}eneration of {O}ut-of-{C}ontext {M}ultimodal {M}edia"",
",online misinformation is a prevalent societal issue with adversaries relying on tools ranging from cheap fakes to sophisticated deep fakes. we are motivated by the threat scenario where an image is used out of context to support a certain narrative. while some prior datasets for detecting image-text inconsistency generate samples via text manipulation we propose a dataset where both image and text are unmanipulated but mismatched. we introduce several strategies for automatically retrieving convincing images for a given caption capturing cases with inconsistent entities or semantic context. our large-scale automatically generated the newsclippings dataset: (1) demonstrates that machine-driven image repurposing is now a realistic threat and (2) provides samples that represent challenging instances of mismatch between text and image in news that are able to mislead humans. we benchmark several state-of-the-art multimodal models on our dataset and analyze their performance across different pretraining domains and visual backbones.
891,kelly-etal-2020-social,"   Social media data as a lens onto care-seeking behavior among women veterans of the {US} armed forces"",
",in this article we examine social media data as a lens onto support-seeking among women veterans of the us armed forces. social media data hold a great deal of promise as a source of information on needs and support-seeking among individuals who are excluded from or systematically prevented from accessing clinical or other institutions ostensibly designed to support them. we apply natural language processing (nlp) techniques to more than 3 million tweets collected from 20000 twitter users. we find evidence that women veterans are more likely to use social media to seek social and community engagement and to discuss mental health and veterans{'} issues significantly more frequently than their male counterparts. by contrast male veterans tend to use social media to amplify political ideologies or to engage in partisan debate. our results have implications for how organizations can provide outreach and services to this uniquely vulnerable population and illustrate the utility of non-traditional observational data sources such as social media to understand the needs of marginalized groups.
892,wang-etal-2020-keep,"   Keep it Consistent: Topic-Aware Storytelling from an Image Stream via Iterative Multi-agent Communication"",
",visual storytelling aims to generate a narrative paragraph from a sequence of images automatically. existing approaches construct text description independently for each image and roughly concatenate them as a story which leads to the problem of generating semantically incoherent content. in this paper we propose a new way for visual storytelling by introducing a topic description task to detect the global semantic context of an image stream. a story is then constructed with the guidance of the topic description. in order to combine the two generation tasks we propose a multi-agent communication framework that regards the topic description generator and the story generator as two agents and learn them simultaneously via iterative updating mechanism. we validate our approach on vist dataset where quantitative results ablations and human evaluation demonstrate our method{'}s good ability in generating stories with higher quality compared to state-of-the-art methods.
893,bisk-etal-2019-benchmarking,"   Benchmarking Hierarchical Script Knowledge"",
",understanding procedural language requires reasoning about both hierarchical and temporal relations between events. for example {``}boiling pasta{''} is a sub-event of {``}making a pasta dish{''} typically happens before {``}draining pasta{''} and requires the use of omitted tools (e.g. a strainer sink...). while people are able to choose when and how to use abstract versus concrete instructions the nlp community lacks corpora and tasks for evaluating if our models can do the same. in this paper we introduce kidscook a parallel script corpus as well as a cloze task which matches video captions with missing procedural details. experimental results show that state-of-the-art models struggle at this task which requires inducing functional commonsense knowledge not explicitly stated in text.
894,muller-strube-2018-transparent,"   Transparent, Efficient, and Robust Word Embedding Access with {WOMBAT}"",
",we present wombat a python tool which supports nlp practitioners in accessing word embeddings from code. wombat addresses common research problems including unified access scaling and robust and reproducible preprocessing. code that uses wombat for accessing word embeddings is not only cleaner more readable and easier to reuse but also much more efficient than code using standard in-memory methods: a python script using wombat for evaluating seven large word embedding collections (8.7m embedding vectors in total) on a simple semeval sentence similarity task involving 250 raw sentence pairs completes in under ten seconds end-to-end on a standard notebook computer.
895,flather-2012-sharpening,"   Sharpening the Claws on {CAT} Tools: Increase Quality {\&} Production, Maximize Limited Resources"",
",making the right connections hinges on linking data from disparate sources. frequently the link may be a person or place so something as simple as a mistranslated name will cause a search to miss relevant documents. to swiftly and accurately exploit a growing flood of foreign language information acquired for the defense of the nation intelligence community (ic) linguists and analysts need assistance in both translation accuracy and productivity. the name translation and standardizing component of a computer-aided translation (cat) tool such as the highlight language analysis suite ensures fast and reliable translation of names from arabic dari farsi and pashto according to a number of government transliteration standards. highlight improves efficiency and maximizes the utilization of scarce human resources.
896,kordoni-etal-2016-enhancing,"   Enhancing Access to Online Education: Quality Machine Translation of {MOOC} Content"",
",the present work is an overview of the tramooc (translation for massive open online courses) research and innovation project a machine translation approach for online educational content. more specifically videolectures assignments and mooc forum text is automatically translated from english into eleven european and bric languages. unlike previous approaches to machine translation the output quality in tramooc relies on a multimodal evaluation schema that involves crowdsourcing error type markup an error taxonomy for translation model comparison and implicit evaluation via text mining i.e. entity recognition and its performance comparison between the source and the translated text and sentiment analysis on the students{'} forum posts. finally the evaluation output will result in more and better quality in-domain parallel data that will be fed back to the translation engine for higher quality output. the translation service will be incorporated into the iversity mooc platform and into the videolectures.net digital library portal.
897,sitbon-bellot-2004-evaluation,"   Evaluation de m{\'e}thodes de segmentation th{\'e}matique lin{\'e}aire non supervis{\'e}es apr{\`e}s adaptation au fran{\c{c}}ais"",
",nous proposons une {\'e}valuation de diff{\'e}rentes m{\'e}thodes et outils de segmentation th{\'e}matique de textes. nous pr{\'e}sentons les outils de segmentation lin{\'e}aire et non supervis{\'e}e dotplotting segmenter c99 texttiling ainsi qu{'}une mani{\`e}re de les adapter et de les tester sur des documents fran{\c{c}}ais. les r{\'e}sultats des tests montrent des diff{\'e}rences en performance notables selon les sujets abord{\'e}s dans les documents et selon que le nombre de segments {\`a} trouver est fix{\'e} au pr{\'e}alable par l{'}utilisateur. ces travaux font partie du projet technolangue agile-oural.
898,skeppstedt-etal-2018-less,"   More or less controlled elicitation of argumentative text: Enlarging a microtext corpus via crowdsourcing"",
",we present an extension of an annotated corpus of short argumentative texts that had originally been built in a controlled text production experiment. our extension more than doubles the size of the corpus by means of crowdsourcing. we report on the setup of this experiment and on the consequences that crowdsourcing had for assembling the data and in particular for annotation. we labeled the argumentative structure by marking claims premises and relations between them following the scheme used in the original corpus but had to make a few modifications in response to interesting phenomena in the data. finally we report on an experiment with the automatic prediction of this argumentation structure: we first replicated the approach of an earlier study on the original corpus and compare the performance to various settings involving the extension.
899,ezen-can-can-2019-hybrid,"   Hybrid {RNN} at {S}em{E}val-2019 Task 9: Blending Information Sources for Domain-Independent Suggestion Mining"",
",social media has an increasing amount of information that both customers and companies can benefit from. these social media posts can include tweets or be in the form of vocalization of complements and complaints (e.g. reviews) of a product or service. researchers have been actively mining this invaluable information source to automatically generate insights. mining sentiments of customer reviews is an example that has gained momentum due to its potential to gather information that customers are not happy about. instead of reading millions of reviews companies prefer sentiment analysis to obtain feedback and to improve their products or services. in this work we aim to identify information that companies can act on or other customers can utilize for making their own experience better. this is different from identifying if reviews of a product or service is negative positive or neutral. to that end we classify sentences of a given review as suggestion or not suggestion so that readers of the reviews do not have to go through thousands of reviews but instead can focus on actionable items and applicable suggestions. to identify suggestions within reviews we employ a hybrid approach that utilizes a recurrent neural network (rnn) along with rule-based features to build a domain-independent suggestion mining model. in this way a model trained on electronics reviews is used to extract suggestions from hotel reviews.
900,subramanian-etal-2021-evaluating,"   Evaluating Debiasing Techniques for Intersectional Biases"",
",bias is pervasive for nlp models motivating the development of automatic debiasing techniques. evaluation of nlp debiasing methods has largely been limited to binary attributes in isolation e.g. debiasing with respect to binary gender or race however many corpora involve multiple such attributes possibly with higher cardinality. in this paper we argue that a truly fair model must consider {`}gerrymandering{'} groups which comprise not only single attributes but also intersectional groups. we evaluate a form of bias-constrained model which is new to nlp as well an extension of the iterative nullspace projection technique which can handle multiple identities.
901,wang-etal-2018-apprentissage,"   Apprentissage d{\'e}s{\'e}quilibr{\'e} pour la d{\'e}tection des signaux de l{'}implication durable dans les conversations en parfumerie (Automatic detection of positive enduring involvement signals in fragrance products reviews)"",
",une simple d{\'e}tection d{'}opinions positives ou n{\'e}gatives ne satisfait plus les chercheurs et les entreprises. le monde des affaires est {\`a} la recherche d{'}un «aper{\c{c}}u des affaires». beaucoup de m{\'e}thodes peuvent {\^e}tre utilis{\'e}es pour traiter le probl{\`e}me. cependant leurs performances lorsque les classes ne sont pas {\'e}quilibr{\'e}es peuvent {\^e}tre d{\'e}grad{\'e}es. notre travail se concentre sur l{'}{\'e}tude des techniques visant {\`a} traiter les donn{\'e}es d{\'e}s{\'e}quilibr{\'e}es en parfumerie. cinq m{\'e}thodes ont {\'e}t{\'e} compar{\'e}es : smote adasyn tomek links smote-tl et la modification du poids des classe. l{'}algorithme d{'}apprentissage choisi est le svm et l{'}{\'e}valuation est r{\'e}alis{\'e}e par le calcul des scores de pr{\'e}cision de rappel et de f-mesure. selon les r{\'e}sultats exp{\'e}rimentaux la m{\'e}thode en ajustant le poids sur des co{\^u}t d{'}erreurs avec svm nous permet d{'}obtenir notre meilleure f-mesure.
902,lane-bird-2019-towards,"   Towards A Robust Morphological Analyzer for Kunwinjku"",
",kunwinjku is an indigenous australian language spoken in northern australia which exhibits agglutinative and polysynthetic properties. members of the community have expressed interest in co-developing language applications that promote their values and priorities. modeling the morphology of the kunwinjku language is an important step towards accomplishing the community{'}s goals. finite state transducers have long been the go-to method for modeling morphologically rich languages and in this paper we discuss some of the distinct modeling challenges present in the morphosyntax of verbs in kunwinjku. we show that a fairly straightforward implementation using standard features of the foma toolkit can account for much of the verb structure. continuing challenges include robustness in the face of variation and unseen vocabulary as well as how to handle complex reduplicative processes. our future work will build off the baseline and challenges presented here.
903,liednikova-etal-2020-learning,"   Learning Health-Bots from Training Data that was Automatically Created using Paraphrase Detection and Expert Knowledge"",
",a key bottleneck for developing dialog models is the lack of adequate training data. due to privacy issues dialog data is even scarcer in the health domain. we propose a novel method for creating dialog corpora which we apply to create doctor-patient interaction data. we use this data to learn both a generation and a hybrid classification/retrieval model and find that the generation model consistently outperforms the hybrid model. we show that our data creation method has several advantages. not only does it allow for the semi-automatic creation of large quantities of training data. it also provides a natural way of guiding learning and a novel method for assessing the quality of human-machine interactions.
904,hu-etal-2020-enhanced,"   Enhanced Sentence Alignment Network for Efficient Short Text Matching"",
",cross-sentence attention has been widely applied in text matching in which model learns the aligned information between two intermediate sequence representations to capture their semantic relationship. however commonly the intermediate representations are generated solely based on the preceding layers and the models may suffer from error propagation and unstable matching especially when multiple attention layers are used. in this paper we pro-pose an enhanced sentence alignment network with simple gated feature augmentation where the model is able to flexibly integrate both original word and contextual features to improve the cross-sentence attention. moreover our model is less complex with fewer parameters compared to many state-of-the-art structures.experiments on three benchmark datasets validate our model capacity for text matching.
905,tran-etal-2021-covrelex,"   {C}ov{R}elex: A {COVID}-19 Retrieval System with Relation Extraction"",
",this paper presents covrelex a scientific paper retrieval system targeting entities and relations via relation extraction on covid-19 scientific papers. this work aims at building a system supporting users efficiently in acquiring knowledge across a huge number of covid-19 scientific papers published rapidly. our system can be accessed via https://www.jaist.ac.jp/is/labs/nguyen-lab/systems/covrelex/.
906,chen-etal-2017-deep,"   Deep Learning for Dialogue Systems"",
",in the past decade goal-oriented spoken dialogue systems have been the most prominent component in today's virtual personal assistants. the classic dialogue systems have rather complex and/or modular pipelines. the advance of deep learning technologies has recently risen the applications of neural models to dialogue modeling. however how to successfully apply deep learning based approaches to a dialogue system is still challenging. hence this tutorial is designed to focus on an overview of the dialogue system development while describing most recent research for building dialogue systems and summarizing the challenges in order to allow researchers to study the potential improvements of the state-of-the-art dialogue systems. the tutorial material is available at http://deepdialogue.miulab.tw.
907,hosu-etal-2018-natural,"   Natural Language Interface for Databases Using a Dual-Encoder Model"",
",we propose a sketch-based two-step neural model for generating structured queries (sql) based on a user{'}s request in natural language. the sketch is obtained by using placeholders for specific entities in the sql query such as column names table names aliases and variables in a process similar to semantic parsing. the first step is to apply a sequence-to-sequence (seq2seq) model to determine the most probable sql sketch based on the request in natural language. then a second network designed as a dual-encoder seq2seq model using both the text query and the previously obtained sketch is employed to generate the final sql query. our approach shows improvements over previous approaches on two recent large datasets (wikisql and senlidb) suitable for data-driven solutions for natural language interfaces for databases.
908,velcin-etal-2014-investigating,"   Investigating the Image of Entities in Social Media: Dataset Design and First Results"",
",the objective of this paper is to describe the design of a dataset that deals with the image (i.e. representation web reputation) of various entities populating the internet: politicians celebrities companies brands etc. our main contribution is to build and provide an original annotated french dataset. this dataset consists of 11527 manually annotated tweets expressing the opinion on specific facets (e.g. ethic communication economic project) describing two french policitians over time. we believe that other researchers might benefit from this experience since designing and implementing such a dataset has proven quite an interesting challenge. this design comprises different processes such as data selection formal definition and instantiation of an image. we have set up a full open-source annotation platform. in addition to the dataset design we present the first results that we obtained by applying clustering methods to the annotated dataset in order to extract the entity images.
909,gupta-etal-2021-mcl,"   {MCL}@{IITK} at {S}em{E}val-2021 Task 2: Multilingual and Cross-lingual Word-in-Context Disambiguation using Augmented Data, Signals, and Transformers"",
",in this work we present our approach for solving the semeval 2021 task 2: multilingual and cross-lingual word-in-context disambiguation (mcl-wic). the task is a sentence pair classification problem where the goal is to detect whether a given word common to both the sentences evokes the same meaning. we submit systems for both the settings - multilingual (the pair{'}s sentences belong to the same language) and cross-lingual (the pair{'}s sentences belong to different languages). the training data is provided only in english. consequently we employ cross-lingual transfer techniques. our approach employs fine-tuning pre-trained transformer-based language models like electra and albert for the english task and xlm-r for all other tasks. to improve these systems{'} performance we propose adding a signal to the word to be disambiguated and augmenting our data by sentence pair reversal. we further augment the dataset provided to us with wic xl-wic and semcor 3.0. using ensembles we achieve strong performance in the multilingual task placing first in the en-en and fr-fr sub-tasks. for the cross-lingual setting we employed translate-test methods and a zero-shot method using our multilingual models with the latter performing slightly better.
910,inoue-etal-2021-multi,"   A multi-party attentive listening robot which stimulates involvement from side participants"",
",we demonstrate the moderating abilities of a multi-party attentive listening robot system when multiple people are speaking in turns. our conventional one-on-one attentive listening system generates listener responses such as backchannels repeats elaborating questions and assessments. in this paper additional robot responses that stimulate a listening user (side participant) to become more involved in the dialogue are proposed. the additional responses elicit assessments and questions from the side participant making the dialogue more empathetic and lively.
911,li-etal-2019-integration,"   Integration of Knowledge Graph Embedding Into Topic Modeling with Hierarchical {D}irichlet Process"",
",leveraging domain knowledge is an effective strategy for enhancing the quality of inferred low-dimensional representations of documents by topic models. in this paper we develop \textit{topic modeling with knowledge graph embedding} (tmkge) a bayesian nonparametric model to employ knowledge graph (kg) embedding in the context of topic modeling for extracting more coherent topics. specifically we build a hierarchical dirichlet process (hdp) based model to flexibly borrow information from kg to improve the interpretability of topics. an efficient online variational inference method based on a stick-breaking construction of hdp is developed for tmkge making tmkge suitable for large document corpora and kgs. experiments on three public datasets illustrate the superior performance of tmkge in terms of topic coherence and document classification accuracy compared to state-of-the-art topic modeling methods.
912,han-etal-2020-domain,"   Domain Knowledge Empowered Structured Neural Net for End-to-End Event Temporal Relation Extraction"",
",extracting event temporal relations is a critical task for information extraction and plays an important role in natural language understanding. prior systems leverage deep learning and pre-trained language models to improve the performance of the task. however these systems often suffer from two shortcomings: 1) when performing maximum a posteriori (map) inference based on neural models previous systems only used structured knowledge that is assumed to be absolutely correct i.e. hard constraints; 2) biased predictions on dominant temporal relations when training with a limited amount of data. to address these issues we propose a framework that enhances deep neural network with distributional constraints constructed by probabilistic domain knowledge. we solve the constrained inference problem via lagrangian relaxation and apply it to end-to-end event temporal relation extraction tasks. experimental results show our framework is able to improve the baseline neural network models with strong statistical significance on two widely used datasets in news and clinical domains.
913,white-2000-contemplating,"   Contemplating automatic {MT} evaluation"",
",researchers developers translators and information consumers all share the problem that there is no accepted standard for machine translation. the problem is much further confounded by the fact that mt evaluations properly done require a considerable commitment of time and resources an anachronism in this day of cross-lingual information processing when new mt systems may developed in weeks instead of years. this paper surveys the needs addressed by several of the classic {``}types{''} of mt and speculates on ways that each of these types might be automated to create relevant near-instantaneous evaluation of approaches and systems.
914,nadeem-etal-2020-systematic,"   A Systematic Characterization of Sampling Algorithms for Open-ended Language Generation"",
",this work studies the widely adopted ancestral sampling algorithms for auto-regressive language models. we use the quality-diversity (q-d) trade-off to investigate three popular sampling methods (top-k nucleus and tempered sampling). we focus on the task of open-ended language generation and first show that the existing sampling algorithms have similar performance. by carefully inspecting the transformations defined by different sampling algorithms we identify three key properties that are shared among them: entropy reduction order preservation and slope preservation. to validate the importance of the identified properties we design two sets of new sampling methods: one set in which each algorithm satisfies all three properties and one set in which each algorithm violates at least one of the properties. we compare their performance with existing algorithms and find that violating the identified properties could lead to drastic performance degradation as measured by the q-d trade-off. on the other hand we find that the set of sampling algorithms that satisfy these properties performs on par with the existing sampling algorithms.
915,dekker-van-der-goot-2020-synthetic,"   Synthetic Data for {E}nglish Lexical Normalization: How Close Can We Get to Manually Annotated Data?"",
",social media is a valuable data resource for various natural language processing (nlp) tasks. however standard nlp tools were often designed with standard texts in mind and their performance decreases heavily when applied to social media data. one solution to this problem is to adapt the input text to a more standard form a task also referred to as normalization. automatic approaches to normalization have shown that they can be used to improve performance on a variety of nlp tasks. however all of these systems are supervised thereby being heavily dependent on the availability of training data for the correct language and domain. in this work we attempt to overcome this dependence by automatically generating training data for lexical normalization. starting with raw tweets we attempt two directions to insert non-standardness (noise) and to automatically normalize in an unsupervised setting. our best results are achieved by automatically inserting noise. we evaluate our approaches by using an existing lexical normalization system; our best scores are achieved by custom error generation system which makes use of some manually created datasets. with this system we score 94.29 accuracy on the test data compared to 95.22 when it is trained on human-annotated data. our best system which does not depend on any type of annotation is based on word embeddings and scores 92.04 accuracy. finally we perform an experiment in which we asked humans to predict whether a sentence was written by a human or generated by our best model. this experiment showed that in most cases it is hard for a human to detect automatically generated sentences.
916,xu-etal-2021-augnlg,"   {A}ug{NLG}: Few-shot Natural Language Generation using Self-trained Data Augmentation"",
",natural language generation (nlg) is a key component in a task-oriented dialogue system which converts the structured meaning representation (mr) to the natural language. for large-scale conversational systems where it is common to have over hundreds of intents and thousands of slots neither template-based approaches nor model-based approaches are scalable. recently neural nlgs started leveraging transfer learning and showed promising results in few-shot settings. this paper proposes augnlg a novel data augmentation approach that combines a self-trained neural retrieval model with a few-shot learned nlu model to automatically create mr-to-text data from open-domain texts. the proposed system mostly outperforms the state-of-the-art methods on the fewshotwoz data in both bleu and slot error rate. we further confirm improved results on the fewshotsgd data and provide comprehensive analysis results on key components of our system. our code and data are available at https://github.com/xinnuoxu/augnlg.
917,jiang-etal-2020-cross,"   Cross-lingual Information Retrieval with {BERT}"",
",multiple neural language models have been developed recently e.g. bert and xlnet and achieved impressive results in various nlp tasks including sentence classification question answering and document ranking. in this paper we explore the use of the popular bidirectional language model bert to model and learn the relevance between english queries and foreign-language documents in the task of cross-lingual information retrieval. a deep relevance matching model based on bert is introduced and trained by finetuning a pretrained multilingual bert model with weak supervision using home-made clir training data derived from parallel corpora. experimental results of the retrieval of lithuanian documents against short english queries show that our model is effective and outperforms the competitive baseline approaches.
918,huang-etal-2020-texthide,"   {T}ext{H}ide: Tackling Data Privacy in Language Understanding Tasks"",
",an unsolved challenge in distributed or federated learning is to effectively mitigate privacy risks without slowing down training or reducing accuracy. in this paper we propose texthide aiming at addressing this challenge for natural language understanding tasks. it requires all participants to add a simple encryption step to prevent an eavesdropping attacker from recovering private text data. such an encryption step is efficient and only affects the task performance slightly. in addition texthide fits well with the popular framework of fine-tuning pre-trained language models (e.g. bert) for any sentence or sentence-pair task. we evaluate texthide on the glue benchmark and our experiments show that texthide can effectively defend attacks on shared gradients or representations and the averaged accuracy reduction is only 1.9{\%}. we also present an analysis of the security of texthide using a conjecture about the computational intractability of a mathematical problem.
919,mille-etal-2018-first,"   The First Multilingual Surface Realisation Shared Task ({SR}{'}18): Overview and Evaluation Results"",
",we report results from the sr{'}18 shared task a new multilingual surface realisation task organised as part of the acl{'}18 workshop on multilingual surface realisation. as in its english-only predecessor task sr{'}11 the shared task comprised two tracks with different levels of complexity: (a) a shallow track where the inputs were full ud structures with word order information removed and tokens lemmatised; and (b) a deep track where additionally functional words and morphological information were removed. the shallow track was offered in ten and the deep track in three languages. systems were evaluated (a) automatically using a range of intrinsic metrics and (b) by human judges in terms of readability and meaning similarity. this report presents the evaluation results along with descriptions of the sr{'}18 tracks data and evaluation methods. for full descriptions of the participating systems please see the separate system reports elsewhere in this volume.
920,lamprinidis-etal-2021-universal,"   Universal Joy A Data Set and Results for Classifying Emotions Across Languages"",
",while emotions are universal aspects of human psychology they are expressed differently across different languages and cultures. we introduce a new data set of over 530k anonymized public facebook posts across 18 languages labeled with five different emotions. using multilingual bert embeddings we show that emotions can be reliably inferred both within and across languages. zero-shot learning produces promising results for low-resource languages. following established theories of basic emotions we provide a detailed analysis of the possibilities and limits of cross-lingual emotion classification. we find that structural and typological similarity between languages facilitates cross-lingual learning as well as linguistic diversity of training data. our results suggest that there are commonalities underlying the expression of emotion in different languages. we publicly release the anonymized data for future research.
921,chen-etal-2020-question,"   Question Directed Graph Attention Network for Numerical Reasoning over Text"",
",numerical reasoning over texts such as addition subtraction sorting and counting is a challenging machine reading comprehension task since it requires both natural language understanding and arithmetic computation. to address this challenge we propose a heterogeneous graph representation for the context of the passage and question needed for such reasoning and design a question directed graph attention network to drive multi-step numerical reasoning over this context graph. our model which combines deep learning and graph reasoning achieves remarkable results in benchmark datasets such as drop.
922,luo-etal-2020-detecting,"   Detecting Stance in Media On Global Warming"",
",citing opinions is a powerful yet understudied strategy in argumentation. for example an environmental activist might say {``}leading scientists agree that global warming is a serious concern{''} framing a clause which affirms their own stance ({``}that global warming is serious{''}) as an opinion endorsed (''[scientists] agree{''}) by a reputable source ({``}leading{''}). in contrast a global warming denier might frame the same clause as the opinion of an untrustworthy source with a predicate connoting doubt: {``}mistaken scientists claim [...].'' our work studies opinion-framing in the global warming (gw) debate an increasingly partisan issue that has received little attention in nlp. we introduce desmog a dataset of stance-labeled gw sentences and train a bert classifier to study novel aspects of argumentation in how different sides of a debate represent their own and each other{'}s opinions. from 56k news articles we find that similar linguistic devices for self-affirming and opponent-doubting discourse are used across gw-accepting and skeptic media though gw-skeptical media shows more opponent-doubt. we also find that authors often characterize sources as hypocritical by ascribing opinions expressing the author{'}s own view to source entities known to publicly endorse the opposing view. we release our stance dataset model and lexicons of framing devices for future work on opinion-framing and the automatic detection of gw stance.
923,ma-etal-2020-powertransformer,"   {P}ower{T}ransformer: Unsupervised Controllable Revision for Biased Language Correction"",
",unconscious biases continue to be prevalent in modern text and media calling for algorithms that can assist writers with bias correction. for example a female character in a story is often portrayed as passive and powerless ({``}{\_}she daydreams about being a doctor{\_}{''}) while a man is portrayed as more proactive and powerful ({``}{\_}he pursues his dream of being a doctor{\_}{''}). we formulate **controllable debiasing** a new revision task that aims to rewrite a given text to correct the implicit and potentially undesirable bias in character portrayals. we then introduce powertransformer as an approach that debiases text through the lens of connotation frames (sap et al. 2017) which encode pragmatic knowledge of implied power dynamics with respect to verb predicates. one key challenge of our task is the lack of parallel corpora. to address this challenge we adopt an unsupervised approach using auxiliary supervision with related tasks such as paraphrasing and self-supervision based on a reconstruction loss building on pretrained language models. through comprehensive experiments based on automatic and human evaluations we demonstrate that our approach outperforms ablations and existing methods from related tasks. furthermore we demonstrate the use of powertransformer as a step toward mitigating the well-documented gender bias in character portrayal in movie scripts.
924,stankovic-etal-2010-gis,"   {GIS} Application Improvement with Multilingual Lexical and Terminological Resources"",
",this paper introduces the results of integration of lexical and terminological resources most of them developed within the human language technology (hlt) group at the university of belgrade with the geological information system of serbia (geoliss) developed at the faculty of mining and geology and funded by the ministry of the environmental protection. the approach to geoliss development which is aimed at the integration of existing geologic archives data from published maps on different scales newly acquired field data and intranet and internet publishing of geologic is given followed by the description of the geologic multilingual vocabulary and other lexical and terminological resources used. two basic results are outlined: multilingual map annotation and improvement of queries for the geoliss geodatabase. multilingual labelling and annotation of maps for their graphic display and printing have been tested with serbian which describes regional information in the local language and english used for sharing geographic information with the world although the geological vocabulary offers the possibility for integration of other languages as well. the resources also enable semantic and morphological expansion of queries the latter being very important in highly inflective languages such as serbian.
925,kim-etal-2016-domainless,"   Domainless Adaptation by Constrained Decoding on a Schema Lattice"",
",in many applications such as personal digital assistants there is a constant need for new domains to increase the system{'}s coverage of user queries. a conventional approach is to learn a separate model every time a new domain is introduced. this approach is slow inefficient and a bottleneck for scaling to a large number of domains. in this paper we introduce a framework that allows us to have a single model that can handle all domains: including unknown domains that may be created in the future as long as they are covered in the master schema. the key idea is to remove the need for distinguishing domains by explicitly predicting the schema of queries. given permitted schema of a query we perform constrained decoding on a lattice of slot sequences allowed under the schema. the proposed model achieves competitive and often superior performance over the conventional model trained separately per domain.
926,ning-etal-2020-easy,"   Easy, Reproducible and Quality-Controlled Data Collection with {CROWDAQ}"",
",high-quality and large-scale data are key to success for ai systems. however large-scale data annotation efforts are often confronted with a set of common challenges: (1) designing a user-friendly annotation interface; (2) training enough annotators efficiently; and (3) reproducibility. to address these problems we introduce crowdaq an open-source platform that standardizes the data collection pipeline with customizable user-interface components automated annotator qualification and saved pipelines in a re-usable format. we show that crowdaq simplifies data annotation significantly on a diverse set of data collection use cases and we hope it will be a convenient tool for the community.
927,cai-etal-2019-retrieval,"   Retrieval-guided Dialogue Response Generation via a Matching-to-Generation Framework"",
",end-to-end sequence generation is a popular technique for developing open domain dialogue systems though they suffer from the \textit{safe response problem}. researchers have attempted to tackle this problem by incorporating generative models with the returns of retrieval systems. recently a skeleton-then-response framework has been shown promising results for this task. nevertheless how to precisely extract a skeleton and how to effectively train a retrieval-guided response generator are still challenging. this paper presents a novel framework in which the skeleton extraction is made by an interpretable matching model and the following skeleton-guided response generation is accomplished by a separately trained generator. extensive experiments demonstrate the effectiveness of our model designs.
928,payan-etal-2021-towards-realistic,"   Towards Realistic Single-Task Continuous Learning Research for {NER}"",
",there is an increasing interest in continuous learning (cl) as data privacy is becoming a priority for real-world machine learning applications. meanwhile there is still a lack of academic nlp benchmarks that are applicable for realistic cl settings which is a major challenge for the advancement of the field. in this paper we discuss some of the unrealistic data characteristics of public datasets study the challenges of realistic single-task continuous learning as well as the effectiveness of data rehearsal as a way to mitigate accuracy loss. we construct a cl ner dataset from an existing publicly available dataset and release it along with the code to the research community.
929,konopik-etal-2017-czech,"   {C}zech Dataset for Semantic Similarity and Relatedness"",
",this paper introduces a czech dataset for semantic similarity and semantic relatedness. the dataset contains word pairs with hand annotated scores that indicate the semantic similarity and semantic relatedness of the words. the dataset contains 953 word pairs compiled from 9 different sources. it contains words and their contexts taken from real text corpora including extra examples when the words are ambiguous. the dataset is annotated by 5 independent annotators. the average spearman correlation coefficient of the annotation agreement is $r = 0.81$. we provide reference evaluation experiments with several methods for computing semantic similarity and relatedness.
930,nguyen-2004-nouvelle,"   Nouvelle m{\'e}thode syntagmatique de vectorisation appliqu{\'e}e au self-organizing map des textes vietnamiens"",
",par ses caract{\'e}ristiques {\'e}minentes dans la pr{\'e}sentation des donn{\'e}es self-organizing map (som) est particuli{\`e}rement convenable {\`a} l{'}organisation des cartes. som se comporte d{'}un ensemble des vecteurs prototypes pour repr{\'e}senter les donn{\'e}es d{'}entr{\'e}e et fait une projection en conservant la topologie {\`a} partir des vecteurs prototypes de n-dimensions sur une carte de 2-dimensions. cette carte deviendra une vision qui refl{\`e}te la structure des classes des donn{\'e}es. nous notons un probl{\`e}me crucial pour som c{'}est la m{\'e}thode de vectorisation des donn{\'e}es. dans nos {\'e}tudes les donn{\'e}es se pr{\'e}sentent sous forme des textes. bien que le mod{\`e}le g{\'e}n{\'e}ral du som soit d{\'e}j{\`a} cr{\'e}{\'e} il nous faut de nouvelles recherches pour traiter des langues sp{\'e}cifiques comme le vietnamien qui sont de nature assez diff{\'e}rente de l{'}anglais. donc nous avons appliqu{\'e} la conception du syntagme pour {\'e}tablir un algorithme qui est capable de r{\'e}soudre ce probl{\`e}me.
931,van-der-wees-etal-2016-measuring,"   Measuring the Effect of Conversational Aspects on Machine Translation Quality"",
",research in statistical machine translation (smt) is largely driven by formal translation tasks while translating informal text is much more challenging. in this paper we focus on smt for the informal genre of dialogues which has rarely been addressed to date. concretely we investigate the effect of dialogue acts speakers gender and text register on smt quality when translating fictional dialogues. we first create and release a corpus of multilingual movie dialogues annotated with these four dialogue-specific aspects. when measuring translation performance for each of these variables we find that bleu fluctuations between their categories are often significantly larger than randomly expected. following this finding we hypothesize and show that smt of fictional dialogues benefits from adaptation towards dialogue acts and registers. finally we find that male speakers are harder to translate and use more vulgar language than female speakers and that vulgarity is often not preserved during translation.
932,faraj-etal-2021-sarcasmdet,"   {S}arcasm{D}et at Sarcasm Detection Task 2021 in {A}rabic using {A}ra{BERT} Pretrained Model"",
",this paper presents one of the top five winning solutions for the shared task on sarcasm and sentiment detection in arabic (subtask-1 sarcasm detection). the goal of the task is to identify whether a tweet is sarcastic or not. our solution has been developed using ensemble technique with arabert pre-trained model. we describe the architecture of the submitted solution in the shared task. we also provide the experiments and the hyperparameter tuning that lead to this result. besides we discuss and analyze the results by comparing all the models that we trained or tested to achieve a better score in a table design. our model is ranked fifth out of 27 teams with an f1 score of 0.5985. it is worth mentioning that our model achieved the highest accuracy score of 0.7830
933,chlebowski-ballier-2020-cest,"   {C}{'}est {``}mm-hm, oui{''} ou {``}mm-hm, non{''} ? Propositions pour une grammaire des composantes acoustiques des interactions nasalis{\'e}es (A modest proposal for the pragmatic of nasal grunts in the {CID} corpus)"",
",cet article se propose d{'}envisager l{'}existence d{'}une grammaire sp{\'e}cifique aux interactions nasalis{\'e}es (chl{\'e}bowski et ballier 2015). notre proposition se fonde sur une annotation des composantes acoustiques de cette sous-cat{\'e}gorie de sons non-lexicaux (ward 2006) dans le corpus cid (bertrand et al. 2008). nous voudrions pr{\'e}senter les contraintes combinatoires et r{\'e}gularit{\'e}s qui semblent s{'}appliquer {\`a} ces composantes acoustiques ainsi que discuter leur structuration. les r{\'e}sultats pr{\'e}liminaires de l{'}analyse des composantes acoustiques semblent sugg{\'e}rer des plages de valeurs par d{\'e}faut pour les r{\'e}alisations des in (notamment pour la dur{\'e}e). la violation de ces usages peut donner lieu {\`a} une analyse de type gricienne d{'}implicature.
934,abdul-hameed-etal-2016-automatic,"   Automatic Creation of a Sentence Aligned {S}inhala-{T}amil Parallel Corpus"",
",a sentence aligned parallel corpus is an important prerequisite in statistical machine translation. however manual creation of such a parallel corpus is time consuming and requires experts fluent in both languages. automatic creation of a sentence aligned parallel corpus using parallel text is the solution to this problem. in this paper we present the first ever empirical evaluation carried out to identify the best method to automatically create a sentence aligned sinhala-tamil parallel corpus. annual reports from sri lankan government institutions were used as the parallel text for aligning. despite both sinhala and tamil being under-resourced languages we were able to achieve an f-score value of 0.791 using a hybrid approach that makes use of a bilingual dictionary.
935,mcleod-etal-2019-multi,"   Multi-Task Learning of System Dialogue Act Selection for Supervised Pretraining of Goal-Oriented Dialogue Policies"",
",this paper describes the use of multi-task neural networks (nns) for system dialogue act selection. these models leverage the representations learned by the natural language understanding (nlu) unit to enable robust initialization/bootstrapping of dialogue policies from medium sized initial data sets. we evaluate the models on two goal-oriented dialogue corpora in the travel booking domain. results show the proposed models improve over models trained without knowledge of nlu tasks.
936,zheng-etal-2020-simultaneous,"   Simultaneous Translation Policies: From Fixed to Adaptive"",
",adaptive policies are better than fixed policies for simultaneous translation since they can flexibly balance the tradeoff between translation quality and latency based on the current context information. but previous methods on obtaining adaptive policies either rely on complicated training process or underperform simple fixed policies. we design an algorithm to achieve adaptive policies via a simple heuristic composition of a set of fixed policies. experiments on chinese -{\textgreater} english and german -{\textgreater} english show that our adaptive policies can outperform fixed ones by up to 4 bleu points for the same latency and more surprisingly it even surpasses the bleu score of full-sentence translation in the greedy mode (and very close to beam mode) but with much lower latency.
937,ait-azzi-kang-2020-extractive,"   Extractive Summarization System for Annual Reports"",
",in this paper we report on our experiments in building a summarization system for generating summaries from annual reports. we adopt an {``}extractive{''} summarization approach in our hybrid system combining neural networks and rules-based algorithms with the expectation that such a system may capture key sentences or paragraphs from the data. a rules-based toc (table of contents) extraction and a binary classifier of narrative section titles are main components of our system allowing to identify narrative sections and best candidates for extracting final summaries. as result we propose one to three summaries per document according to the classification score of narrative section titles.
938,spiewak-etal-2017-opi,"   {OPI}-{JSA} at {S}em{E}val-2017 Task 1: Application of Ensemble learning for computing semantic textual similarity"",
",semantic textual similarity (sts) evaluation assesses the degree to which two parts of texts are similar based on their semantic evaluation. in this paper we describe three models submitted to sts semeval 2017. given two english parts of a text each of proposed methods outputs the assessment of their semantic similarity. we propose an approach for computing monolingual semantic textual similarity based on an ensemble of three distinct methods. our model consists of recursive neural network (rnn) text auto-encoders ensemble with supervised a model of vectorized sentences using reduced part of speech (pos) weighted word embeddings as well as unsupervised a method based on word coverage (takelab). additionally we enrich our model with additional features that allow disambiguation of ensemble methods based on their efficiency. we have used multi-layer perceptron as an ensemble classifier basing on estimations of trained gradient boosting regressors. results of our research proves that using such ensemble leads to a higher accuracy due to a fact that each member-algorithm tends to specialize in particular type of sentences. simple model based on pos weighted word2vec word embeddings seem to improve performance of more complex rnn based auto-encoders in the ensemble. in the monolingual english-english sts subtask our ensemble based model achieved mean pearson correlation of .785 compared with human annotators.
939,powell-blodgett-2008-use,"   The Use of Machine-generated Transcripts during Human Translation"",
",at the request of the usg national virtual translation center the university of maryland center for advanced study of language conducted a study that assessed the role of several factors mediating transcript usefulness during translation tasks. these factors included source language (mandarin or modern standard arabic) native speaker status of the translators transcript quality (low or moderate word error rate) and transcript functionality (static or dynamic). using 54 mandarin and 54 arabic translators (half native speakers in each language) and broadcast news clips for input the study demonstrated that translation environments that provide dynamic transcripts with low or moderate word error rates are likely to improve performance (measured as integrated speed and accuracy scores) among non-native speakers without decreasing performance among native speakers.
940,bao-etal-2021-span-fine,"   Span Fine-tuning for Pre-trained Language Models"",
",pre-trained language models (prlm) have to carefully manage input units when training on a very large text with a vocabulary consisting of millions of words. previous works have shown that incorporating span-level information over consecutive words in pre-training could further improve the performance of prlms. however given that span-level clues are introduced and fixed in pre-training previous methods are time-consuming and lack of flexibility. to alleviate the inconvenience this paper presents a novel span fine-tuning method for prlms which facilitates the span setting to be adaptively determined by specific downstream tasks during the fine-tuning phase. in detail any sentences processed by the prlm will be segmented into multiple spans according to a pre-sampled dictionary. then the segmentation information will be sent through a hierarchical cnn module together with the representation outputs of the prlm and ultimately generate a span-enhanced representation. experiments on glue benchmark show that the proposed span fine-tuning method significantly enhances the prlm and at the same time offer more flexibility in an efficient way.
941,villegas-etal-2012-using,"   Using Language Resources in Humanities research"",
",in this paper we present two real cases in the fields of discourse analysis of newspapers and communication research which demonstrate the impact of language resources (lr) and nlp in the humanities. we describe our collaboration with (i) the feminario research group from the uab which has been investigating androcentric practices in spanish general press since the 80s and whose research suggests that spanish general press has undergone a dehumanization process that excludes women and men and (ii) the municipals'11 online project which investigates the spanish local election campaign in the blogosphere. we will see how nlp tools and lrs make possible the so called e-humanities research' as they provide humanities with tools to perform intensive and automatic text analyses. language technologies have evolved a lot and are mature enough to provide useful tools to researchers dealing with large amount of textual data. the language resources that have been developed within the field of nlp have proven to be useful for other disciplines that are unaware of their existence and nevertheless would greatly benefit from them as they provide (i) exhaustiveness -to guarantee that data coverage is wide and representative enough- and (ii) reliable and significant results -to guarantee that the reported results are statistically significant.
942,tebbifakhr-etal-2020-automatic,"   Automatic Translation for Multiple {NLP} tasks: a Multi-task Approach to Machine-oriented {NMT} Adaptation"",
",although machine translation (mt) traditionally pursues {``}human-oriented{''} objectives humans are not the only possible consumers of mt output. for instance when automatic translations are used to feed downstream natural language processing (nlp) components in cross-lingual settings they should ideally pursue {``}machine-oriented{''} objectives that maximize the performance of these components. tebbifakhr et al. (2019) recently proposed a reinforcement learning approach to adapt a generic neural mt(nmt) system by exploiting the reward from a downstream sentiment classifier. but what if the downstream nlp tasks to serve are more than one? how to avoid the costs of adapting and maintaining one dedicated nmt system for each task? we address this problem by proposing a multi-task approach to machine-oriented nmt adaptation which is capable to serve multiple downstream tasks with a single system. through experiments with spanish and italian data covering three different tasks we show that our approach can outperform a generic nmt system and compete with single-task models in most of the settings.
943,geoffrois-2016-evaluating,"   Evaluating Interactive System Adaptation"",
",enabling users of intelligent systems to enhance the system performance by providing feedback on their errors is an important need. however the ability of systems to learn from user feedback is difficult to evaluate in an objective and comparative way. indeed the involvement of real users in the adaptation process is an impediment to objective evaluation. this issue can be solved by using an oracle approach where users are simulated by oracles having access to the reference test data. another difficulty is to find a meaningful metric despite the fact that system improvements depend on the feedback provided and on the system itself. a solution is to measure the minimal amount of information needed to correct all system errors. it can be shown that for any well defined non interactive task the interactively supervised version of the task can be evaluated by combining such an oracle-based approach and a minimum supervision rate metric. this new evaluation protocol for adaptive systems is not only expected to drive progress for such systems but also to pave the way for a specialisation of actors along the value chain of their technological development.
944,qu-etal-2019-adversarial,"   Adversarial Category Alignment Network for Cross-domain Sentiment Classification"",
",cross-domain sentiment classification aims to predict sentiment polarity on a target domain utilizing a classifier learned from a source domain. most existing adversarial learning methods focus on aligning the global marginal distribution by fooling a domain discriminator without taking category-specific decision boundaries into consideration which can lead to the mismatch of category-level features. in this work we propose an adversarial category alignment network (acan) which attempts to enhance category consistency between the source domain and the target domain. specifically we increase the discrepancy of two polarity classifiers to provide diverse views locating ambiguous features near the decision boundaries. then the generator learns to create better features away from the category boundaries by minimizing this discrepancy. experimental results on benchmark datasets show that the proposed method can achieve state-of-the-art performance and produce more discriminative features.
945,luo-etal-2017-learning,"   Learning to Predict Charges for Criminal Cases with Legal Basis"",
",the charge prediction task is to determine appropriate charges for a given case which is helpful for legal assistant systems where the user input is fact description. we argue that relevant law articles play an important role in this task and therefore propose an attention-based neural network method to jointly model the charge prediction task and the relevant article extraction task in a unified framework. the experimental results show that besides providing legal basis the relevant articles can also clearly improve the charge prediction results and our full model can effectively predict appropriate charges for cases with different expression styles.
946,barreiro-batista-2018-contractions,"   {C}ontractions: To Align or Not to Align, That Is the Question"",
",this paper performs a detailed analysis on the alignment of portuguese contractions based on a previously aligned bilingual corpus. the alignment task was performed manually in a subset of the english-portuguese clue4translation alignment collection. the initial parallel corpus was pre-processed and a decision was made as to whether the contraction should be maintained or decomposed in the alignment. decomposition was required in the cases in which the two words that have been concatenated i.e. the preposition and the determiner or pronoun go in two separate translation alignment pairs (e.g. [no seio de] [a uni{\~a}o europeia] | [within] [the european union]). most contractions required decomposition in contexts where they are positioned at the end of a multiword unit. on the other hand contractions tend to be maintained when they occur in the beginning or in the middle of the multiword unit i.e. in the frozen part of the multiword (e.g. [no que diz respeito a] | [with regard to] or [al{\'e}m disso] [in addition]. a correct alignment of multiwords and phrasal units containing contractions is instrumental for machine translation paraphrasing and variety adaptation.
947,khanpour-caragea-2018-fine,"   Fine-Grained Emotion Detection in Health-Related Online Posts"",
",detecting fine-grained emotions in online health communities provides insightful information about patients{'} emotional states. however current computational approaches to emotion detection from health-related posts focus only on identifying messages that contain emotions with no emphasis on the emotion type using a set of handcrafted features. in this paper we take a step further and propose to detect fine-grained emotion types from health-related posts and show how high-level and abstract features derived from deep neural networks combined with lexicon-based features can be employed to detect emotions.
948,swarup-etal-2020-instance,"   {A}n {I}nstance {L}evel {A}pproach for {S}hallow {S}emantic {P}arsing in {S}cientific {P}rocedural {T}ext"",
",in specific domains such as procedural scientific text human labeled data for shallow semantic parsing is especially limited and expensive to create. fortunately such specific domains often use rather formulaic writing such that the different ways of expressing relations in a small number of grammatically similar labeled sentences may provide high coverage of semantic structures in the corpus through an appropriately rich similarity metric. in light of this opportunity this paper explores an instance-based approach to the relation prediction sub-task within shallow semantic parsing in which semantic labels from structurally similar sentences in the training set are copied to test sentences. candidate similar sentences are retrieved using scibert embeddings. for labels where it is possible to copy from a similar sentence we employ an instance level copy network when this is not possible a globally shared parametric model is employed. experiments show our approach outperforms both baseline and prior methods by 0.75 to 3 f1 absolute in the wet lab protocol corpus and 1 f1 absolute in the materials science procedural text corpus.
949,escudero-etal-2014-use,"   On the use of a fuzzy classifier to speed up the {S}p{\_}{T}o{BI} labeling of the Glissando {S}panish corpus"",
",in this paper we present the application of a novel automatic prosodic labeling methodology for speeding up the manual labeling of the glissando corpus (spanish read news items). the methodology is based on the use of soft classification techniques. the output of the automatic system consists on a set of label candidates per word. the number of predicted candidates depends on the degree of certainty assigned by the classifier to each of the predictions. the manual transcriber checks the sets of predictions to select the correct one. we describe the fundamentals of the fuzzy classification tool and its training with a corpus labeled with sp tobi labels. results show a clear coherence between the most confused labels in the output of the automatic classifier and the most confused labels detected in inter-transcriber consistency tests. more importantly in a preliminary test the real time ratio of the labeling process was 1:66 when the template of predictions is used and 1:80 when it is not.
950,nguyen-bryant-2020-canvec,"   {C}an{VEC} - the Canberra {V}ietnamese-{E}nglish Code-switching Natural Speech Corpus"",
",this paper introduces the canberra vietnamese-english code-switching corpus (canvec) an original corpus of natural mixed speech that we semi-automatically annotated with language information part of speech (pos) tags and vietnamese translations. the corpus which was built to inform a sociolinguistic study on language variation and code-switching consists of 10 hours of recorded speech (87k tokens) between 45 vietnamese-english bilinguals living in canberra australia. we describe how we collected and annotated the corpus by pipelining several monolingual toolkits to considerably speed up the annotation process. we also describe how we evaluated the automatic annotations to ensure corpus reliability. we make the corpus available for research purposes.
951,vickers-etal-2021-factuality,"   In Factuality: Efficient Integration of Relevant Facts for Visual Question Answering"",
",visual question answering (vqa) methods aim at leveraging visual input to answer questions that may require complex reasoning over entities. current models are trained on labelled data that may be insufficient to learn complex knowledge representations. in this paper we propose a new method to enhance the reasoning capabilities of a multi-modal pretrained model (vision+language bert) by integrating facts extracted from an external knowledge base. evaluation on the kvqa dataset benchmark demonstrates that our method outperforms competitive baselines by 19{\%} achieving new state-of-the-art results. we also perform an extensive analysis highlighting the limitations of our best performing model through an ablation study.
952,bian-etal-2019-domain,"   Domain Adaptation for Person-Job Fit with Transferable Deep Global Match Network"",
",person-job fit has been an important task which aims to automatically match job positions with suitable candidates. previous methods mainly focus on solving the match task in single-domain setting which may not work well when labeled data is limited. we study the domain adaptation problem for person-job fit. we first propose a deep global match network for capturing the global semantic interactions between two sentences from a job posting and a candidate resume respectively. furthermore we extend the match network and implement domain adaptation in three levels sentence-level representation sentence-level match and global match. extensive experiment results on a large real-world dataset consisting of six domains have demonstrated the effectiveness of the proposed model especially when there is not sufficient labeled data.
953,yan-etal-2021-control,"   Control Image Captioning Spatially and Temporally"",
",generating image captions with user intention is an emerging need. the recently published localized narratives dataset takes mouse traces as another input to the image captioning task which is an intuitive and efficient way for a user to control what to describe in the image. however how to effectively employ traces to improve generation quality and controllability is still under exploration. this paper aims to solve this problem by proposing a novel model called loopcag which connects contrastive constraints and attention guidance in a loop manner engaged explicit spatial and temporal constraints to the generating process. precisely each generated sentence is temporally aligned to the corresponding trace sequence through a contrastive learning strategy. besides each generated text token is supervised to attend to the correct visual objects under heuristic spatial attention guidance. comprehensive experimental results demonstrate that our loopcag model learns better correspondence among the three modalities (vision language and traces) and achieves sota performance on trace-controlled image captioning task. moreover the controllability and explainability of loopcag are validated by analyzing spatial and temporal sensitivity during the generation process.
954,yimam-biemann-2018-demonstrating,"   Demonstrating {P}ar4{S}em - A Semantic Writing Aid with Adaptive Paraphrasing"",
",in this paper we present par4sem a semantic writing aid tool based on adaptive paraphrasing. unlike many annotation tools that are primarily used to collect training examples par4sem is integrated into a real word application in this case a writing aid tool in order to collect training examples from usage data. par4sem is a tool which supports an adaptive iterative and interactive process where the underlying machine learning models are updated for each iteration using new training examples from usage data. after motivating the use of ever-learning tools in nlp applications we evaluate par4sem by adopting it to a text simplification task through mere usage.
955,chisholm-etal-2017-learning,"   Learning to generate one-sentence biographies from {W}ikidata"",
",we investigate the generation of one-sentence wikipedia biographies from facts derived from wikidata slot-value pairs. we train a recurrent neural network sequence-to-sequence model with attention to select facts and generate textual summaries. our model incorporates a novel secondary objective that helps ensure it generates sentences that contain the input facts. the model achieves a bleu score of 41 improving significantly upon the vanilla sequence-to-sequence model and scoring roughly twice that of a simple template baseline. human preference evaluation suggests the model is nearly as good as the wikipedia reference. manual analysis explores content selection suggesting the model can trade the ability to infer knowledge against the risk of hallucinating incorrect information.
956,roberts-etal-2010-linguistic,"   A Linguistic Resource for Semantic Parsing of Motion Events"",
",this paper presents a corpus of annotated motion events and their event structure. we consider motion events triggered by a set of motion evoking words and contemplate both literal and figurative interpretations of them. figurative motion events are extracted into the same event structure but are marked as figurative in the corpus. to represent the event structure of motion we use the framenet annotation standard which encodes motion in over 70 frames. in order to acquire a diverse set of texts that are different from framenet's we crawled blog and news feeds for five different domains: sports newswire finance military and gossip. we then annotated these documents with an automatic framenet parser. its output was manually corrected to account for missing and incorrect frames as well as missing and incorrect frame elements. the corpus utd-motionevent may act as a resource for semantic parsing detection of figurative language spatial reasoning and other tasks.
957,kumar-etal-2020-data,"   Data Augmentation using Pre-trained Transformer Models"",
",language model based pre-trained models such as bert have provided significant gains across different nlp tasks. in this paper we study different types of transformer based pre-trained models such as auto-regressive models (gpt-2) auto-encoder models (bert) and seq2seq models (bart) for conditional data augmentation. we show that prepending the class labels to text sequences provides a simple yet effective way to condition the pre-trained models for data augmentation. additionally on three classification benchmarks pre-trained seq2seq model outperforms other data augmentation methods in a low-resource setting. further we explore how different pre-trained model based data augmentation differs in-terms of data diversity and how well such methods preserve the class-label information.
958,jacobs-etal-2018-economic,"   Economic Event Detection in Company-Specific News Text"",
",this paper presents a dataset and supervised classification approach for economic event detection in english news articles. currently the economic domain is lacking resources and methods for data-driven supervised event detection. the detection task is conceived as a sentence-level classification task for 10 different economic event types. two different machine learning approaches were tested: a rich feature set support vector machine (svm) set-up and a word-vector-based long short-term memory recurrent neural network (rnn-lstm) set-up. we show satisfactory results for most event types with the linear kernel svm outperforming the other experimental set-ups
959,nakov-etal-2021-second,"   A Second Pandemic? Analysis of Fake News about {COVID}-19 Vaccines in {Q}atar"",
",while covid-19 vaccines are finally becoming widely available a second pandemic that revolves around the circulation of anti-vaxxer {``}fake news{''} may hinder efforts to recover from the first one. with this in mind we performed an extensive analysis of arabic and english tweets about covid-19 vaccines with focus on messages originating from qatar. we found that arabic tweets contain a lot of false information and rumors while english tweets are mostly factual. however english tweets are much more propagandistic than arabic ones. in terms of propaganda techniques about half of the arabic tweets express doubt and 1/5 use loaded language while english tweets are abundant in loaded language exaggeration fear name-calling doubt and flag-waving. finally in terms of framing arabic tweets adopt a health and safety perspective while in english economic concerns dominate.
960,field-etal-2020-generative,"   A Generative Approach to Titling and Clustering {W}ikipedia Sections"",
",we evaluate the performance of transformer encoders with various decoders for information organization through a new task: generation of section headings for wikipedia articles. our analysis shows that decoders containing attention mechanisms over the encoder output achieve high-scoring results by generating extractive text. in contrast a decoder without attention better facilitates semantic encoding and can be used to generate section embeddings. we additionally introduce a new loss function which further encourages the decoder to generate high-quality embeddings.
961,deng-etal-2020-hierarchical,"   Hierarchical Bi-Directional Self-Attention Networks for Paper Review Rating Recommendation"",
",review rating prediction of text reviews is a rapidly growing technology with a wide range of applications in natural language processing. however most existing methods either use hand-crafted features or learn features using deep learning with simple text corpus as input for review rating prediction ignoring the hierarchies among data. in this paper we propose a hierarchical bi-directional self-attention network framework (habnet) for paper review rating prediction and recommendation which can serve as an effective decision-making tool for the academic paper review process. specifically we leverage the hierarchical structure of the paper reviews with three levels of encoders: sentence encoder (level one) intra-review encoder (level two) and inter-review encoder (level three). each encoder first derives contextual representation of each level then generates a higher-level representation and after the learning process we are able to identify useful predictors to make the final acceptance decision as well as to help discover the inconsistency between numerical review ratings and text sentiment conveyed by reviewers. furthermore we introduce two new metrics to evaluate models in data imbalance situations. extensive experiments on a publicly available dataset (peerread) and our own collected dataset (openreview) demonstrate the superiority of the proposed approach compared with state-of-the-art methods.
962,zhao-etal-2017-n,"   N-gram Model for {C}hinese Grammatical Error Diagnosis"",
",detection and correction of chinese grammatical errors have been two of major challenges for chinese automatic grammatical error diagnosis.this paper presents an n-gram model for automatic detection and correction of chinese grammatical errors in nlptea 2017 task. the experiment results show that the proposed method is good at correction of chinese grammatical errors.
963,ljubesic-etal-2017-adapting,"   Adapting a State-of-the-Art Tagger for {S}outh {S}lavic Languages to Non-Standard Text"",
",in this paper we present the adaptations of a state-of-the-art tagger for south slavic languages to non-standard texts on the example of the slovene language. we investigate the impact of introducing in-domain training data as well as additional supervision through external resources or tools like word clusters and word normalization. we remove more than half of the error of the standard tagger when applied to non-standard texts by training it on a combination of standard and non-standard training data while enriching the data representation with external resources removes additional 11 percent of the error. the final configuration achieves tagging accuracy of 87.41{\%} on the full morphosyntactic description which is nevertheless still quite far from the accuracy of 94.27{\%} achieved on standard text.
964,bastan-etal-2020-authors,"   Author{'}s Sentiment Prediction"",
",even though sentiment analysis has been well-studied on a wide range of domains there hasn{'}tbeen much work on inferring author sentiment in news articles. to address this gap we introducepersent a crowd-sourced dataset that captures the sentiment of an author towards the mainentity in a news article. our benchmarks of multiple strong baselines show that this is a difficultclassification task. bert performs the best amongst the baselines. however it only achievesa modest performance overall suggesting that fine-tuning document-level representations aloneisn{'}t adequate for this task. making paragraph-level decisions and aggregating over the entiredocument is also ineffective. we present empirical and qualitative analyses that illustrate thespecific challenges posed by this dataset. we release this dataset with 5.3k documents and 38kparagraphs with 3.2k unique entities as a challenge in entity sentiment analysis.
965,zhou-etal-2019-ynu,"   {YNU}-{HPCC} at {S}em{E}val-2019 Task 6: Identifying and Categorising Offensive Language on {T}witter"",
",this document describes the submission of team ynu-hpcc to semeval-2019 for three sub-tasks of task 6: sub-task a sub-task b and sub-task c. we have submitted four systems to identify and categorise offensive language. the first subsystem is an attention-based 2-layer bidirectional long short-term memory (bilstm). the second subsystem is a voting ensemble of four different deep learning architectures. the third subsystem is a stacking ensemble of four different deep learning architectures. finally the fourth subsystem is a bidirectional encoder representations from transformers (bert) model. among our models in sub-task a our first subsystem performed the best ranking 16th among 103 teams; in sub-task b the second subsystem performed the best ranking 12th among 75 teams; in sub-task c the fourth subsystem performed best ranking 4th among 65 teams.
966,verhagen-2010-brandeis,"   The {B}randeis Annotation Tool"",
",the brandeis annotation tool (bat) is a web-based text annotation tool that is centered around the notions of layered annotation and task decomposition. it allows annotations to refer to other annotations and to take a complicated task and split it into easier subtasks. the central organizing concept of bat is the annotation layer. a corpus administrator can create annotation layers that involve annotation of extents attributes or relations. the layer definition includes the labels used the attributes that are available and restrictions on the values for those attributes. for each annotation layer files can be assigned to one or more annotators and one judge. when annotators log in the assigned layers and files therein are presented. when selecting a file to annotate the interface uses the layer definition to display the annotation interface. the web-interface connects administrators and annotators to a central repository for all data and simplifies many of the housekeeping tasks while keeping requirements at a minimum (that is users only need an internet connection and a well-behaved browser). bat has been used mainly for temporal annotation but can be considered a more general tool for several kinds of textual annotation.
967,wang-etal-2019-using,"   Using {R}hetorical {S}tructure {T}heory to Assess Discourse Coherence for Non-native Spontaneous Speech"",
",this study aims to model the discourse structure of spontaneous spoken responses within the context of an assessment of english speaking proficiency for non-native speakers. rhetorical structure theory (rst) has been commonly used in the analysis of discourse organization of written texts; however limited research has been conducted to date on rst annotation and parsing of spoken language in particular non-native spontaneous speech. due to the fact that the measurement of discourse coherence is typically a key metric in human scoring rubrics for assessments of spoken language we conducted research to obtain rst annotations on non-native spoken responses from a standardized assessment of academic english proficiency. subsequently automatic parsers were trained on these annotations to process non-native spontaneous speech. finally a set of features were extracted from automatically generated rst trees to evaluate the discourse structure of non-native spontaneous speech which were then employed to further improve the validity of an automated speech scoring system.
968,friedrich-etal-2020-sofc,"   The {SOFC}-Exp Corpus and Neural Approaches to Information Extraction in the Materials Science Domain"",
",this paper presents a new challenging information extraction task in the domain of materials science. we develop an annotation scheme for marking information on experiments related to solid oxide fuel cells in scientific publications such as involved materials and measurement conditions. with this paper we publish our annotation guidelines as well as our sofc-exp corpus consisting of 45 open-access scholarly articles annotated by domain experts. a corpus and an inter-annotator agreement study demonstrate the complexity of the suggested named entity recognition and slot filling tasks as well as high annotation quality. we also present strong neural-network based models for a variety of tasks that can be addressed on the basis of our new data set. on all tasks using bert embeddings leads to large performance gains but with increasing task complexity adding a recurrent neural network on top seems beneficial. our models will serve as competitive baselines in future work and analysis of their performance highlights difficult cases when modeling the data and suggests promising research directions.
969,liu-etal-2020-cross-lingual,"   Cross-Lingual Document Retrieval with Smooth Learning"",
",cross-lingual document search is an information retrieval task in which the queries{'} language and the documents{'} language are different. in this paper we study the instability of neural document search models and propose a novel end-to-end robust framework that achieves improved performance in cross-lingual search with different documents{'} languages. this framework includes a novel measure of the relevance smooth cosine similarity between queries and documents and a novel loss function smooth ordinal search loss as the objective function. we further provide theoretical guarantee on the generalization error bound for the proposed framework. we conduct experiments to compare our approach with other document search models and observe significant gains under commonly used ranking metrics on the cross-lingual document retrieval task in a variety of languages.
970,stymne-ahrenberg-2010-using,"   Using a Grammar Checker for Evaluation and Postprocessing of Statistical Machine Translation"",
",one problem in statistical machine translation (smt) is that the output often is ungrammatical. to address this issue we have investigated the use of a grammar checker for two purposes in connection with smt: as an evaluation tool and as a postprocessing tool. to assess the feasibility of the grammar checker on smt output we performed an error analysis which showed that the precision of error identification in general was higher on smt output than in previous studies on human texts. using the grammar checker as an evaluation tool gives a complementary picture to standard metrics such as bleu which do not account well for grammaticality. we use the grammar checker as a postprocessing tool by automatically applying the error correction suggestions it gives. there are only small overall improvements of the postprocessing on automatic metrics but the sentences that are affected by the changes are improved as shown both by automatic metrics and by a human error analysis. these results indicate that grammar checker techniques are a useful complement to smt.
971,jauregi-unanue-etal-2019-rewe,"   {R}e{WE}: Regressing Word Embeddings for Regularization of Neural Machine Translation Systems"",
",regularization of neural machine translation is still a significant problem especially in low-resource settings. to mollify this problem we propose regressing word embeddings (rewe) as a new regularization technique in a system that is jointly trained to predict the next word in the translation (categorical value) and its word embedding (continuous value). such a joint training allows the proposed system to learn the distributional properties represented by the word embeddings empirically improving the generalization to unseen sentences. experiments over three translation datasets have showed a consistent improvement over a strong baseline ranging between 0.91 and 2.4 bleu points and also a marked improvement over a state-of-the-art system.
972,schutz-2001-blueprints,"   Blueprints for {MT} evolution: reflections on elements of style"",
",in this paper organized in essay style i first assess the situation of machine translation which is characterized on the one hand by unsatisfied user expectations and on the other hand by an ever increasing need for translation technology to fulfil the promises of the global knowledge society which is promoted by almost all governments and industries worldwide. the assessment is followed by an outline of the design of a blueprint that describes possible steps of an mt evolution regarding short term mid term and long term developments. although some user communities might aim at an mt revolution the evolutionary implementation of the different aspects of the blueprint fit seamless with the foundation that we are faced with in the assessment part. with the blueprint the thesis of this mt evolution essay is established and the stage is opened for the antithesis in which i develop the points for an mt revolution. finally in the synthesis part i develop a combined view which then completes the discussion and the establishment of a blueprint for mt evolution.
973,kochkina-liakata-2020-estimating,"   Estimating predictive uncertainty for rumour verification models"",
",the inability to correctly resolve rumours circulating online can have harmful real-world consequences. we present a method for incorporating model and data uncertainty estimates into natural language processing models for automatic rumour verification. we show that these estimates can be used to filter out model predictions likely to be erroneous so that these difficult instances can be prioritised by a human fact-checker. we propose two methods for uncertainty-based instance rejection supervised and unsupervised. we also show how uncertainty estimates can be used to interpret model performance as a rumour unfolds.
974,pouran-ben-veyseh-etal-2020-acronym,"   What Does This Acronym Mean? Introducing a New Dataset for Acronym Identification and Disambiguation"",
",acronyms are the short forms of phrases that facilitate conveying lengthy sentences in documents and serve as one of the mainstays of writing. due to their importance identifying acronyms and corresponding phrases (i.e. acronym identification (ai)) and finding the correct meaning of each acronym (i.e. acronym disambiguation (ad)) are crucial for text understanding. despite the recent progress on this task there are some limitations in the existing datasets which hinder further improvement. more specifically limited size of manually annotated ai datasets or noises in the automatically created acronym identification datasets obstruct designing advanced high-performing acronym identification models. moreover the existing datasets are mostly limited to the medical domain and ignore other domains. in order to address these two limitations we first create a manually annotated large ai dataset for scientific domain. this dataset contains 17506 sentences which is substantially larger than previous scientific ai datasets. next we prepare an ad dataset for scientific domain with 62441 samples which is significantly larger than previous scientific ad dataset. our experiments show that the existing state-of-the-art models fall far behind human-level performance on both datasets proposed by this work. in addition we propose a new deep learning model which utilizes the syntactical structure of the sentence to expand an ambiguous acronym in a sentence. the proposed model outperforms the state-of-the-art models on the new ad dataset providing a strong baseline for future research on this dataset.
975,brandsen-etal-2020-creating,"   Creating a Dataset for Named Entity Recognition in the Archaeology Domain"",
",in this paper we present the development of a training dataset for dutch named entity recognition (ner) in the archaeology domain. this dataset was created as there is a dire need for semantic search within archaeology in order to allow archaeologists to find structured information in collections of dutch excavation reports currently totalling around 60000 (658 million words) and growing rapidly. to guide this search task ner is needed. we created rigorous annotation guidelines in an iterative process then instructed five archaeology students to annotate a number of documents. the resulting dataset contains {\textasciitilde}31k annotations between six entity types (artefact time period place context species {\&} material). the inter-annotator agreement is 0.95 and when we used this data for machine learning we observed an increase in f1 score from 0.51 to 0.70 in comparison to a machine learning model trained on a dataset created in prior work. this indicates that the data is of high quality and can confidently be used to train ner classifiers.
976,krstev-etal-2010-description,"   A Description of Morphological Features of {S}erbian: a Revision using Feature System Declaration"",
",in this paper we discuss some well-known morphological descriptions used in various projects and applications (most notably multext-east and unitex) and illustrate the encountered problems on serbian. we have spotted four groups of problems: the lack of a value for an existing category the lack of a category the interdependence of values and categories lacking some description and the lack of a support for some types of categories. at the same time various descriptions often describe exactly the same morphological property using different approaches. we propose a new morphological description for serbian following the feature structure representation defined by the iso standard. in this description we try do incorporate all characteristics of serbian that need to be specified for various applications. we have developed several xslt scripts that transform our description into descriptions needed for various applications. we have developed the first version of this new description but we treat it as an ongoing project because for some properties we have not yet found the satisfactory solution.
977,gan-etal-2021-exploring,"   Exploring Underexplored Limitations of Cross-Domain Text-to-{SQL} Generalization"",
",recently there has been significant progress in studying neural networks for translating text descriptions into sql queries under the zero-shot cross-domain setting. despite achieving good performance on some public benchmarks we observe that existing text-to-sql models do not generalize when facing domain knowledge that does not frequently appear in the training data which may render the worse prediction performance for unseen domains. in this work we investigate the robustness of text-to-sql models when the questions require rarely observed domain knowledge. in particular we define five types of domain knowledge and introduce spider-dk (dk is the abbreviation of domain knowledge) a human-curated dataset based on the spider benchmark for text-to-sql translation. nl questions in spider-dk are selected from spider and we modify some samples by adding domain knowledge that reflects real-world question paraphrases. we demonstrate that the prediction accuracy dramatically drops on samples that require such domain knowledge even if the domain knowledge appears in the training set and the model provides the correct predictions for related training samples.
978,he-cohen-2020-english,"   {E}nglish-to-{C}hinese Transliteration with Phonetic Auxiliary Task"",
",approaching named entities transliteration as a neural machine translation (nmt) problem is common practice. while many have applied various nmt techniques to enhance machine transliteration models few focus on the linguistic features particular to the relevant languages. in this paper we investigate the effect of incorporating phonetic features for english-to-chinese transliteration under the multi-task learning (mtl) setting{---}where we define a phonetic auxiliary task aimed to improve the generalization performance of the main transliteration task. in addition to our system we also release a new english-to-chinese dataset and propose a novel evaluation metric which considers multiple possible transliterations given a source name. our results show that the multi-task model achieves similar performance as the previous state of the art with a model of a much smaller size.
979,noecker-jr-ryan-2012-distractorless,"   Distractorless Authorship Verification"",
",authorship verification is the task of given a document and a candi- date author determining whether or not the document was written by the candi- date author. traditional approaches to authorship verification have revolved around a candidate author vs. everything else approach. thus perhaps the most important aspect of performing authorship verification on a document is the development of an appropriate distractor set to represent everything not the candidate author. the validity of the results of such experiments hinges on the ability to develop an appropriately representative set of distractor documents. here we propose a method for performing authorship verification without the use of a distractor set. using only training data from the candidate author we are able to perform authorship verification with high confidence (greater than 90{\%} accuracy rates across a large corpus).
980,faruqui-etal-2017-cross,"   Cross-Lingual Word Representations: Induction and Evaluation"",
",in recent past nlp as a field has seen tremendous utility of distributional word vector representations as features in downstream tasks. the fact that these word vectors can be trained on unlabeled monolingual corpora of a language makes them an inexpensive resource in nlp. with the increasing use of monolingual word vectors there is a need for word vectors that can be used as efficiently across multiple languages as monolingually. therefore learning bilingual and multilingual word embeddings/vectors is currently an important research topic. these vectors offer an elegant and language-pair independent way to represent content across different languages.this tutorial aims to bring nlp researchers up to speed with the current techniques in cross-lingual word representation learning. we will first discuss how to induce cross-lingual word representations (covering both bilingual and multilingual ones) from various data types and resources (e.g. parallel data comparable data non-aligned monolingual data in different languages dictionaries and theasuri or even images eye-tracking data). we will then discuss how to evaluate such representations intrinsically and extrinsically. we will introduce researchers to state-of-the-art methods for constructing cross-lingual word representations and discuss their applicability in a broad range of downstream nlp applications.we will deliver a detailed survey of the current methods discuss best training and evaluation practices and use-cases and provide links to publicly available implementations datasets and pre-trained models.
981,park-etal-2016-classifying,"   Classifying Out-of-vocabulary Terms in a Domain-Specific Social Media Corpus"",
",in this paper we consider the problem of out-of-vocabulary term classification in web forum text from the automotive domain. we develop a set of nine domain- and application-specific categories for out-of-vocabulary terms. we then propose a supervised approach to classify out-of-vocabulary terms according to these categories drawing on features based on word embeddings and linguistic knowledge of common properties of out-of-vocabulary terms. we show that the features based on word embeddings are particularly informative for this task. the categories that we predict could serve as a preliminary automatically-generated source of lexical knowledge about out-of-vocabulary terms. furthermore we show that this approach can be adapted to give a semi-automated method for identifying out-of-vocabulary terms of a particular category automotive named entities that is of particular interest to us.
982,chiarcos-2014-towards,"   Towards interoperable discourse annotation. Discourse features in the Ontologies of Linguistic Annotation"",
",this paper describes the extension of the ontologies of linguistic annotation (olia) with respect to discourse features. the olia ontologies provide a a terminology repository that can be employed to facilitate the conceptual (semantic) interoperability of annotations of discourse phenomena as found in the most important corpora available to the community including ontonotes the rst discourse treebank and the penn discourse treebank. along with selected schemes for information structure and coreference discourse relations are discussed with special emphasis on the penn discourse treebank and the rst discourse treebank. for an example contained in the intersection of both corpora i show how ontologies can be employed to generalize over divergent annotation schemes.
983,lin-etal-2018-denoising,"   Denoising Distantly Supervised Open-Domain Question Answering"",
",distantly supervised open-domain question answering (ds-qa) aims to find answers in collections of unlabeled text. existing ds-qa models usually retrieve related paragraphs from a large-scale corpus and apply reading comprehension technique to extract answers from the most relevant paragraph. they ignore the rich information contained in other paragraphs. moreover distant supervision data inevitably accompanies with the wrong labeling problem and these noisy data will substantially degrade the performance of ds-qa. to address these issues we propose a novel ds-qa model which employs a paragraph selector to filter out those noisy paragraphs and a paragraph reader to extract the correct answer from those denoised paragraphs. experimental results on real-world datasets show that our model can capture useful information from noisy data and achieve significant improvements on ds-qa as compared to all baselines.
984,zhao-lee-2020-talk,"   Talk to Papers: Bringing Neural Question Answering to Academic Search"",
",we introduce talk to papers which exploits the recent open-domain question answering (qa) techniques to improve the current experience of academic search. it{'}s designed to enable researchers to use natural language queries to find precise answers and extract insights from a massive amount of academic papers. we present a large improvement over classic search engine baseline on several standard qa datasets and provide the community a collaborative data collection tool to curate the first natural language processing research qa dataset via a community effort.
985,caro-quintana-2021-integration,"   Integration of Machine Translation and Translation Memory: Post-Editing Efforts"",
",the development of translation technologies like translation memory and machine translation has completely changed the translation industry and translator{'}s workflow in the last decades. nevertheless tm and mt have been developed separately until very recently. this ongoing project will study the external integration of tm and mt examining if the productivity and post-editing efforts of translators are higher or lower than using only tm. to this end we will conduct an experiment where translation students and professional translators will be asked to translate two short texts; then we will check the post-editing efforts (temporal technical and cognitive efforts) and the quality of the translated texts.
986,prabhakaran-etal-2021-releasing,"   On Releasing Annotator-Level Labels and Information in Datasets"",
",a common practice in building nlp datasets especially using crowd-sourced annotations involves obtaining multiple annotator judgements on the same data instances which are then flattened to produce a single {``}ground truth{''} label or score through majority voting averaging or adjudication. while these approaches may be appropriate in certain annotation tasks such aggregations overlook the socially constructed nature of human perceptions that annotations for relatively more subjective tasks are meant to capture. in particular systematic disagreements between annotators owing to their socio-cultural backgrounds and/or lived experiences are often obfuscated through such aggregations. in this paper we empirically demonstrate that label aggregation may introduce representational biases of individual and group perspectives. based on this finding we propose a set of recommendations for increased utility and transparency of datasets for downstream use cases.
987,zhang-etal-2020-improving-adversarial,"   Improving Adversarial Text Generation by Modeling the Distant Future"",
",auto-regressive text generation models usually focus on local fluency and may cause inconsistent semantic meaning in long text generation. further automatically generating words with similar semantics is challenging and hand-crafted linguistic rules are difficult to apply. we consider a text planning scheme and present a model-based imitation-learning approach to alleviate the aforementioned issues. specifically we propose a novel guider network to focus on the generative process over a longer horizon which can assist next-word prediction and provide intermediate rewards for generator optimization. extensive experiments demonstrate that the proposed method leads to improved performance.
988,jancso-etal-2020-acqdiv,"   The {ACQDIV} Corpus Database and Aggregation Pipeline"",
",we present the acqdiv corpus database and aggregation pipeline a tool developed as part of the european research council (erc) funded project acqdiv which aims to identify the universal cognitive processes that allow children to acquire any language. the corpus database represents 15 corpora from 14 typologically maximally diverse languages. here we give an overview of the project database and our extensible software package for adding more corpora to the current language sample. lastly we discuss how we use the corpus database to mine for universal patterns in child language acquisition corpora and we describe avenues for future research.
989,hubers-etal-2020-dedicated,"   Dedicated Language Resources for Interdisciplinary Research on Multiword Expressions: Best Thing since Sliced Bread"",
",multiword expressions such as idioms (beat about the bush) collocations (plastic surgery) and lexical bundles (in the middle of) are challenging for disciplines like natural language processing (nlp) psycholinguistics and second language acquisition  due to their more or less fixed character. idiomatic expressions are especially problematic because they convey a figurative meaning that cannot always be inferred from the literal meanings of the component words. researchers acknowledge that important properties that characterize idioms such as frequency of exposure familiarity transparency and imageability should be taken into account in research but these are typically properties that rely on subjective judgments. this is probably one of the reasons why many studies that investigated idiomatic expressions collected limited information about idiom properties for very small numbers of idioms only. in this paper we report on cross-boundary work aimed at developing a set of tools and language resources that are considered crucial for this kind of multifaceted research. we discuss the results of our research and suggest possible avenues for future research
990,servan-dymetman-2015-adaptation,"   Adaptation par enrichissement terminologique en traduction automatique statistique fond{\'e}e sur la g{\'e}n{\'e}ration et le filtrage de bi-segments virtuels"",
",nous pr{\'e}sentons des travaux pr{\'e}liminaires sur une approche permettant d{'}ajouter des termes bilingues {\`a} un syst{\`e}me de traduction automatique statistique (tas) {\`a} base de segments. les termes sont non seulement inclus individuellement mais aussi avec des contextes les englobant. tout d{'}abord nous g{\'e}n{\'e}rons ces contextes en g{\'e}n{\'e}ralisant des motifs (ou patrons) observ{\'e}s pour des mots de m{\^e}me nature syntaxique dans un corpus bilingue. enfin nous filtrons les contextes qui n{'}atteignent pas un certain seuil de confiance {\`a} l{'}aide d{'}une m{\'e}thode de s{\'e}lection de bi-segments inspir{\'e}e d{'}une approche de s{\'e}lection de donn{\'e}es pr{\'e}c{\'e}demment appliqu{\'e}e {\`a} des textes bilingues align{\'e}s.
991,schulz-kuhn-2016-learning,"   Learning from Within? Comparing {P}o{S} Tagging Approaches for Historical Text"",
",in this paper we investigate unsupervised and semi-supervised methods for part-of-speech (pos) tagging in the context of historical german text. we locate our research in the context of digital humanities where the non-canonical nature of text causes issues facing an natural language processing world in which tools are mainly trained on standard data. data deviating from the norm requires tools adjusted to this data. we explore to which extend the availability of such training material and resources related to it influences the accuracy of pos tagging. we investigate a variety of algorithms including neural nets conditional random fields and self-learning techniques in order to find the best-fitted approach to tackle data sparsity. although methods using resources from related languages outperform weakly supervised methods using just a few training examples we can still reach a promising accuracy with methods abstaining additional resources.
992,chalkidis-etal-2021-multieurlex,"   {M}ulti{EURLEX} - A multi-lingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer"",
",we introduce multi-eurlex a new multilingual dataset for topic classification of legal documents. the dataset comprises 65k european union (eu) laws officially translated in 23 languages annotated with multiple labels from the eurovoc taxonomy. we highlight the effect of temporal concept drift and the importance of chronological instead of random splits. we use the dataset as a testbed for zero-shot cross-lingual transfer where we exploit annotated training documents in one language (source) to classify documents in another language (target). we find that fine-tuning a multilingually pretrained model (xlm-roberta mt5) in a single source language leads to catastrophic forgetting of multilingual knowledge and consequently poor zero-shot transfer to other languages. adaptation strategies namely partial fine-tuning adapters bitfit lnfit originally proposed to accelerate fine-tuning for new end-tasks help retain multilingual knowledge from pretraining substantially improving zero-shot cross-lingual transfer but their impact also depends on the pretrained model used and the size of the label set.
993,manor-li-2019-plain,"   Plain {E}nglish Summarization of Contracts"",
",unilateral legal contracts such as terms of service play a substantial role in modern digital life. however few read these documents before accepting the terms within as they are too long and the language too complicated. we propose the task of summarizing such legal documents in plain english which would enable users to have a better understanding of the terms they are accepting. we propose an initial dataset of legal text snippets paired with summaries written in plain english. we verify the quality of these summaries manually and show that they involve heavy abstraction compression and simplification. initial experiments show that unsupervised extractive summarization methods do not perform well on this task due to the level of abstraction and style differences. we conclude with a call for resource and technique development for simplification and style transfer for legal language.
994,rubino-sumita-2020-intermediate,"   Intermediate Self-supervised Learning for Machine Translation Quality Estimation"",
",pre-training sentence encoders is effective in many natural language processing tasks including machine translation (mt) quality estimation (qe) due partly to the scarcity of annotated qe data required for supervised learning. in this paper we investigate the use of an intermediate self-supervised learning task for sentence encoder aiming at improving qe performances at the sentence and word levels. our approach is motivated by a problem inherent to qe: mistakes in translation caused by wrongly inserted and deleted tokens. we modify the translation language model (tlm) training objective of the cross-lingual language model (xlm) to orientate the pre-trained model towards the target task. the proposed method does not rely on annotated data and is complementary to qe methods involving pre-trained sentence encoders and domain adaptation. experiments on english-to-german and english-to-russian translation directions show that intermediate learning improves over domain adaptated models. additionally our method reaches results in par with state-of-the-art qe models without requiring the combination of several approaches and outperforms similar methods based on pre-trained sentence encoders.
995,manshadi-etal-2012-annotation,"   An Annotation Scheme for Quantifier Scope Disambiguation"",
",annotating natural language sentences with quantifier scoping has proved to be very hard. in order to overcome the challenge previous work on building scope-annotated corpora has focused on sentences with two explicitly quantified noun phrases (nps). furthermore it does not address the annotation of scopal operators or complex nps such as plurals and definites. we present the first annotation scheme for quantifier scope disambiguation where there is no restriction on the type or the number of scope-bearing elements in the sentence. we discuss some of the most prominent complex scope phenomena encountered in annotating the corpus such as plurality and type-token distinction and present mechanisms to handle those phenomena.
996,lacerra-etal-2021-genesis,"   {G}ene{S}is: {A} {G}enerative {A}pproach to {S}ubstitutes in {C}ontext"",
",the lexical substitution task aims at generating a list of suitable replacements for a target word in context ideally keeping the meaning of the modified text unchanged. while its usage has increased in recent years the paucity of annotated data prevents the finetuning of neural models on the task hindering the full fruition of recently introduced powerful architectures such as language models. furthermore lexical substitution is usually evaluated in a framework that is strictly bound to a limited vocabulary making it impossible to credit appropriate but out-of-vocabulary substitutes. to assess these issues we proposed genesis (generating substitutes in contexts) the first generative approach to lexical substitution. thanks to a seq2seq model we generate substitutes for a word according to the context it appears in attaining state-of-the-art results on different benchmarks. moreover our approach allows silver data to be produced for further improving the performances of lexical substitution systems. along with an extensive analysis of genesis results we also present a human evaluation of the generated substitutes in order to assess their quality. we release the fine-tuned models the generated datasets and the code to reproduce the experiments at https://github.com/sapienzanlp/genesis.
997,radhakrishnan-2020-seed,"   A Seed Corpus of {H}indu Temples in {I}ndia"",
",temples are an integral part of culture and heritage of india and are centers of religious practice for practicing hindus. a scientific study of temples can reveal valuable insights into indian culture and heritage. however to the best of our knowledge learning resources that aid such a study are either not publicly available or non-existent. in this endeavour we present our initial efforts to create a corpus of hindu temples in india. in this paper we present a simple re-usable platform that creates temple corpus from web text on temples. curation is improved using classifiers trained on textual data in wikipedia articles on hindu temples. the training data is verified by human volunteers. the temple corpus consists of 4933 high accuracy facts about 573 temples. we make the corpus and the platform freely available. we also test the re-usability of the platform by creating a corpus of museums in india. we believe the temple corpus will aid scientific study of temples and the platform will aid in construction of similar corpuses. we believe both these will significantly contribute in promoting research on culture and heritage of a region.
998,stajner-etal-2017-sentence,"   Sentence Alignment Methods for Improving Text Simplification Systems"",
",we provide several methods for sentence-alignment of texts with different complexity levels. using the best of them we sentence-align the newsela corpora thus providing large training materials for automatic text simplification (ats) systems. we show that using this dataset even the standard phrase-based statistical machine translation models for ats can outperform the state-of-the-art ats systems.
999,klavans-2018-computational,"   Computational Challenges for Polysynthetic Languages"",
",given advances in computational linguistic analysis of complex languages using machine learning as well as standard finite state transducers coupled with recent efforts in language revitalization the time was right to organize a first workshop to bring together experts in language technology and linguists on the one hand with language practitioners and revitalization experts on the other. this one-day meeting provides a promising forum to discuss new research on polysynthetic languages in combination with the needs of linguistic communities where such languages are written and spoken.
